\chapter{Evaluation}
\label{ch:evaluation}

This chapter evaluates the technical performance, scalability, and efficiency of the developed Mixed Reality Environment. The assessment focuses on how the system handles increasing complexity and compares the results to the previous VERA framework developed by Gehricke \cite{Geh24}.

\section{Hardware and System Specifications}
The performance of a real-time Mixed Reality simulation is heavily dependent on the underlying hardware, as it must simultaneously handle physics calculations, ROS 2 communication, and high-frequency stereoscopic rendering. The benchmarks for this thesis were conducted on a desktop workstation, while the reference measurements for the original VERA framework by Gehricke \cite{Geh24} were performed on a high-performance laptop. The respective hardware configurations are summarized in Table \ref{tab:hardware_specs}.

\begin{table}[H]
    \centering
    \caption{Comparison of hardware configurations used for evaluation.}
    \label{tab:hardware_specs}
    \begin{tabular}{lll}
        \toprule
        \textbf{Component} & \textbf{Proposed System (Host)} & \textbf{VERA (Gehricke \cite{Geh24})} \\ 
        \midrule
        Processor (CPU)    & Intel i7-8700K (up to 4.7 GHz) & Intel i9-14900HX \\
        Graphics (GPU)     & NVIDIA RTX 2070 Super (8 GB)   & N/A (CPU-based rendering) \\
        Memory (RAM)       & 16 GB DDR4                     & 32 GB RAM \\
        OS                 & Windows 11 / WSL 2             & Ubuntu 24.04 (Native) \\
        \bottomrule
    \end{tabular}
\end{table}

A critical distinction must be made regarding the rendering architecture. Gehricke's system relied entirely on the CPU for its 2D Pygame-based visualization, which introduces processing contention between logic and drawing. In contrast, the system presented here leverages a dedicated GPU for hardware-accelerated rendering and physics. This offloading is essential not only for the stereoscopic demands of the Virtual Reality interface but also for maintaining high frame rates in complex scenes.

\section{Component Breakdown Analysis}
\label{sec:component_breakdown}

To identify the computational cost of the system's core functions, a granular profiling analysis was conducted using a custom \texttt{PerformanceBreakdownLogger} script. This tool utilizes the Unity \textit{ProfilerRecorder} API to capture the Main Thread execution time of three key subsystems: \textbf{Physics} (\texttt{Physics.Simulate}), \textbf{Rendering} (\texttt{Camera.Render}), and \textbf{Scripts} (\texttt{Update.ScriptRunBehaviour}). Data was recorded for 60 seconds across four test scenarios in both Digital Twin (DT) mode and Virtual Mode.

Figure \ref{fig:component_breakdown} visualizes the distribution of CPU time, while Table \ref{tab:component_data} details the specific execution metrics and memory usage.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/compinente break down.png}
    \caption{Component Breakdown Analysis illustrating the CPU execution time for Physics, Rendering, and Scripts across four scenarios in Digital Twin (DT) and Virtual modes.}
    \label{fig:component_breakdown}
\end{figure}

The data reveals that \textbf{Scripts} account for the vast majority of the frame time, ranging from 2.5 ms in the \textit{Paint} scenario to over 3.5 ms in the \textit{Farm} scenario. This overhead is primarily driven by the C\# logic required for ROS 2 serialization, message handling, and state management. Interestingly, in complex scenarios like \textit{Pong} and \textit{Farm}, the Digital Twin mode exhibits slightly higher script execution times (e.g., 3.55 ms vs. 3.35 ms in Farm) compared to Virtual Mode. This indicates that the overhead of synchronizing the digital twin with external telemetry and processing coordinate transforms is computationally comparable to, or slightly more demanding than, running the internal simulation logic for the virtual robot.

\begin{table}[H]
    \centering
    \caption{Average execution time and memory usage over a 60-second capture period.}
    \label{tab:component_data}
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Scenario} & \textbf{Mode} & \textbf{Physics (ms)} & \textbf{Render (ms)} & \textbf{Scripts (ms)} & \textbf{Memory (MB)} \\ 
        \midrule
        Paint & DT Mode & 0.215 & 0.009 & 2.523 & 4374.05 \\
         & Virtual Mode & 0.251 & 0.008 & 2.532 & 4643.03 \\
        \midrule
        Pong & DT Mode & 0.210 & 0.012 & 3.342 & 4459.98 \\
         & Virtual Mode & 0.187 & 0.008 & 2.746 & 4664.51 \\
        \midrule
        Logistics & DT Mode & 0.212 & 0.008 & 3.313 & 4433.06 \\
         & Virtual Mode & 0.204 & 0.008 & 3.189 & 4626.84 \\
        \midrule
        Farm & DT Mode & 0.245 & 0.011 & 3.554 & 4379.40 \\
         & Virtual Mode & 0.220 & 0.007 & 3.352 & 4632.64 \\
        \bottomrule
    \end{tabular}
\end{table}

Conversely, the \textbf{Physics} and \textbf{Render} components contribute negligibly to the CPU frame time (averaging $<$0.25 ms and $\approx$0.01 ms respectively). The extremely low CPU render times confirm that the rendering workload is successfully offloaded to the GPU. This is a significant architectural improvement over Gehricke's Pygame implementation, where rendering times averaged 3.75 ms on the CPU \cite{Geh24}.

\section{Performance and Scalability Analysis}
The scalability of the system was evaluated by measuring the Main Thread CPU time while incrementally increasing the number of interactive objects in the scene. To ensure reproducible and statistically stable results, the data was collected through an automated process: objects were spawned in fixed batches, followed by a one-second settling phase. This pause is critical to allow the physics engine to resolve initial contacts and allow rigid bodies to enter a "sleep" state, ensuring that the measurements reflect a stable environment. After this settling period, performance samples were collected over a one-second window to filter out singular frame spikes, such as those caused by background OS tasks or garbage collection.

The benchmarks were conducted across four configurations to isolate the costs of VR and dynamic physics. The resulting data is visualized in Figure \ref{fig:scalability_chart}, using a logarithmic scale to capture the wide variance in frame times.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/scalability_comparison.png}
    \caption{Scalability evaluation illustrating CPU frame time vs. object count. The 60 FPS target (16.67 ms) serves as the threshold for visual stability in Mixed Reality.}
    \label{fig:scalability_chart}
\end{figure}

The \textit{Non-VR Settled} mode serves as the baseline for the engine's rendering efficiency. In this mode, the system maintains a frame time between 3.12 ms and 3.68 ms for up to 9,000 objects. 
Gehricke \cite{Geh24} reported an average update time of approximately 3.75 ms for his 2D Pygame visualizer, with spikes occurring around 11,250 objects due to rendering limitations. The proposed Unity-based architecture matches and slightly improves upon this baseline ($\approx$3.12 ms), despite rendering a full 3D environment with lighting and materials. This demonstrates the efficiency of static mesh batching in modern game engines compared to immediate-mode 2D drawing.

When activating Virtual Reality (\textit{VR Settled}), the frame time stabilizes at approximately 13.8 ms. This constant overhead is characteristic of the stereoscopic rendering pipeline and the synchronization required for the 72 Hz display of the Meta Quest Pro. In this static state, the system successfully maintains the required 60 FPS threshold for up to 30,000 objects, significantly exceeding the limit observed in the original VERA framework.

However, the \textit{Active Physics} scenarios reveal a computational bottleneck not present in the previous work, simply because the previous work did not simulate physics. Gehricke's collision checks were simple distance heuristics with linear scalability. In contrast, this system resolves mass, inertia, and friction using Nvidia PhysX. While rendering scales linearly, the cost of resolving thousands of simultaneous physical collisions increases exponentially. The system remains performant for up to 3,000 dynamic objects, but exceeds the 16.67 ms threshold beyond 5,000 objects.

\section{Latency Analysis}
\label{sec:latency_analysis}

In addition to frame rate stability, the system responsiveness was evaluated by measuring the visualization latency. This metric is defined as the time elapsed between the reception of a pose update from the ArUco tracking system and the completion of the rendered frame displaying that update. A custom \texttt{LatencyMonitor} script was utilized to capture timestamps across four distinct application scenarios, ranging from low-complexity environments to high-fidelity simulations: \textit{Paint} (dynamic texture modification), \textit{Pong} (fast-moving physics objects), \textit{Logistics} (dynamic object spawning), and \textit{Farm} (complex geometry and vegetation).

Figure \ref{fig:latency_chart} illustrates the recorded latency for both the AR Projection (Non-VR) and Virtual Reality (VR) modes. In the standard AR Projection mode, the system demonstrates high responsiveness, with average latencies ranging from 3.02 ms in the simple Paint scenario to 4.41 ms in the complex Farm environment.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/latency.png}
    \caption{Visualization latency measured across four application scenarios. The chart compares the delay between pose reception and frame completion in AR Projection mode versus Virtual Reality mode. Error bars indicate the jitter (min/max range).}
    \label{fig:latency_chart}
\end{figure}

These results represent a significant improvement over the original VERA framework. Gehricke reported a collision-to-visualization latency of approximately 43.8 ms for an environment with 800 objects \cite{Geh24}. Furthermore, Gehricke identified a critical scalability issue where the message queue for marker updates became saturated in environments with over 1,250 objects, leading to latencies spiking into the range of several seconds.

The proposed architecture eliminates this bottleneck. By utilizing the native \texttt{Ros2ForUnity} plugin, which communicates directly via the RCL layer without the overhead of Python callbacks used in the previous work, the system maintains a deterministic latency profile. Even in the complex \textit{Farm} scenario, the Non-VR latency remains below 5 ms.

Activating Virtual Reality introduces a consistent latency overhead, increasing the average processing time to approximately 9.4 ms for Paint and up to 10.7 ms for the Farm scenario. This increase is attributed to the computational cost of stereoscopic rendering and the synchronization constraints of the VR headset. Despite this increase, the total visualization latency remains well below the critical 20 ms motion-to-photon threshold required to prevent motion sickness in VR, validating the system's suitability for immersive Robot-in-the-Loop interaction.