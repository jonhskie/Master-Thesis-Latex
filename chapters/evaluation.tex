\chapter{Evaluation}
\label{ch:evaluation}

This chapter provides an evaluation of the developed Mixed Reality Environment. It starts with a qualitative analysis of the system’s functional capabilities, particularly the mixed reality interfaces and the practical execution of the robotic application scenarios. Next, the chapter details the technical setup and then presents a quantitative analysis of the system’s performance. This analysis aims to identify the operational limits and efficiency of the framework.
\section{Mixed Reality Interface Validation}
\label{sec:mr_validation}

This section looks at the VR and AR interfaces to assess the system’s human-robot interaction capabilities. The assessment focuses on the usability of control mechanisms, the visualization of the robot's state, and the synchronization between the physical and virtual environments.

\subsection{Virtual Reality Interface}
\label{subsec:eval_vr}
The VR interface meets the requirement to view the digital twin in 3D space (\hyperref[req:FR-16]{FR-16}) by placing the operator directly inside the simulated environment. The environment is rendered at a 1:1 scale, allowing for a natural perception of the robot and its surroundings. Users can move through physical walking or joystick control, with discrete rotation steps added to reduce simulation sickness.

Interaction with the environment is facilitated by a ray-casting mechanism controlled by the VR handheld controllers, as shown in Figure~\ref{fig:vr_interface_fpv}. When the input is triggered, a visual ray projects from the controller, serving as a tool for orientation and targeting. This mechanism is mainly used for two tasks: painting on the virtual ground and setting navigation goals for the ROS 2 navigation stack.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/vr_view.png} % Placeholder filename
    \caption{First-person view of the VR interface. The user places objects in the virtual environment and utilizes a ray-cast from the controller to paint on the virtual ground.}
    \label{fig:vr_interface_fpv}
\end{figure}

To support dynamic scenario modification (\hyperref[req:FR-13]{FR-13}), a storage area is located adjacent to the robot's operating field. This area contains various manipulatable objects, including boxes and cuboids in different sizes and colors. A distance grab mechanism allows the user to pick up objects from a distance by pointing the controller at the target and pressing the grip button. This action snaps the object directly to the virtual hand, so the operator does not need to crouch to retrieve items. Users can take these objects and place them into the active simulation area to create obstacles or targets. The physics engine provides realistic collision handling. The placed objects can act as physical barriers that the robot must either push or navigate around.

Data visualization in VR is handled through two primary elements, as depicted in Figure~\ref{fig:vr_overview}. First, a floating status billboard is attached to the robot’s digital twin. This billboard is designed to always face the user’s camera view, making sure that telemetry data is readable from any angle. Second, a large virtual monitor is positioned behind the robot's operation area. This monitor subscribes to image topics from the ROS 2 network, such as the raw camera feed or debug output from the perception stack. Users can switch the displayed video source by pressing a specific key on the keyboard.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/vr_storage.png} % Placeholder filename
    \caption{ Overview of the Virtual Reality environment. The storage area containing manipulatable objects is visible next to the robot’s play area. A large virtual monitor displays the live camera stream from the ROS 2 network.}
    \label{fig:vr_overview}
\end{figure}
\subsection{Augmented Reality Projection}

%###########################################################fertig##############################################################
\label{subsec:eval_ar}

The AR projection meets requirement \hyperref[req:FR-15]{FR-15} by mapping the virtual environment directly onto the physical floor. This setup allows observers without VR equipment to monitor the robot’s internal status and its virtual context in real time. The projection aligns well with the physical robot, especially in the center of the tracking area. There is some drift at the outer edges due to lens distortion in the camera of the ARuco tracking system, but the accuracy is still good enough for evaluating navigation tasks.
Like the VR interface, the AR system displays a status billboard and a video feed plane. These elements are placed at a fixed distance behind the physical robot, as shown in Figure~\ref{fig:ar_projection}, to prevent the projection from being occluded by the robot's chassis or casting shadows. To ensure the visual clarity of the physical testbed for external observers or to modify the visual scene, the system allows for the selective activation and deactivation of individual components such as the digital body of the robot, light sources, and information panels via keyboard commands.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/ar_projection.jpg} % Placeholder filename
    \caption{The Augmented Reality projection in the physical laboratory. The system projects the virtual environment, the status billboard, and the live camera feed onto the floor, following the movement of the physical EMAROs robot.}
    \label{fig:ar_projection}
\end{figure}
\section{Scenario Realization and Application Analysis}
\label{sec:scenario_analysis}

To showcase the system’s flexibility, four different scenarios were created. These test cases confirmed the environmental capabilities and the logic of the robot. Additionally, the system verified that it could load these various scenarios in real time, showing it can switch environments without restarting the main application (\hyperref[req:FR-06]{FR-06}).

Throughout all applications, the fundamental integration between the physical robot and the simulation was maintained through ROS 2 communication and the usage of a \texttt{/clock} topic for time synchronization (\hyperref[req:FR-04]{FR-04}, \hyperref[req:FR-05]{FR-05}).  The robot’s physical state was continuously reflected by its digital twin, ensuring that spatial relationships and coordinate transformations were consistent between the real and virtual worlds (\hyperref[req:FR-07]{FR-07}, \hyperref[req:FR-10]{FR-10}).  A system-wide safety watchdog operated during the entire evaluation. If the robot left the designated tracking area, the system issued a zero-velocity command to avoid the robot from leaving the tracking area (\hyperref[req:FR-08]{FR-08}).

%#######################################################fertig##################################################################
\subsection{Application 1: Dynamic Surface Interaction (Paint)}
\label{subsec:eval_paint}

The first scenario creates an interactive workspace to evaluate the system's capacity for real-time environmental modification. The environment consists of a high-contrast white floor where the surface texture serves as the primary navigation medium. To realize this, the system utilizes the \texttt{TrackPainter} script attached to the robot and the \texttt{MouseSurfacePainter} used by the operator. Both components interact with a \texttt{SharedPaintTextureRegistry}, a specialized class that manages texture synchronization. This architecture ensures that when a user draws a path using the VR controller's ray-cast or the desktop mouse, the pixel data is instantly updated for all participants, enabling the robot to perceive the changes without delay (\hyperref[req:FR-13]{FR-13}).

The robotic application deployed in this scenario, shown in Figure~\ref{fig:paint_ar}, is a vision-based Line Follower. The robot operates without a pre-existing map, relying exclusively on the live video stream from its virtual camera to detect the path geometry (\hyperref[req:FR-11]{FR-11}). The control logic is governed by a finite state machine that autonomously transitions between searching for visual features, aligning the robot's heading with the path vector, and executing a PID-controlled following behavior based on the sensor data (\hyperref[req:FR-19]{FR-19}).

During the evaluation, the robotic application demonstrated robust performance. The PID controller successfully maintained the robot's position on the line with minimal oscillation, even when the user drew complex curves. When the line ended or was cleared by the user, the robot immediately transitioned to a search behavior, rotating to scan for new visual features. Once a new line was drawn, the perception stack identified it within a single frame, and the robot successfully realigned its heading to resume tracking. This behavior confirms that the virtual user interactions serve as effective, real-time sensory inputs for the physical robot.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/paint_scenario_ar.jpg} % Placeholder
    \caption{Augmented Reality projection of the Paint scenario. The physical EMAROs robot (center) follows a red path drawn by the user. The projected line serves as the visual stimulus for the robot's internal perception stack, closing the loop between user input and physical action.}
    \label{fig:paint_ar}
\end{figure}

\subsection{Application 2: Reactive Physics Interaction (Pong)}
\label{subsec:eval_pong}

The Pong scenario evaluates the system's ability to facilitate continuous, dynamic interactions between a physical robot and virtual physics entities, as depicted in Figure~\ref{fig:pong_scenario}. The environment implements a classic arcade game logic, utilizing \texttt{PongGameManager} and \texttt{PongField} scripts to manage the game state and boundaries. The critical component for mixed reality interaction is the \texttt{PaddlePlank} script attached to the robot's digital twin. This script creates a kinematic box collider that mirrors the physical robot's transform, forming a solid barrier in the simulation. The ball's physics are governed by the \texttt{PongBall} script, which employs a hybrid control approach: it utilizes Unity's PhysX engine for collision detection but calculates reflection vectors mathematically. This ensures predictable energy conservation during high-speed impacts, regardless of friction settings (\hyperref[req:FR-02]{FR-02}).

In this application, the human operator acts as the high-level planner. Using the VR controller, the operator observes the trajectory of the virtual ball and issues navigation goals to the robot's navigation stack to intercept it via the \texttt{NavigationGoalController} (\hyperref[req:FR-17]{FR-17}).

The integration of the navigation stack proved effective for these dynamic interception tasks. Despite the latency inherent in wireless communication, the robot successfully reached the commanded goals in time to deflect the virtual ball. The physical-virtual coupling was validated by the ball's behavior; as the robot moved to intercept, the virtual paddle mirrored its transform exactly, causing the ball to bounce realistically off the moving robot (\hyperref[req:FR-13]{FR-13}). No "tunneling" artifacts were observed, confirming that the system supports time-sensitive, reactive control.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/pong_scenario.jpg} % Placeholder
    \caption{Top-down view of the Pong scenario. The user utilizes the Nav2 stack to position the physical robot (left) to intercept the virtual ball. The ball interacts with the digital twin's geometry, bouncing off the attached paddle based on the robot's real-world position.}
    \label{fig:pong_scenario}
\end{figure}
%#########################################################################################################################
\subsection{Application 3: Autonomous Logistics}
\label{subsec:eval_logistics}

The Logistics scenario assesses the system's support for complex, multi-stage missions, illustrated in Figure~\ref{fig:logistics_ar}, involving object manipulation and decision-making. The environment simulates a warehouse populated with colored transport boxes and delivery zones. Specific C\# components were implemented to handle the logic: \texttt{BoxEquipment} tags objects with semantic types (e.g., Red or Blue), and \texttt{DeliveryZone} scripts monitor spatial triggers. Crucially, the validation logic is decoupled from the robot; when a valid delivery occurs, the \texttt{LogisticsManager} acts as the simulation authority, formatting a status string and transmitting it via the ROS 2 network to the robot (\hyperref[req:FR-12]{FR-12}, \hyperref[req:FR-20]{FR-20}).

The robotic application executes a Search-Transport-Deliver cycle driven by a state machine. The robot autonomously coordinates its navigation stack to search the floor plan, identifies targets using a color-based perception processor, and approaches them for pickup. Upon reaching a target, the robot triggers an attachment command, which is processed by the \texttt{AttachmentCommandManager} to kinematically lock the virtual box to the robot's chassis (\hyperref[req:FR-13]{FR-13}).

The autonomous agent successfully completed the full logistics cycle without human intervention. The visual servoing logic reliably aligned the robot with the target boxes, and the attachment system prevented any visual jitter during transport. A key feature validated in this scenario is the robot's spatial memory logic; after locating a delivery zone once, the agent successfully stored the coordinates and navigated directly to them in subsequent runs, significantly reducing the mission time. Additionally, the robot's navigation stack successfully detected dynamic obstacles placed by the VR user, such as black walls, and replanned trajectories in real time, demonstrating the robustness of the autonomous behaviors in a changing environment.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/logistics_ar.jpg} % Placeholder
    \caption{Augmented Reality projection of the Logistics scenario. The physical robot (center) navigates through the projected warehouse to transport a virtual box to the designated colored zone. The environment allows the VR user to dynamically place obstacles, forcing the robot to replan its path.}
    \label{fig:logistics_ar}
\end{figure}
%#########################################################################################################################
\subsection{Application 4: Smart Farming}
\label{subsec:eval_farming}

The Smart Farming scenario, shown in Figure~\ref{fig:smart_farming_ar}, serves as the most comprehensive integration test, combining navigation, complex state management, and tool-based environmental modification. The environment features a \texttt{FieldTileSystem} that manages the visual state of the ground, managing transitions between states such as "Untouched," "Seeded," and "Growing." The robot operates interchangeable tools—Plow, Seeder, and Harvester—which utilize the \texttt{FarmEquipmentBase} script. This script employs a multi-raycast mechanism to detect the specific tiles beneath the robot and modify their state only when the tool is physically attached and active (\hyperref[req:FR-13]{FR-13}).

The robotic application executes a sequential workflow consisting of field mapping, tool attachment, and coverage path planning. The robot must autonomously identify field boundaries via edge detection, swap tools, and drive precise rows to process the crop.

The application demonstrated a high degree of reliability in identifying field boundaries. The edge detection processor consistently identified the transition between the field texture and the outer floor, allowing the robot to accurately map the four corners of the field. Based on these corners, the coverage path planner generated valid waypoints, and the navigation stack executed the path with sufficient precision to ensure full coverage of the tiles. The evaluation confirmed that the tools modified the ground texture only when physically attached to the robot, changing the state from "Cultivated" to "Seeded" as the robot progressed. Furthermore, the robot handled user interference gracefully; when the VR operator placed obstacles like rocks in the field, the robot successfully navigated around them or skipped unreachable waypoints, validating the robustness of the navigation goals (\hyperref[req:FR-17]{FR-17}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/smart_farming_ar.jpg} % Placeholder
    \caption{AR projection of the Smart Farming scenario. The physical EMAROs robot, equipped with the Seeder tool, navigates the virtual field, modifying the ground texture from "Cultivated" to "Seeded" as it progresses.}
    \label{fig:smart_farming_ar}
\end{figure}
%#########################################################################################################################
\section{Hardware and System Specifications}
The performance of a real-time Mixed Reality simulation is heavily dependent on the underlying hardware, as it must simultaneously handle physics calculations, ROS 2 communication, and high-frequency stereoscopic rendering. The benchmarks for this thesis were conducted on a desktop workstation, while the reference measurements for the original VERA framework by Gehricke~\cite{Geh24} were performed on a high-performance laptop. The respective hardware configurations are summarized in Table~\ref{tab:hardware_specs}.

\begin{table}[H]
    \centering
    \caption{Comparison of hardware configurations used for evaluation.}
    \label{tab:hardware_specs}
    \begin{tabular}{lll}
        \toprule
        \textbf{Component} & \textbf{Proposed System (Host)} & \textbf{VERA (Gehricke~\cite{Geh24})} \\ 
        \midrule
        Processor (CPU)    & Intel i7-8700K (up to 4.7 GHz) & Intel i9-14900HX \\
        Graphics (GPU)     & NVIDIA RTX 2070 Super (8 GB)   & N/A (CPU-based rendering) \\
        Memory (RAM)       & 16 GB DDR4                     & 32 GB RAM \\
        OS                 & Windows 11 / WSL 2             & Ubuntu 24.04 (Native) \\
        \bottomrule
    \end{tabular}
\end{table}
%#########################################################################################################################
\section{Performance Analysis of Robotic Scenarios}
\label{sec:scenario_performance}

To evaluate the system's performance under realistic operating conditions, a granular profiling analysis was conducted during the execution of the four target scenarios described in Section~\ref{sec:scenario_realization}. A custom \texttt{PerformanceBreakdownLogger} script utilized the Unity \textit{ProfilerRecorder} API to capture the Main Thread execution time of three key subsystems: \textbf{Physics} (\texttt{Physics.Simulate}), \textbf{Rendering} (\texttt{Camera.Render}), and \textbf{Scripts} (\texttt{Update.ScriptRunBehaviour}). 

This data provides the primary validation for \textbf{NFR-02 (Frame Rate)}, demonstrating the system's ability to maintain real time performance during complex robotic tasks. Figure~\ref{fig:component_breakdown} visualizes the distribution of CPU time, while Table~\ref{tab:component_data} details the specific execution metrics.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/compinente break down.png}
    \caption{Performance breakdown illustrating the CPU execution time for Physics, Rendering, and Scripts across the four operational scenarios. The total active frame time remains well below the 16.67 ms threshold required for 60 FPS.}
    \label{fig:component_breakdown}
\end{figure}

The data reveals that the system performs efficiently across all scenarios. The total active CPU time (sum of Physics, Render, and Scripts) averages between 2.7 ms and 3.8 ms. It is important to note that while the VR hardware operates at a fixed 72 Hz refresh rate (requiring a frame every 13.88 ms), the software completes its actual calculations in under 4 ms. Consequently, the CPU remains idle for the remaining $\approx$10 ms of each frame interval, waiting for the vertical synchronization (V-Sync) signal. This substantial computational headroom validates that the system can handle even more complex logic or additional sensor processing before dropping below the hardware's native frame rate.

\textbf{Scripts} account for the majority of the active frame time, ranging from 2.5 ms in the \textit{Paint} scenario to over 3.5 ms in the \textit{Farm} scenario. This overhead is driven by the C\# logic required for ROS 2 serialization, message handling, and state management required by \textbf{FR-04 (ROS 2 Integration)}. Interestingly, in complex scenarios like \textit{Pong} and \textit{Farm}, the Digital Twin mode exhibits slightly higher script execution times (e.g., 3.55 ms vs. 3.35 ms in Farm) compared to Virtual Mode. This indicates that the overhead of synchronizing the digital twin with external telemetry (\textbf{FR-07}) is computationally comparable to running the internal simulation logic for the virtual robot.

\begin{table}[H]
    \centering
    \caption{Average execution time and memory usage over a 60-second capture period.}
    \label{tab:component_data}
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Scenario} & \textbf{Mode} & \textbf{Physics (ms)} & \textbf{Render (ms)} & \textbf{Scripts (ms)} & \textbf{Memory (MB)} \\ 
        \midrule
        Paint & DT Mode & 0.215 & 0.009 & 2.523 & 4374.05 \\
         & Virtual Mode & 0.251 & 0.008 & 2.532 & 4643.03 \\
        \midrule
        Pong & DT Mode & 0.210 & 0.012 & 3.342 & 4459.98 \\
         & Virtual Mode & 0.187 & 0.008 & 2.746 & 4664.51 \\
        \midrule
        Logistics & DT Mode & 0.212 & 0.008 & 3.313 & 4433.06 \\
         & Virtual Mode & 0.204 & 0.008 & 3.189 & 4626.84 \\
        \midrule
        Farm & DT Mode & 0.245 & 0.011 & 3.554 & 4379.40 \\
         & Virtual Mode & 0.220 & 0.007 & 3.352 & 4632.64 \\
        \bottomrule
    \end{tabular}
\end{table}

Conversely, the \textbf{Physics} and \textbf{Render} components contribute negligibly to the CPU frame time (averaging $<$0.25 ms and $\approx$0.01 ms respectively). The extremely low CPU render times confirm that the rendering workload is successfully offloaded to the GPU. This is a significant architectural improvement over Gehricke's Pygame implementation, where rendering times averaged 3.75 ms on the CPU~\cite{Geh24}.
%#########################################################################################################################
\section{Deep Profiling and Script Impact Analysis}
\label{sec:deep_profiling}

To understand why "Scripts" consume the majority of the frame budget in the operational scenarios, a Deep Profiling session was conducted. Unlike standard profiling, which only captures high-level timing data, Deep Profiling instruments every C\# method call to provide a granular breakdown of the execution stack. 

It is important to note that Deep Profiling introduces significant overhead, inflating the execution time of small methods by a factor of 10 to 20. Consequently, the absolute values presented in this section should not be interpreted as runtime performance in a release build, but rather as relative indicators of computational complexity.

\begin{table}[H]
    \centering
    \caption{Relative impact of key scripts during a Deep Profiling session. The data highlights the perception pipeline (Camera/Material) as the primary consumer of resources, while navigation logic remains highly efficient.}
    \label{tab:script_impact}
    \begin{tabular}{lrr}
        \toprule
        \textbf{Script / Component} & \textbf{Deep Profile Time (ms)} & \textbf{\% of Script Budget} \\ 
        \midrule
        \texttt{CameraSensor.Update} & 7.98 & 68.2\% \\
        \texttt{RosImageToMaterial.Update} & 1.68 & 14.4\% \\
        \texttt{XRInteractionManager} & 1.18 & 10.1\% \\
        \texttt{PoseRobotController.FixedUpdate} & 0.59 & 5.0\% \\
        Other Logic Scripts (Combined) & $<$ 0.10 & $<$ 1.0\% \\
        \bottomrule
    \end{tabular}
\end{table}

As summarized in Table~\ref{tab:script_impact}, the analysis identifies the visual perception pipeline required for \textbf{FR-11 (Sensor Simulation)} and \textbf{FR-15 (AR Projection)} as the most demanding subsystem. The \texttt{CameraSensor.Update} method accounts for the majority of the script execution time. This cost is attributed to the \texttt{Texture2D.ReadPixels} operation required to transfer the rendered frame from the GPU to the CPU memory for serialization into a ROS 2 message. Similarly, the \texttt{RosImageToMaterial.Update}, which performs the inverse operation (uploading received textures to the GPU for AR projection), represents the second-largest cost.

In stark contrast, the robotic logic and environmental interaction scripts exhibited a negligible computational footprint. The components detailed in the Implementation chapter, including the \texttt{LidarSensor} (raycasting), \texttt{TrackPainter} (texture modification), \texttt{ClockPublisher} (time sync), and \texttt{DynamicObjectManager}, all registered execution times below 0.05 ms even under deep profiling instrumentation. 

For example, the \texttt{PoseRobotController}, which handles the critical synchronization of the digital twin's transform (\textbf{FR-07}), consumed only 0.59 ms in the physics loop. This confirms that the architectural bottleneck of the system lies entirely in the memory bandwidth required for high-definition video streaming, while the core simulation logic, physics interactions (\textbf{FR-02}), and ROS 2 state synchronization remain highly efficient.
%#########################################################################################################################
\section{Scalability and Stress Testing}
\label{sec:scalability}

While the previous sections validated the performance of the specific application scenarios, this section evaluates the theoretical limits of the system architecture. A stress test was conducted by measuring the Main Thread CPU time while incrementally increasing the number of interactive objects in the scene to validate \textbf{NFR-04 (Scalability)}.

To ensure reproducible results, objects were spawned in fixed batches followed by a one-second settling phase. The benchmarks were conducted across four configurations to isolate the costs of VR and dynamic physics. The resulting data is visualized in Figure~\ref{fig:scalability_chart}, using a logarithmic scale to capture the wide variance in frame times.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/scalability_comparison.png}
    \caption{Scalability stress test illustrating CPU frame time vs. object count. The 60 FPS target (16.67 ms) serves as the threshold for visual stability in Mixed Reality.}
    \label{fig:scalability_chart}
\end{figure}

The \textit{Non-VR Settled} mode demonstrates the engine's rendering efficiency, maintaining a frame time between 3.12 ms and 3.68 ms for up to 9,000 objects. Gehricke~\cite{Geh24} reported an average update time of approximately 3.75 ms for his 2D Pygame visualizer, with spikes occurring around 11,250 objects. The proposed Unity-based architecture matches and slightly improves upon this baseline, despite rendering a full 3D environment.

When activating Virtual Reality (\textit{VR Settled}), the frame time stabilizes at approximately 13.8 ms. This constant overhead is characteristic of the stereoscopic rendering pipeline and the synchronization required for the 72 Hz display of the Meta Quest Pro (\textbf{FR-03}). As established in the scenario analysis, this value primarily represents V-Sync wait time rather than active computation limits. In this static state, the system successfully maintains the required 60 FPS threshold for up to 30,000 objects, significantly exceeding the limit observed in the original VERA framework.

However, the \textit{Active Physics} scenarios reveal the computational bottleneck. Unlike the previous work which relied on simple distance heuristics, this system simulates mass, inertia, and friction using Nvidia PhysX. While rendering scales linearly, the cost of resolving thousands of simultaneous physical collisions increases exponentially. The system remains performant for up to 3,000 dynamic objects, but exceeds the 16.67 ms threshold beyond 5,000 objects. This limit far exceeds the requirements of standard scenarios (like the Farm or Logistics applications), which typically contain fewer than 100 dynamic objects.
%#########################################################################################################################
\section{Latency Analysis}
\label{sec:latency_analysis}

System responsiveness was evaluated by measuring the visualization latency to validate \textbf{NFR-01 (Low Latency)}. This metric is defined as the time elapsed between the reception of a pose update from the ArUco tracking system and the completion of the rendered frame displaying that update. A custom \texttt{LatencyMonitor} script was utilized to capture timestamps across the four operational scenarios.

Figure~\ref{fig:latency_chart} illustrates the recorded latency for both the AR Projection (Non-VR) and Virtual Reality (VR) modes. In the standard AR Projection mode, the system demonstrates high responsiveness, with average latencies ranging from 3.02 ms in the simple Paint scenario to 4.41 ms in the complex Farm environment.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/latency.png}
    \caption{Visualization latency measured across four application scenarios. The chart compares the delay between pose reception and frame completion in AR Projection mode versus Virtual Reality mode. Error bars indicate the jitter (min/max range).}
    \label{fig:latency_chart}
\end{figure}

These results represent a significant improvement over the original VERA framework. Gehricke reported a collision-to-visualization latency of approximately 43.8 ms for an environment with 800 objects~\cite{Geh24}. Furthermore, Gehricke identified a critical scalability issue where the message queue for marker updates became saturated in environments with over 1,250 objects, leading to latencies spiking into the range of several seconds.

The proposed architecture eliminates this bottleneck. By utilizing the native \texttt{Ros2ForUnity} plugin, which communicates directly via the RCL layer without the overhead of Python callbacks used in the previous work, the system maintains a deterministic latency profile. Even in the complex \textit{Farm} scenario, the Non-VR latency remains below 5 ms, satisfying the low-latency requirement (\textbf{NFR-01}) essential for Robot-in-the-Loop synchronization.

Activating Virtual Reality introduces a consistent latency overhead, increasing the average processing time to approximately 9.4 ms for Paint and up to 10.7 ms for the Farm scenario. This increase is attributed to the computational cost of stereoscopic rendering and the synchronization constraints of the VR headset. Despite this increase, the total visualization latency remains well below the critical 20 ms motion-to-photon threshold required to prevent motion sickness in VR, validating the system's suitability for immersive Robot-in-the-Loop interaction (\textbf{FR-16}).