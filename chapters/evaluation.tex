\chapter{Evaluation}
\label{ch:evaluation}

This chapter evaluates the technical performance, scalability, and efficiency of the developed Mixed Reality Environment. The assessment focuses on how the system handles increasing complexity and compares the results to the previous VERA framework.

\section{Hardware and System Specifications}
The performance of a real-time Mixed Reality simulation is heavily dependent on the underlying hardware, as it must simultaneously handle physics calculations, ROS 2 communication, and high-frequency stereoscopic rendering. The benchmarks for this thesis were conducted on a desktop workstation, while the reference measurements for the original VERA framework by Gehricke \cite{Geh24} were performed on a high-performance laptop. The respective hardware configurations are summarized in Table \ref{tab:hardware_specs}.

\begin{table}[H]
    \centering
    \caption{Comparison of hardware configurations used for evaluation.}
    \label{tab:hardware_specs}
    \begin{tabular}{lll}
        \toprule
        \textbf{Component} & \textbf{Proposed System (Host)} & \textbf{VERA (Gehricke \cite{Geh24})} \\ 
        \midrule
        Processor (CPU)    & Intel i7-8700K (up to 4.7 GHz) & Intel i9-14900HX \\
        Graphics (GPU)     & NVIDIA RTX 2070 Super (8 GB)   & N/A (CPU-based rendering) \\
        Memory (RAM)       & 16 GB DDR4                     & 32 GB RAM \\
        OS                 & Windows 11 / WSL 2             & Ubuntu 24.04 (Native) \\
        \bottomrule
    \end{tabular}
\end{table}

It is important to note that Gehricke's system relied entirely on the CPU for its 2D Pygame-based visualization. In contrast, the system presented here leverages a dedicated GPU for hardware-accelerated rendering and physics, which is essential for the stereoscopic demands of the Virtual Reality interface.

\section{Component Breakdown Analysis}
\label{sec:component_breakdown}

To identify the computational cost of the system's core functions, a granular profiling analysis was conducted using a custom \texttt{PerformanceBreakdownLogger} script. This tool utilizes the Unity \textit{ProfilerRecorder} API to capture the Main Thread execution time of three key subsystems: \textbf{Physics} (\texttt{Physics.Simulate}), \textbf{Rendering} (\texttt{Camera.Render}), and \textbf{Scripts} (\texttt{Update.ScriptRunBehaviour}). Data was recorded for 60 seconds across four test scenarios in both Digital Twin (DT) mode and Virtual Mode.

Figure \ref{fig:component_breakdown} visualizes the distribution of CPU time, while Table \ref{tab:component_data} details the specific execution metrics and memory usage.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/compinente break down.png}
    \caption{Component Breakdown Analysis illustrating the CPU execution time for Physics, Rendering, and Scripts across four scenarios in Digital Twin (DT) and Virtual modes.}
    \label{fig:component_breakdown}
\end{figure}

The data reveals that \textbf{Scripts} account for the vast majority of the frame time, ranging from 2.5 ms in the \textit{Paint} scenario to over 3.5 ms in the \textit{Farm} scenario. This overhead is primarily driven by the C\# logic required for ROS 2 serialization, message handling, and state management. Interestingly, in complex scenarios like \textit{Pong} and \textit{Farm}, the Digital Twin mode exhibits slightly higher script execution times (e.g., 3.55 ms vs. 3.35 ms in Farm) compared to Virtual Mode. This indicates that the overhead of synchronizing the digital twin with external telemetry and processing coordinate transforms is computationally comparable to, or slightly more demanding than, running the internal simulation logic for the virtual robot.

\begin{table}[H]
    \centering
    \caption{Average execution time and memory usage over a 60-second capture period.}
    \label{tab:component_data}
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Scenario} & \textbf{Mode} & \textbf{Physics (ms)} & \textbf{Render (ms)} & \textbf{Scripts (ms)} & \textbf{Memory (MB)} \\ 
        \midrule
        Paint & DT Mode & 0.215 & 0.009 & 2.523 & 4374.05 \\
         & Virtual Mode & 0.251 & 0.008 & 2.532 & 4643.03 \\
        \midrule
        Pong & DT Mode & 0.210 & 0.012 & 3.342 & 4459.98 \\
         & Virtual Mode & 0.187 & 0.008 & 2.746 & 4664.51 \\
        \midrule
        Logistics & DT Mode & 0.212 & 0.008 & 3.313 & 4433.06 \\
         & Virtual Mode & 0.204 & 0.008 & 3.189 & 4626.84 \\
        \midrule
        Farm & DT Mode & 0.245 & 0.011 & 3.554 & 4379.40 \\
         & Virtual Mode & 0.220 & 0.007 & 3.352 & 4632.64 \\
        \bottomrule
    \end{tabular}
\end{table}

Conversely, the \textbf{Physics} and \textbf{Render} components contribute negligibly to the CPU frame time (averaging $<$0.25 ms and $\approx$0.01 ms respectively). The low render times on the CPU confirm that the rendering workload is successfully offloaded to the GPU, validating the hardware-accelerated architecture. Memory usage remains stable between 4.3 GB and 4.6 GB, with Virtual Mode consistently utilizing approximately 200 MB more memory, likely due to the maintenance of additional internal simulation states.

\section{Performance and Scalability Analysis}
The scalability of the system was evaluated by measuring the Main Thread CPU time while incrementally increasing the number of interactive objects in the scene. To ensure reproducible and statistically stable results, the data was collected through an automated process: objects were spawned in fixed batches, followed by a one-second settling phase. This pause is critical to allow the physics engine to resolve initial contacts and allow rigid bodies to enter a "sleep" state, ensuring that the measurements reflect a stable environment. After this settling period, performance samples were collected over a one-second window to filter out singular frame spikes, such as those caused by background OS tasks or garbage collection.

The benchmarks were conducted across four configurations to isolate the costs of VR and dynamic physics. The resulting data is visualized in Figure \ref{fig:scalability_chart}, using a logarithmic scale to capture the wide variance in frame times.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/scalability_comparison.png}
    \caption{Scalability evaluation illustrating CPU frame time vs. object count. The 60 FPS target (16.67 ms) serves as the threshold for visual stability in Mixed Reality.}
    \label{fig:scalability_chart}
\end{figure}

The \textit{Non-VR Settled} mode serves as the baseline for the engine's rendering efficiency. In this mode, the system maintains a frame time between 3.12 ms and 3.68 ms for up to 9,000 objects. This is a slight improvement over the 3.75 ms reported by Gehricke \cite{Geh24} for a 2D system, demonstrating that the 3D engine handles static mesh batching with high efficiency.

When activating Virtual Reality (\textit{VR Settled}), the frame time stabilizes at approximately 13.8 ms. This constant overhead is characteristic of the stereoscopic rendering pipeline and the synchronization required for the 72 Hz display of the Meta Quest Pro. In this static state, the system successfully maintains the required 60 FPS threshold for up to 30,000 objects, significantly exceeding the 11,250-object limit of the original VERA framework.

However, the \textit{Active Physics} scenarios reveal a clear computational bottleneck. While rendering scales linearly, the cost of resolving thousands of simultaneous physical collisions increases exponentially. The system remains performant for up to 3,000 dynamic objects, but exceeds the 16.67 ms threshold beyond 5,000 objects due to the complexity of the collision solver.

\section{Latency Analysis}
\label{sec:latency_analysis}

In addition to frame rate stability, the system responsiveness was evaluated by measuring the visualization latency. This metric is defined as the time elapsed between the reception of a pose update from the ArUco tracking system and the completion of the rendered frame displaying that update. A custom \texttt{LatencyMonitor} script was utilized to capture timestamps across four distinct application scenarios, ranging from low-complexity environments to high-fidelity simulations: \textit{Paint} (dynamic texture modification), \textit{Pong} (fast-moving physics objects), \textit{Logistics} (dynamic object spawning), and \textit{Farm} (complex geometry and vegetation).

Figure \ref{fig:latency_chart} illustrates the recorded latency for both the AR Projection (Non-VR) and Virtual Reality (VR) modes. In the standard AR Projection mode, the system demonstrates high responsiveness, with average latencies ranging from 3.02 ms in the simple Paint scenario to 4.41 ms in the complex Farm environment. The minimal increase in latency ($\approx$1.4 ms) between the simplest and most complex scenarios indicates that the orthographic projection pipeline is highly optimized and largely independent of scene complexity. The jitter, represented by the error bars, remains low in Non-VR mode, confirming a deterministic rendering loop suitable for real-time robot tracking.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/latency.png}
    \caption{Visualization latency measured across four application scenarios. The chart compares the delay between pose reception and frame completion in AR Projection mode versus Virtual Reality mode. Error bars indicate the jitter (min/max range).}
    \label{fig:latency_chart}
\end{figure}

Activating Virtual Reality introduces a consistent latency overhead, increasing the average processing time to approximately 9.4 ms for Paint and up to 10.7 ms for the Farm scenario. This increase is attributed to the computational cost of stereoscopic rendering (rendering two viewpoints simultaneously) and the synchronization constraints of the VR headset's 72 Hz refresh rate. Furthermore, the jitter increases significantly in VR mode, particularly in the Logistics and Farm scenarios. This variance is likely caused by the overhead of the ALVR streaming protocol and the wireless transmission of frame data to the headset. Despite this increase, the total visualization latency remains well below the critical 20 ms motion-to-photon threshold required to prevent motion sickness in VR, validating the system's suitability for immersive Robot-in-the-Loop interaction.