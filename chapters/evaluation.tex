\chapter{Evaluation}
\label{ch:evaluation}

This chapter provides an evaluation of the developed Mixed Reality Environment. It starts with a qualitative analysis of the system’s functional capabilities, particularly the mixed reality interfaces and the practical execution of the robotic application scenarios. Next, the chapter details the technical setup and then presents a quantitative analysis of the system’s performance. This analysis aims to identify the operational limits and efficiency of the framework.
\section{Mixed Reality Interface Validation}
\label{sec:mr_validation}

This section looks at the VR and AR interfaces to assess the system’s human-robot interaction capabilities. The assessment focuses on the usability of control mechanisms, the visualization of the robot's state, and the synchronization between the physical and virtual environments.

\subsection{Virtual Reality Interface}
\label{subsec:eval_vr}
The VR interface meets the requirement to view the digital twin in 3D space (\hyperref[req:FR-16]{FR-16}) by placing the operator directly inside the simulated environment. The environment is rendered at a 1:1 scale, allowing for a natural perception of the robot and its surroundings. Users can move through physical walking or joystick control, with discrete rotation steps added to reduce simulation sickness.

Interaction with the environment is facilitated by a ray-casting mechanism controlled by the VR handheld controllers, as shown in Figure~\ref{fig:vr_interface_fpv}. When the input is triggered, a visual ray projects from the controller, serving as a tool for orientation and targeting. This mechanism is mainly used for two tasks: painting on the virtual ground and setting navigation goals for the ROS 2 navigation stack.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/vr_view.png} % Placeholder filename
    \caption{First-person view of the VR interface. The user places objects in the virtual environment and utilizes a ray-cast from the controller to paint on the virtual ground.}
    \label{fig:vr_interface_fpv}
\end{figure}

To support dynamic scenario modification (\hyperref[req:FR-13]{FR-13}), a storage area is located adjacent to the robot's operating field. This area contains various manipulatable objects, including boxes and cuboids in different sizes and colors. A distance grab mechanism allows the user to pick up objects from a distance by pointing the controller at the target and pressing the grip button. This action snaps the object directly to the virtual hand, so the operator does not need to crouch to retrieve items. Users can take these objects and place them into the active simulation area to create obstacles or targets. The physics engine provides realistic collision handling. The placed objects can act as physical barriers that the robot must either push or navigate around.

Data visualization in VR is handled through two primary elements, as depicted in Figure~\ref{fig:vr_overview}. First, a floating status billboard is attached to the robot’s digital twin. This billboard is designed to always face the user’s camera view, making sure that telemetry data is readable from any angle. Second, a large virtual monitor is positioned behind the robot's operation area. This monitor subscribes to image topics from the ROS 2 network, such as the raw camera feed or debug output from the perception stack. Users can switch the displayed video source by pressing a specific key on the keyboard.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/vr_storage.png} % Placeholder filename
    \caption{ Overview of the Virtual Reality environment. The storage area containing manipulatable objects is visible next to the robot’s play area. A large virtual monitor displays the live camera stream from the ROS 2 network.}
    \label{fig:vr_overview}
\end{figure}
\subsection{Augmented Reality Projection}

\label{subsec:eval_ar}

The AR projection meets requirement \hyperref[req:FR-15]{FR-15} by mapping the virtual environment directly onto the physical floor. This setup allows observers without VR equipment to monitor the robot’s internal status and its virtual context in real time. The projection aligns well with the physical robot, especially in the center of the tracking area. There is some drift at the outer edges due to lens distortion in the camera of the ARuco tracking system, but the accuracy is still good enough for evaluating navigation tasks.
Like the VR interface, the AR system displays a status billboard and a video feed plane. These elements are placed at a fixed distance behind the physical robot, as shown in Figure~\ref{fig:ar_projection}, to prevent the projection from being occluded by the robot's chassis or casting shadows. To ensure the visual clarity of the physical testbed for external observers or to modify the visual scene, the system allows for the selective activation and deactivation of individual components such as the digital body of the robot, light sources, and information panels via keyboard commands.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/ar_projection.jpg} % Placeholder filename
    \caption{The Augmented Reality projection in the physical laboratory. The system projects the virtual environment, the status billboard, and the live camera feed onto the floor, following the movement of the physical EMAROs robot.}
    \label{fig:ar_projection}
\end{figure}
\section{Scenario Realization and Application Analysis}
\label{sec:scenario_analysis}

To showcase the system’s flexibility, four different scenarios were created. These test cases confirmed the environmental capabilities and the logic of the robot. Additionally, the system verified that it could load these various scenarios in real time, showing it can switch environments without restarting the main application (\hyperref[req:FR-06]{FR-06}).

Throughout all applications, the fundamental integration between the physical robot and the simulation was maintained through ROS 2 communication and the usage of a \texttt{/clock} topic for time synchronization (\hyperref[req:FR-04]{FR-04}, \hyperref[req:FR-05]{FR-05}).  The robot’s physical state was continuously reflected by its digital twin, ensuring that spatial relationships and coordinate transformations were consistent between the real and virtual worlds (\hyperref[req:FR-07]{FR-07}, \hyperref[req:FR-10]{FR-10}).  A system-wide safety watchdog operated during the entire evaluation. If the robot left the designated tracking area, the system issued a zero-velocity command to avoid the robot from leaving the tracking area (\hyperref[req:FR-08]{FR-08}).

\subsection{Application 1: Interactive Path Following}
\label{subsec:eval_path_following}

The Paint scenario, shown in Figure~\ref{fig:path_following_ar} evaluates the system's capacity for environmental modifications (\hyperref[req:FR-13]{FR-13}). In this application, the robot follows a line trajectory that the user draws onto the floor surface. The system ensures that paths drawn via VR controllers or mouse inputs are reflected in the texture data, enabling the robot to perceive and follow the line.

The robotic application used in this scenario is a camera-based Line Follower. The robot operates solely on the live video feed from its virtual camera to detect the path (\hyperref[req:FR-11]{FR-11}). The control logic is based on a finite state machine, which autonomously switches between searching for visual features, aligning its heading with the path vector, and executing a PID-controlled following behavior based on the sensor data (\hyperref[req:FR-19]{FR-19}).

The robotic application exhibited robust performance during the evaluation. The PID controller kept the robot on the line with minimal deviation. If the path ended or was erased by the user, the robot smoothly transitioned to a search state, looking for new visual inputs. When a new line was created, the perception system quickly detected it, allowing the robot to re-adjust its heading and continue tracking.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/paint_scenario_ar.jpg} % Placeholder
    \caption{Augmented Reality projection of the Paint scenario. The physical EMAROs robot follows a red path drawn by the user.}
    \label{fig:path_following_ar}
\end{figure}

\subsection{Application 2: Arcade Game}
\label{subsec:eval_pong}

The Pong scenario, shown in Figure~\ref{fig:pong_scenario}, evaluates dynamic interactions between the physical robot and virtual physics objects. The robot acts as a paddle in a simulated arcade game. A \texttt{PaddlePlank} script on the digital twin model creates a kinematic collider that reflects the physical movement and deflects the virtual ball. To maintain consistent physics (\hyperref[req:FR-02]{FR-02}), the system detects collisions using PhysX but calculates the ball's reflection vector separately from the physics engine. This prevents energy loss from simulated friction through the PhysX engine and keeps the ball's speed constant.

The human operator acts as the high-level planner for this application. By observing the virtual ball's path in VR or AR, the operator sets navigation goals through the \texttt{NavigationGoalController} to position the robot for interception (\hyperref[req:FR-17]{FR-17}).

The integration of the navigation stack proved effective for steering the robot and the interception task. The robot consistently reached its set goals, and the virtual paddle on the digital twin deflected the ball, showing realistic collisions behavior (\hyperref[req:FR-13]{FR-13}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/pong_scenario.jpg} % Placeholder
    \caption{Overview of the Pong scenario. The user steers the EMAROs robot to reflect the virtual ball via the navigation stack.}
    \label{fig:pong_scenario}
\end{figure}


\subsection{Application 3: Autonomous Logistics}
\label{subsec:eval_logistics}

The Logistics scenario, illustrated in Figure~\ref{fig:logistics_ar}, assesses the system's support for complex, multi-stage missions involving object manipulation and decision-making. The environment simulates a warehouse filled with colored transport boxes and delivery zones. Specific C\# components handle the game logic: \texttt{BoxEquipment} assigns semantic types (e.g., Red or Blue) to objects, while \texttt{DeliveryZone} scripts monitor specific areas. When a box enters a zone, the script checks if the object's type matches the zone's requirements. If the delivery is correct, the box is removed (\hyperref[req:FR-12]{FR-12}).

The robotic application operates through a state machine that coordinates a sequence of autonomous behaviors. First, the robot utilizes its navigation stack to explore the environment and identify target objects via the perception system. Once a target is located, the robot approaches it and initiates a pickup sequence. This triggers an attachment command, which the \texttt{AttachmentCommandManager} processes to kinematically lock the virtual box to the robot's chassis for transport (\hyperref[req:FR-13]{FR-13}).

 The autonomous agent successfully completed the entire logistics process without any human help. The vision-based control logic accurately aligned the robot with the target boxes, and the mechanism for attaching and detaching objects worked correctly. Moreover, the robot's spatial memory logic was effective. After finding a delivery zone of a specific color once, the agent stored the coordinates and was able to navigate directly to them in later runs involving boxes of the same color. The robot's navigation stack also successfully detected dynamic obstacles placed by the VR user and adjusted its trajectories in real time.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/logistics_ar.jpg} % Placeholder
    \caption{Augmented Reality projection of the Logistics scenario. The robot navigates through the projected warehouse to transport a virtual box to the designated colored zone. The environment allows the VR user to dynamically place obstacles, forcing the robot to replan its path.}
    \label{fig:logistics_ar}
\end{figure}

\subsection{Application 4: Smart Farming}
\label{subsec:eval_farming}

The Farm scenario in Figure~\ref{fig:smart_farming_ar}, serves as the most comprehensive integration test. It combines navigation, complex state management, and tool-based environmental modification. The scenario has a \texttt{FieldTileSystem} that manages the visual state of an agricultural field. It handles transitions between stages like "Untouched", "Seeded", and "Growing". The robot operates interchangeable tools such as a Plow, a Seeder, and a Harvester to modify the field state. These tools operate with the \texttt{FarmEquipmentBase} script, which uses multiple raycasts to detect the specific tiles beneath the robot (\hyperref[req:FR-13]{FR-13}). The state of the tiles only changes when the tool is attached and active.

The robotic application follows a sequential workflow. This includes field mapping, tool searching and attachment, and path planning. The robot must find field boundaries on its own using edge detection. It also needs to swap tools and drive precise rows to process the field

The application demonstrated high reliability in field boundary detection. The perception system consistently distinguished the transition between the field texture and the outer floor, enabling the robot to trace the perimeter and accurately map the field's four corners. Based on this layout, the coverage path planner generated valid waypoints, and the navigation stack executed the trajectory with sufficient precision to ensure full coverage. Additionally, the robot independently executed the tool exchange sequence. It located the required implements, stored their coordinates, and returned each tool to its original position before retrieving the next one. The evaluation confirmed that the tools modified the field only when physically attached to the robot, correctly updating the terrain state as the robot progressed. Furthermore, the robot handled user interference effectively. When the VR operator placed obstacles in the field, the robot successfully navigated around them or skipped unreachable waypoints (\hyperref[req:FR-17]{FR-17}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/smart_farming_ar.jpg} % Placeholder
    \caption{AR projection of the Farm scenario. The EMAROs, fitted with the Seeder tool, moves through the virtual field and modifies the ground texture.}
    \label{fig:smart_farming_ar}
\end{figure}

\section{Hardware and System Specifications}
The performance of the Mixed Reality simulation relies heavily on the hardware. It must handle physics calculations, ROS 2 communication, and stereoscopic rendering at once. The benchmarks for this thesis were done on a desktop workstation, while the original VERA framework by Gehricke~\cite{Geh24} was measured on a high-performance laptop. The different hardware configurations are shown in Table~\ref{tab:hardware_specs}.

\begin{table}[H]
    \centering
    \caption{Comparison of hardware configurations used for evaluation.}
    \label{tab:hardware_specs}
    \begin{tabular}{lll}
        \toprule
        \textbf{Component} & \textbf{This Work} & \textbf{Original VERA~\cite{Geh24}} \\ 
        \midrule
        CPU    & Intel i7-8700K                 & Intel i9-14900HX \\
        GPU    & NVIDIA RTX 2070 Super (8 GB)   & N/A (CPU-based rendering) \\
        RAM    & 16 GB                         & 32 GB \\
        OS     & Windows 11 / WSL 2 (Ubuntu 24.04) & Ubuntu 24.04 \\
        \bottomrule
    \end{tabular}
\end{table}
\cite{check if correcte hw und cpu based rendering}
%#############################################################fertig############################################################
\section{Performance Analysis of Robotic Scenarios}
\label{sec:scenario_performance}

To evaluate the system's performance under realistic operating conditions, a granular profiling analysis was conducted during the execution of the four target scenarios described in Section~\ref{sec:scenario_analysis}. A custom \texttt{PerformanceBreakdownLogger} script utilized the Unity \textit{ProfilerRecorder} API to capture the Main Thread execution time of three key subsystems: Physics (\texttt{Physics.Simulate}), Rendering (\texttt{Camera.Render}), and Scripts (\texttt{Update.ScriptRunBehaviour}). 

This data provides the primary validation for the frame rate requirement \hyperref[req:NFR-02]{NFR-02}, demonstrating the system's ability to maintain real time performance during complex robotic tasks. Figure~\ref{fig:component_breakdown} visualizes the distribution of CPU time, while Table~\ref{tab:component_data} details the specific execution metrics.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/compinente break down.png}
    \caption{Performance breakdown illustrating the CPU execution time for Physics, Rendering, and Scripts across the four operational scenarios. The total active frame time remains well below the 16.67 ms threshold required for 60 FPS.}
    \label{fig:component_breakdown}
\end{figure}

The data reveals that the system performs efficiently across all scenarios. The total active CPU time, which is the sum of physics, rendering, and script execution, averages between 2.7 ms and 3.8 ms. It is important to note that while the VR hardware operates at a fixed 72 Hz refresh rate, which requires a frame every 13.88 ms, the software completes its actual calculations in under 4 ms. Consequently, the CPU remains idle for the remaining $\approx$10 ms of each frame interval, waiting for the vertical synchronization (V-Sync) signal\cite{lavalle2023virtual}. This substantial computational headroom validates that the system can handle even more complex logic or additional sensor processing before dropping below the hardware's native frame rate.

Script execution constitutes the largest portion of the active frame time, ranging from 2.5 ms in the \hyperref[subsec:eval_path_following]{Paint} scenario to over 3.5 ms in the \hyperref[subsec:eval_farming]{Farm} scenario. This processing load stems from the C\# logic responsible for ROS 2 serialization, message handling, and state management (\hyperref[req:FR-04]{FR-04}). Notably, in the \hyperref[subsec:eval_pong]{Pong} and \hyperref[subsec:eval_farming]{Farm} scenarios, the Digital Twin mode demonstrates marginally higher execution times (e.g., 3.55 ms versus 3.35 ms in Farm) than the Virtual Mode. This finding suggests that the overhead for synchronizing the digital twin with the pose tracking system (\hyperref[req:FR-07]{FR-07}) is computationally comparable to executing the physics-based motion control of the virtual robot.

\begin{table}[H]
    \centering
    \caption{Average execution time and memory usage over a 60-second capture period.}
    \label{tab:component_data}
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Scenario} & \textbf{Mode} & \textbf{Physics (ms)} & \textbf{Render (ms)} & \textbf{Scripts (ms)} & \textbf{Memory (MB)} \\ 
        \midrule
        Paint & DT Mode & 0.215 & 0.009 & 2.523 & 4374.05 \\
         & Virtual Mode & 0.251 & 0.008 & 2.532 & 4643.03 \\
        \midrule
        Pong & DT Mode & 0.210 & 0.012 & 3.342 & 4459.98 \\
         & Virtual Mode & 0.187 & 0.008 & 2.746 & 4664.51 \\
        \midrule
        Logistics & DT Mode & 0.212 & 0.008 & 3.313 & 4433.06 \\
         & Virtual Mode & 0.204 & 0.008 & 3.189 & 4626.84 \\
        \midrule
        Farm & DT Mode & 0.245 & 0.011 & 3.554 & 4379.40 \\
         & Virtual Mode & 0.220 & 0.007 & 3.352 & 4632.64 \\
        \bottomrule
    \end{tabular}
\end{table}

Conversely, the Physics and Render components contribute negligibly to the CPU frame time (averaging $<$0.25 ms and $\approx$0.01 ms respectively). The extremely low CPU render times confirm that the rendering workload is successfully offloaded to the GPU. This is a significant improvement over Gehricke's Pygame implementation, where rendering times averaged 3.75 ms on the CPU~\cite{Geh24}. \cite{Check ob ide werete correct in ghreicje}
%#########################################################################################################################
\section{Deep Profiling and Script Impact Analysis}
\label{sec:deep_profiling}

To understand why the Scripts consume the majority of the frame budget in the operational scenarios, a Deep Profiling session was conducted. Unlike standard profiling, which only captures high-level timing data, Deep Profiling instruments every C\# method call to provide a granular breakdown of the execution stack~\cite{UnityDocumentation}. 

It is important to note that Deep Profiling introduces significant overhead, inflating the execution time of small methods by a substantial factor. Consequently, the absolute values presented in this section should not be interpreted as standard editor execution, but rather as relative indicators of computational complexity.

\begin{table}[H]
    \centering
    \caption{Relative impact of key scripts during a Deep Profiling session. The data highlights the image processing pipeline (Camera/Material) as the primary consumer of resources, while navigation logic remains highly efficient.}
    \label{tab:script_impact}
    \begin{tabular}{lrr}
        \toprule
        \textbf{Script / Component} & \textbf{Deep Profile Time (ms)} & \textbf{\% of Script Budget} \\ 
        \midrule
        \texttt{CameraSensor.Update} (Section~\ref{subsec:sensor_simulation}) & 7.98 & 68.2\% \\
        \texttt{RosImageToMaterial.Update} (Section~\ref{subsec:sensor_data_projection}) & 1.68 & 14.4\% \\
        \texttt{XRInteractionManager} (Section~\ref{subsec:interactive_control_interfaces}) & 1.18 & 10.1\% \\
        \texttt{PoseRobotController.FixedUpdate} (Section~\ref{sec:robot_representation_and_control}) & 0.59 & 5.0\% \\
        Other Logic Scripts (Combined) & $<$ 0.10 & $<$ 1.0\% \\
        \bottomrule
    \end{tabular}
\end{table}

As summarized in Table~\ref{tab:script_impact}, the analysis identifies the image processing pipeline required for \hyperref[req:FR-11]{FR-11} Sensor Simulation and \hyperref[req:FR-15]{FR-15} AR Projection as the most demanding subsystem. The \texttt{CameraSensor.Update} method accounts for the majority of the script execution time. This cost is attributed to the \texttt{Texture2D.ReadPixels} operation required to transfer the rendered frame from the GPU to the CPU memory for serialization into a ROS 2 message. Similarly, the \texttt{RosImageToMaterial.Update}, which performs the inverse operation of uploading received textures to the GPU for AR projection, represents the second-largest cost.

In stark contrast, the robotic logic and environmental interaction scripts exhibited a negligible computational footprint. The components detailed in the Implementation chapter, including the \texttt{LidarSensor} for raycasting, \texttt{TrackPainter} for texture modification, \texttt{ClockPublisher} for time sync, and \texttt{DynamicObjectManager}, all registered execution times below 0.05 ms even under deep profiling instrumentation. 

For example, the \texttt{PoseRobotController}, which handles the critical synchronization of the digital twin's transform \hyperref[req:FR-07]{FR-07}, consumed only 0.59 ms in the physics loop. This confirms that the architectural bottleneck of the system lies entirely in the memory bandwidth required for video streaming, while the core simulation logic, physics interactions \hyperref[req:FR-02]{FR-02}, and ROS~2 state synchronization are more efficient.
%#########################################################################################################################
\section{Scalability and Stress Testing}
\label{sec:scalability}

While the previous sections validated the performance of the specific application scenarios, this section evaluates the theoretical limits of the system architecture. A stress test was conducted by measuring the Main Thread CPU time while incrementally increasing the number of interactive objects in the scene to validate \hyperref[req:NFR-04]{NFR-04} (Scalability).

To ensure reproducible results, objects were spawned in fixed batches followed by a one-second settling phase. The benchmarks were conducted across four configurations to isolate the costs of VR and dynamic physics. The resulting data is visualized in Figure~\ref{fig:scalability_chart}, using a logarithmic scale to capture the wide variance in frame times.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/scalability_comparison.png}
    \caption{Scalability stress test illustrating CPU frame time vs. object count. The 60 FPS target (16.67 ms) serves as the threshold for visual stability in Mixed Reality.}
    \label{fig:scalability_chart}
\end{figure}

The evaluation of settled objects (\textit{Non-VR Settled})on a desktop display demonstrates the engine's rendering efficiency, maintaining a frame time between 3.12 ms and 3.68 ms for up to 9,000 objects. Gehricke~\cite{Geh24} reported an average update time of approximately 3.75 ms for his 2D Pygame visualizer, with spikes occurring around 11,250 objects. The proposed Unity-based architecture matches and slightly improves upon this baseline, despite rendering a full 3D environment. \cite{chekc ob values pasen}

When activating Virtual Reality (\textit{VR Settled}), the frame time stabilizes at approximately 13.8 ms. This constant overhead is characteristic of the stereoscopic rendering pipeline and the synchronization required for the 72 Hz display of the Meta Quest Pro (\hyperref[req:FR-03]{FR-03}). As established in the scenario analysis, this value primarily represents V-Sync wait time rather than active computation limits. In this static state, the system successfully maintains the required 60 FPS threshold for up to 30,000 objects, significantly exceeding the limit observed in the original VERA framework~\cite{Geh24}.

However, the evaluation of dynamic physical interactions (\textit{Active}) reveals the computational bottleneck. Unlike the previous work which relied on simple distance heuristics, this system simulates mass, inertia, and friction using Nvidia PhysX. While rendering scales linearly, the cost of resolving thousands of simultaneous physical collisions increases exponentially. The system remains performant for up to 3,000 dynamic objects, but exceeds the 16.67 ms threshold beyond 5,000 objects. This limit far exceeds the requirements of the implemented scenarios, such as the \hyperref[subsec:eval_farming]{Farm} or \hyperref[subsec:eval_logistics]{Logistics} applications, which contain fewer than 50 dynamic objects.
%#########################################################################################################################
\section{Latency Analysis}
\label{sec:latency_analysis}

System responsiveness was evaluated by measuring the visualization latency to validate \hyperref[req:NFR-01]{NFR-01} (Low Latency). This metric is defined as the time elapsed between the reception of a pose update from the ArUco tracking system and the completion of the rendered frame displaying that update. A custom \texttt{LatencyMonitor} script was utilized to capture timestamps across the four operational scenarios.

Figure~\ref{fig:latency_chart} illustrates the recorded latency for both the standalone AR Projection configuration and the combined AR and VR operation. When utilizing the AR Projection exclusively, the system demonstrates high responsiveness, with average latencies ranging from 3.02 ms in the simple \hyperref[subsec:eval_path_following]{Paint} scenario to 4.41 ms in the complex \hyperref[subsec:eval_farming]{Farm} environment.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/latency.png}
    \caption{Visualization latency measured across four application scenarios. The chart compares the delay between pose reception and frame completion in AR Projection mode versus Virtual Reality mode. Error bars indicate the jitter (min/max range).}
    \label{fig:latency_chart}
\end{figure}

These results represent a significant improvement over the original VERA framework. Gehricke reported a collision-to-visualization latency of approximately 43.8 ms for an environment with 800 objects~\cite{Geh24}. Furthermore, Gehricke identified a critical scalability issue where the message queue for marker updates became saturated in environments with over 1,250 objects, leading to latencies spiking into the range of several seconds.

The proposed architecture eliminates this bottleneck. By utilizing the native \texttt{Ros2ForUnity} plugin, which communicates directly via the RCL layer without the overhead of Python callbacks used in the previous work, the system maintains a deterministic latency profile. Even in the complex \hyperref[subsec:eval_farming]{Farm} scenario, the Non-VR latency remains below 5 ms, satisfying the low-latency requirement (\hyperref[req:NFR-01]{NFR-01}) essential for Robot-in-the-Loop synchronization.

Activating Virtual Reality introduces a consistent latency overhead, increasing the average processing time to approximately 9.4 ms for Paint and up to 10.7 ms for the Farm scenario. This increase is attributed to the computational cost of stereoscopic rendering and the synchronization constraints of the VR headset for V-Sync. Despite this increase, the total visualization latency remains well below the approximately 20 ms motion-to-photon threshold required to prevent motion sickness in VR~\cite{Chang20102020, Yang2019}, validating the system's suitability for immersive Robot-in-the-Loop interaction (\hyperref[req:FR-16]{FR-16}, \hyperref[req:NFR-02]{NFR-02}).