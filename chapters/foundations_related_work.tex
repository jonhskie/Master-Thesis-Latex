\chapter{Background and Related Work}
\label{chap:background}
Einführung in das kapitel \cite{NOT FINAL}
 \section{Robot-in-the-Loop-Testing}
\label{sec:RitL}
The validation and verification of modern Robotic and Autonomous Systems (RAS) is a significant challenge due to their complexity \cite{AMV23}. This is because they integrate software, mechanical, and electrical engineering all at once \cite{AMV23}. A central problem is ensuring that the software and hardware work together seamlessly, especially when testing both simultaneously \cite{AMV23}. The X-in-the-Loop (XitL) simulation paradigm addresses this challenge \cite{Brayanov2019}. It offers a way to combine the flexibility of software simulation with the realism of physical experiments \cite{Brayanov2019}.

A foundational XitL method is Hardware-in-the-Loop (HIL) simulation, which is a technique for testing mechatronic systems \cite{Mihalic2022, Brayanov2019}. The main principle of HIL is creating a closed loop between the real hardware that is being tested and a simulation that represents the rest of the system or its operational environment \cite{Mihalic2022}. This setup effectively tricks the hardware into behaving as if it were operating in a real system, which allows for testing across a wide range of virtual scenarios \cite{Brayanov2019}. The main reason for using HIL is to shorten development cycles and prevent costly or dangerous failures by making exhaustive testing possible before the system is actually completely implemented \cite{Mihalic2022}.This is why HIL is essential in industries like automotive, aerospace, and robotics, where real-world testing can be too expensive, dangerous, or even impossible \cite{Brayanov2019}.

Robot-in-the-Loop (RitL) \cite{Mihalic2022} simulation extends the Hardware-in-the-Loop (HIL) concept. Instead of just a component, the hardware under test is a complete robotic system, such as an uncrewed vehicle \cite{Mihalic2022}. RitL replaces components of an pure simulated setup with the actual robot, increasing the realism of testing \cite{Hu05}. \cite{NOT FINAL}As illustrated in Figure \ref{fig:ritl_concept}\cite{NOT FINAL}, a typical RitL configuration has the robot's real actuators operating in the physical world, while while its sensors interact with a simulated environment instead \cite{Hu05, Mihalic2022}. This creates a hybrid setup where, for example, the robot might use virtual sensors to see objects in the simulation but use its real motors to move physically \cite{Hu05}. To keep the physical and virtual worlds synchronized, the real robot often has a virtual counterpart in the simulation which state is updated as the physical robot acts and moves \cite{Hu05}.

\begin{figure}[h]
\centering
% \includegraphics[width=0.8\textwidth]{path/to/ritl_concept_diagram.png}
\caption{\cite{NOT FINAL}The real robot's actions affect its virtual counterpart, and the virtual environment provides sensor data back to the real robot.\cite{NOT FINAL}}
\label{fig:ritl_concept}
\end{figure}

The main benefit of RitL is that it uses the dynamics of actual hardware for repeatable testing of high-level software, such as navigation algorithms \cite{Mihalic2022}. This method avoids the expense, complexity, and risk of full physical testbeds while being a secure and useful substitute \cite{Mihalic2022}. RitL becomes therefore an tool for safely evaluating system performance in the lab \cite{Hu05, Mihalic2022}. This is especially important when working on projects that are hard to replicate, such as large-scale robot swarms or Mars rover missions, or when safety is at risk \cite{Hu05, Mihalic2022}.

The RitL paradigm has been widely applied to validate complex autonomous systems, particularly in the automotive field using Vehicle-in-the-Loop (VitL) testing. For example, the Dynamic Vehicle-in-the-Loop (DynViL) architecture integrates a real test vehicle with the high-fidelity CARLA simulator, which operates on the Unreal Engine \cite{DSR22}. As shown in Figure \ref{fig:vitl_setup}, this approach allows a vehicle on an empty track to be stimulated by sensor data from a virtual world \cite{DSR22}. This lets automated driving functions to be safely and reliably tested in situations that would be hazardous to physically replicate \cite{DSR22}. CARLA and the Unreal Engine are also used in a similar Vehicle-in-Virtual-Environment (VVE) method which establishes a closed loop in which the motion of the real vehicle is tracked and reflected in the virtual world, making it possible its control systems to respond to simulated events \cite{Cao2023}.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{images/vehicle_in_the_loop_dsr+22.jpg}
\caption{An example of a Vehicle-in-the-Loop (VitL) setup. A real car on a test track connected to a high-fidelity simulator like CARLA that generates virtual traffic and sensor data. \cite{DSR22}}
\label{fig:vitl_setup}
\end{figure}

These examples show a trend towards using game engine based simulators to evaluate autonomous vehicles. This method falls somewhere between physical testing, where safety and consistency can be problematic, and pure software simulation, which frequently lacks vehicle dynamics fidelity \cite{Cao2023}. Some systems even stimulate the vehicle’s actual sensors. For instance, the Radar Target Simulator (RTS) can feed artificial radar echoes from a virtual scene to the vehicle's actual radar sensor \cite{Diewald2021}. This allows for end-to-end validation of the entire perception and control pipeline \cite{Diewald2021}.

The X-in-the-Loop approach is found in many different areas, not just a single field. In marine robotics, a VIL framework was developed to test the long term autonomy of a robot swarm. In this system, an Autonomous Surface Vehicle (ASV) interacts with multiple simulated underwater sensor nodes to test cooperative behaviors without the logistical cost of deploying a full swarm \cite{Babic2020}. In the aerospace domain, the RFlySim platform uses an FPGA-based HIL system to create a high fidelity simulation for testing UAV autopilot systems in a lab, which reduces the need for expensive and risky outdoor flights \cite{Dai2021}.

These approaches continue to move toward deeper integration. This includes the introduction of Scenario-in-the-Loop (SciL) frameworks that aim to completely blur the lines between real and virtual testing \cite{Szalay2021}. These systems rely on creating detailed digital twins of the entire test environment and combining real and virtual components to run complex, mixed reality test scenarios \cite{Szalay2021}.

\section{Digital Twins}
\label{sec:DT}
The Digital Twin (DT) is an important concept in many current X-in-the-Loop frameworks. The DT serves as the virtual counterpart to a physical system, acting as a virtual copy that allows real-time monitoring and simulation \cite{AA23}. The idea is to create a digital information model of a physical system that stays linked to it for the duration of its lifecycle \cite{Grieves2017}.

This concept was initially known as the Information Mirroring Model and consists of three core elements: the physical product, its corresponding virtual model, and the data connection that connects them \cite{Grieves2017, AA23}. A diagram of this structure is shown in Figure \ref{fig:dt_concept}. The virtual model is more than a basic geometric shape, it is often a detailed simulation that can model mechanical, electrical, and software properties of a system \cite{Leng2021}. Information moves in both directions, so sensor data can flow from the real world to update the virtual model \cite{Grieves2017, Leng2021}. In turn, the virtual model can also send commands back to control or optimize the physical system \cite{Grieves2017, Leng2021}. This continuous, bidirectional data exchange characterizes a Digital Twin \cite{AA23}.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{images/info_mirror_model_Grieves15.png}
\caption{The foundational concept of a Digital Twin, illustrating the three core components: the physical entity, the virtual model, and the bi-directional data connection that links them \cite{Grieves2017, Leng2021}. \cite{Grieves15}}
\label{fig:dt_concept}
\end{figure}

Digital Twins in robotics are frequently created using simulation software and the middleware that connects the virtual simulation to the actual hardware is often the Robot Operating System (ROS) \cite{MFG22}. Stączek et al. used the Gazebo simulator with ROS to create a DT of a factory floor, which they used to test and optimize the navigation algorithms of a mobile robot in narrow corridors \cite{Staczek2021}.  To validate a deep learning-based harvesting robot, R. Singh et al. adopted a similar strategy and created a DT of a greenhouse in Gazebo \cite{Singh2024a}. Other simulators like CoppeliaSim and Webots are also common. Magrin et al. used CoppeliaSim and ROS to create a DT as a learning tool for mobile robot design \cite{Magrin2021}, while Marques et al. used Webots for an Automated Guided Vehicle (AGV), synchronizing it via an OPC-UA server \cite{Marques2024}. These examples show a common approach of using simulators to simulate robot kinematics and sensor feedback for closed-loop testing.

More recently, game engines have become a popular choice for creating Digital Twins, as they offer more realistic graphics and physics. Unity \cite{Uni23}, for instance, is used developers to build virtual environments that are highly realistic. \cite{NOT FINAL}This can be seen in Figure \ref{fig:dt_environments}\cite{NOT FINAL}. Pérez et al. used Unity to develop a DT of a multi-robot cell with a Virtual Reality (VR) interface for virtual commissioning and operator training \cite{Perez2020}.

\begin{figure}[h]
\centering
% \includegraphics[width=0.9\textwidth]{path/to/dt_environments_comparison.png}
\caption{\cite{NOT FINAL}A comparison of robotic Digital Twin environments, showing a view from a traditional robotics simulator (left) versus a high-fidelity visualization from a modern game engine like Unity (right).\cite{NOT FINAL}}
\label{fig:dt_environments}
\end{figure}

Another important consideration are the technical capabilities these engines. Yang et al. \cite{Yang2020} simulated the physics of a UAV using Unity and its built-in NVIDIA PhysX engine. They also demonstrated the creation of virtual sensors, such as a LiDAR, directly within the game engine using its raycasting API \cite{Yang2020}. Research has also been done on the performance and reliability of these game engine-based frameworks. Kwon et al. developed a safety-critical DT in Unity and ROS 2, reaching a data-transmission latency of 13 ms for predicting collisions \cite{Kwon2025}. Similarly, M. Singh et al. created a DT using ROS and Unityand and conducted performance validation with a communication latency of 77.67 ms and a positional accuracy of 99.99\% \cite{Singh2024b}.

\section{Mixed Reality}
\label{sec:MR}
Beyond the testing paradigm and the digital twin, the way a user interacts with the system is another part of a modern robotics framework. Virtual, Augmented, and Mixed Reality (VAM) have become promising technologies for improving the information exchange between humans and robots \cite{Walker2023}. In robotics, Augmented Reality (AR) is an especially useful tool for enhancing Human-Robot Interaction (HRI) by integrating 3D virtual objects into a real-world environment in real-time \cite{MV20}.

The relationship between these technologies is formally described by the foundational Reality-Virtuality (RV) Continuum concept, first introduced by Milgram and Kishino \cite{MK94}. As illustrated in Figure \ref{fig:rv_continuum}, this continuum is a scale that is anchored by a purely real environment at one end and a completely virtual one at the other \cite{MK94, Skarbez2021, MV20}.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{images/reality_virtuality_continuum_wpc+23.png}
\caption{The Reality-Virtuality Continuum, illustrating the spectrum from a real environment to a completely virtual one. \cite{Walker2023}}
\label{fig:rv_continuum}
\end{figure}

A Virtual Reality (VR) environment is an endpoint of the continuum, where the user is totally immersed in and can interact with a fully synthetic world \cite{MK94}. This approach is useful for HRI research, as it allows for testing interactions with virtual robots in scenarios where it might be unsafe or too expensive for physical hardware \cite{Walker2023}. The general term Mixed Reality (MR) describes the entire spectrum between the two extremes, where real and virtual worlds are combined into a single display \cite{MK94}. MR is made up of two main categories: Augmented Reality (AR) and Augmented Virtuality (AV) \cite{MK94, MV20}. AR is the process of adding virtual objects to a real environment \cite{MK94}. This lets for example, HRI researchers to place 3D data and intentions of a robot directly into the physical space of an user \cite{Walker2023}. The opposite is Augmented Virtuality (AV), where a primarily virtual world is enhanced with elements from the real world, like live video feeds \cite{MK94}.

\section{Robot Operating System 2 (ROS~2)}
\label{sec:ros2}
ROS~2 is not a operating system but a middleware framework to simplify the creation of complex robotic systems \cite{MFG22}. It was redesigned from scratch to meet the challenges posed by modern robotics in domains ranging from logistics and agriculture to space missions and became the standard for both research and industry \cite{macenski2023survey, MFG22}. At its heart, ROS~2 is designed based on principles of distribution, abstraction, asynchrony, and modularity that together allow the development of scalable and robust applications \cite{MFG22}.
The architecture of ROS~2 is based on a distributed network of independent programs called \textbf{Nodes } \cite{MFG22}. A node is a single, self-contained executable that performs a specific task, such as controlling a motor, processing sensor data, or, in the case of this thesis, bridging communication to the Unity simulation \cite{MFG22}. The nodes communicate through a set of defined patterns. The most common pattern is the publish-subscribe mechanism called \textbf{Topics}, illustrated in Figure \ref{fig:ros2_comm_concept} \cite{MFG22}. In this model, nodes can publish data as messages to a topic, while other nodes can subscribe to that topic to receive data asynchronously \cite{MFG22}. For tasks that require a direct request and a guaranteed response, ROS~2 offers \textbf{Services}, following a synchronous request-response pattern \cite{MFG22}. For long-running tasks that require continuous feedback and the ability to be preempted, ROS~2 provides a unique communication pattern called \textbf{Actions} \cite{MFG22}. An action consists of a goal, a feedback stream, and a final result, making it optimal for managing tasks like navigation where the progress of a robot toward a goal has to be monitored over time \cite{macenski2020marathon2}.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=1.5cm and 2cm, auto, >=stealth]
        % --- NODES USING THE 'align' OPTION FOR ROBUSTNESS ---
        \node[draw, rectangle, rounded corners, align=center] (pub_node) {Node A \\ \textbf{(Publisher)}};
        \node[draw, ellipse, fill=blue!20, right=of pub_node, align=center] (topic) {Topic \\ \texttt{/sensor\_data}};
        \node[draw, rectangle, rounded corners, right=of topic, align=center] (sub_node1) {Node B \\ \textbf{(Subscriber)}};
        \node[draw, rectangle, rounded corners, below=of sub_node1, yshift=-0.25cm, align=center] (sub_node2) {Node C \\ \textbf{(Subscriber)}};

        % Arrows
        \draw[->, thick] (pub_node) -- (topic) node[midway, above] {Message};
        \draw[->, thick] (topic) -- (sub_node1);
        \draw[->, thick] (topic) -- (sub_node2);
    \end{tikzpicture}
    \caption{The ROS~2 publish-subscribe model. Node A publishes messages onto a central topic, and any number of subscriber nodes (B and C) may receive that data without direct knowledge of the publisher.}
    \label{fig:ros2_comm_concept}
\end{figure}

An essential part of any mobile robot is coordinate frame management, which in ROS~2 is accomplished through its transform library, \textbf{tf2} \cite{foote2013tf}. The tf2 library standardizes the tracking of spatial relationships between the various parts of the robot and environment, placing them into a tree-like data structure, as illustrated in Figure \ref{fig:tf_tree_concept} \cite{foote2013tf}. This permits any node in the system to request at any time the position and orientation of any frame relative to another, a feature critical for transforming sensor data into a useful frame of reference \cite{foote2013tf}.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        level 1/.style={sibling distance=3.5cm},
        level 2/.style={sibling distance=2.2cm}, % Adjusted spacing
        edge from parent/.style={draw, -latex},
        every node/.style={draw, rectangle, rounded corners, text centered, font=\small} % Made font slightly smaller for compactness
    ]
    \node {world}
        child { node {robot\_base\_link}
            child { node {camera\_link} }
            child { node {imu\_link} }
            child { node {lidar\_link} }
        };
    \end{tikzpicture}
    \caption{A simplified example of a ROS~2 transform tree (tf tree). The library maintains the hierarchical relationships so that a program can easily determine the transform from the \texttt{camera\_link} to the \texttt{world} frame, for instance.}
    \label{fig:tf_tree_concept}
\end{figure}

In the architecture of this thesis, ROS~2 provides the central \textbf{data backbone}, a term borrowed from digital engineering that defines an integrated communication layer for all relevant system knowledge \cite{perzylo2020backbone}. This is the "glue" for connecting the physical EMAROs robot and its tracking system with the virtual Unity environment. The digital twin of the EMAROs robot subscribes to ROS~2 topics to receive real-time data from the physical world and can synchronize its state through this communication layer \cite{singh2024unity}. In this project, the \texttt{ros2-for-unity} asset of Robotec.AI is utilized \cite{Rob24}. This approach has the particular advantage of not bridging the communication but instead implementing the ROS~2 middleware stack (RCL and below) directly in Unity. This makes entities in the simulation native ROS~2 nodes, letting them respect QoS (Quality of Service) settings and obtain latencies significantly lower than possible with bridged solutions \cite{Rob24}.
For navigation for both the physical and virtual robots, this thesis utilizes \textbf{Navigation2 (Nav2)}, the official, next-generation autonomous navigation framework for ROS~2 ~\cite{macenski2020marathon2}. Nav2 was designed from the ground up to orchestrate planning, control, and recovery tasks using configurable Behavior Trees that are highly modular and can be changed at runtime to create custom navigation behaviors ~\cite{macenski2020marathon2}. Core to its operation, Nav2 separates the navigation task into two closely related, yet distinct components, namely a \textbf{global planner}, which seeks out an acceptable, long-range path through the environment, and a \textbf{local trajectory planner} or controller, which generates velocity commands in order to follow that route while reacting to immediate obstacles ~\cite{macenski2023survey}. Using the Nav2 stack, a high-level goal, e.g. a coordinate in the environment, can be sent via a ROS~2 action to either the physical EMAROs robot, or the purely virtual model, and the system will take care of all complex path planning and collision avoidance autonomously.

\section{The VERA Framework}
\label{sec:vera}

This thesis directly builds on the "Virtual Environment for mobile Robotic Applications" framework, which was developed earlier by Gehricke \cite{Geh24}. VERA was designed initially as a modular and scalable platform to bridge the validation gap between pure software simulation and real-world testing. It combines the concepts Digital Twins, Augmented Reality, and Vehicle-in-the-Loop testing to enable robots to interact with a virtual environment projected into the real world \cite{Geh24}.

The core idea of VERA is projecting a dynamic, interactive virtual world onto the physical floor where a real robot operates. The system synchronizes the virtual state with the actions of the physical robot in such a way that scenarios are possible where the robot can detect and react to virtual obstacles as if they were real. This system provides a flexible testbed for the development and evaluation of robotic applications without having to physically build complex environments \cite{Geh24}.

\begin{figure}[h]
    \centering
    % TODO: Insert Figure 4.1 from Gehricke's Thesis (The VERA hardware setup with projector)
    % \includegraphics[width=0.8\textwidth]{images/vera_setup_placeholder.png}
    \caption{The physical setup of the VERA platform: A projector and tracking system are mounted on a frame above the test area. \cite{Geh24}}
    \label{fig:vera_setup}
\end{figure}

\subsection{The EMAROs Test Platform}
\label{sec:emaros_platform}
The capabilities of VERA are demonstrated by a mobile mini-robot called EMAROs (Educational Modular Autonomous Robot Osnabrück), which was developed at Osnabrück University specifically for research and education in robotics and artificial intelligence \cite{Geh24}.

The modular hardware architecture of EMAROs is based on three main PCBs: a \\textbf{Host-PCB} containing high-level computing, a \\textbf{Power-PCB} managing energy supply and motor control and a \\textbf{Base-PCB} integrating ground sensors \cite{Geh24}. The specific variant used in this work is equipped with a Raspberry Pi Compute Module 4 (CM4) running Ubuntu and the Robot Operating System 2 (ROS~2) \cite{Geh24}.

\begin{figure}[h]
    \centering
    % TODO: Insert Figure 3.1 from Gehricke's Thesis (The EMAROs robot)
    % \includegraphics[width=0.5\textwidth]{images/emaros_robot_placeholder.png}
    \caption{The EMAROs robot is the robotic platform used in the testbed, equipped with a modular sensor suite and running ROS~2 \cite{Geh24}.}
    \label{fig:emaros_robot}
\end{figure}

For environmental perception, the robot is equipped with Time-of-Flight (ToF) distance sensors and an Inertial Measurement Unit (IMU) for orientation tracking \cite{Geh24}. Moreover, there are two wide-angle cameras that can be configured for stereo vision applications or line-following tasks \cite{Geh24}. Within VERA, EMAROs takes on the role of a "Physical Twin," streaming real-time telemetry including odometry, battery status, and sensor data to the simulation framework via ROS~2 topics \cite{Geh24}.

\subsection{Original VERA System Architecture}
The software architecure of the original VERA framework was mainly a ROS~2 implementation, and its structure included only three main components: the \textbf{Virtual Environment Positioning System (VEPS)}, the \textbf{Environment Manager}, and the \textbf{Visualization System} \cite{Geh24}.

Accurate localization is important to synchronize the physical robot with the projected virtual world and was provided by the \textbf{VEPS} in the original VERA framework \cite{Geh24}. The original VEPS implementation relied on a depth-based approach where a ceiling-mounted Allied Vision Ruby 3D stereo camera captured point clouds of the test area \cite{Geh24}. The point clouds were filtered by a Median Absolute Deviation (MAD) algorithm to calculate the centroid of the robot \cite{Geh24}.
However, for maximum robustness and precision for this thesis, the positioning system was updated to use an ArUco marker-based tracking system developed in parallel research by \cite{JuliaBA}. Although the point-cloud method provided a markerless solution, the ArUco-based approach has higher reliability and lower noise under changing light conditions  \cite{JuliaBA,Geh24}. By attaching a fiducial marker to the top of the EMAROs robot, the system is able to calculate the 6D pose (position and orientation) of the robot \cite{JuliaBA}. This pose is published into the ROS~2 network, where it can instantly update the position of the digital twin \cite{JuliaBA}.

\begin{figure}[h]
    \centering
    % TODO: Insert a photo of the ArUco tracking setup or the camera view with markers
    % \includegraphics[width=0.8\textwidth]{images/aruco_tracking_placeholder.png}
    \caption{Visualization of the tracking system. The original VERA used point clouds (left), while the current iteration uses ArUco markers (right) for robust pose estimation \cite{JuliaBA}.}
    \label{fig:veps_comparison}
\end{figure}

The \textbf{Virtual Environment Manager} is the central brain of the simulation \cite{Geh24}. Implemented as a C++ ROS~2 node, it parses environment definitions from standard Gazebo \texttt{world.sdf} files \cite{Geh24}. It keeps track of the state of all virtual objects, including their positions and properties, such as whether an object is movable or static \cite{Geh24}.
One of the key roles of the manager is to manage the interactions of the robot with the virtual world \cite{Geh24}. It operates a continuous loop, typically at 20 Hz, which checks the distance between the position of the robot given by the VEPS and the virtual objects \cite{Geh24}. The manager triggers predefined actions if a collision is detected, such as removing a collectible object or stopping the robot \cite{Geh24}. It relies on basic distance heuristics and not a physics engine, hence limiting interactions to simple ones \cite{Geh24}.

The \textbf{Visualization System} is responsible for rendering the virtual environment so it can be projected onto the physical floor \cite{Geh24}. In the original VERA framework, this was achieved using a custom ROS~2 node built with Pygame, a Python library designed for 2D game development \cite{Geh24}.
The visualizer subscribes to the object lists published by the Manager and the robot's path history \cite{Geh24}. It provides a 2D, top-down, orthogonal view of the scene, drawing obstacles as simple geometric shapes \cite{Geh24}. It gives Augmented Reality features, projecting dynamic information directly into the workspace \cite{Geh24}. The driven of the robot is displayed as a trail, while a system status panel with real-time CPU and battery usage is shown behind the robot \cite{Geh24}.

\begin{figure}[h]
    \centering
    % TODO: Insert Figure 4.11 from Gehricke's Thesis (The Pygame projection with path)
    % \includegraphics[width=0.8\textwidth]{images/pygame_visualizer_placeholder.png}
    \caption{Original VERA visualization via Pygame, showing a projection of the robot's path and virtual obstacles onto the floor. \cite{Geh24}}
    \label{fig:pygame_vis}
\end{figure}

\subsection{Identified Limitations}
While VERA successfully demonstrated the concept of Robot-in-the-Loop testing with AR projections, its original software implementation contains several critical limitations, that this thesis sets out to overcome:
\begin{itemize}
    \item \textbf{Lack of Realistic Physics:} The custom Environment Manager does not feature a physics engine \cite{Geh24}. Collision detection is performed through simple radius checks, meaning the robot cannot push objects or experience friction nor can it interact with complex geometries \cite{Geh24}. This reduces the realism of the Digital Twin \cite{Geh24}.
    \item \textbf{Performance Scalability:} Pygame-based visualizer utilizes CPU for rendering and has performance-related issues for complex scenarios \cite{Geh24}. Gehricke \cite{Geh24} noted that in an environments with a high number of dynamic objects (e.g., $>$800), the message queue became saturated, leading to considerable latency and incomplete visual updates.
    \item \textbf{Limited Interaction (No VR):} The system was designed strictly for 2D floor projections \cite{Geh24}. It does not support any kind of immersive Virtual Reality (VR) interfaces that would enable a human operator to view the scene in 3D or to interact with the robot remotely \cite{Geh24}. Therefore, the integration of Virtual Reality was identified as a necessary future step for Human-Robot Interaction (HRI) \cite{Geh24}.
\end{itemize}

\section{Simulation Engines for Robotics}
\label{sec:sim_engines}
The choice of a capable simulation engine is an important part in solving the limitations of the original VERA framework. The choice of platform affects the efficiency, accuracy, and applicability of the Digital Twin \cite{Singh2025}. Based on this, the review of simulation engines in this thesis is guided by four criteria which are required for a functional Mixed Reality Digital Twin:
\begin{itemize}
    \item \textbf{Visual-Fidelity:} It is not only about rendering quality but to support realistic sensor simulation, such as cameras, and to provide an immersive experience to the user in VR that lacks in traditional robotics simulators \cite{dosSantos2025}.
    \item \textbf{Physics Accuracy:} To accurately predict the kinematic and dynamic behaviour of the EMAROs robot, the engine needs to have a strong physics backend \cite{Singh2025}.
    \item \textbf{Native VR/AR Support:} In order to extend the VERA with the proposed Virtual Reality interfaces, the engine must have a mature framework for VR devices and provide capabilities for creating interactive environments without extensive middleware \cite{Coronado2023}.
    \item \textbf{Robust ROS~2 Integration:} ROS~2 forms the data backbone of the system and should enable low-latency, high-throughput communication to synchronize the digital twin with the physical robot in real-time \cite{Singh2025}.
\end{itemize}

\subsection{Robotics Simulators}
\textbf{Gazebo} is the most used simulator in Industry 4.0 applications because of its deep integration with ROS and and the fact that it is open-source \cite{Singh2025}. It delivers detailed physics simulations along with sensor data emulations, making it ideal for precise engineering applications \cite{Singh2025}. However, while this simulator enjoys benefits from a large community and extensive documentation, there are noticeable drawbacks concerning visualization \cite{dosSantos2025}. For example, the rendering quality of Gazebo is only moderate and not photorealistic to an extent that would enable immersive VR experiences \cite{dosSantos2025}. Moreover, some works note that Gazebo has a steep learning curve for new users due to its complex configuration and less intuitive user interface compared to modern commercial tools \cite{Singh2025, Gonzalez2025}.

\textbf{Webots} well established opensource platform, appreciated for cross-platform support and a large library of robot models \cite{Kargar2024}. Although it is quite effective in educational applications, the rendering capabilities of the software are somewhat dated compared to the current game engines, making it not fitting for high-fidelity Mixed Reality graphics \cite{Kargar2024}.

\subsection{Physics-Based Simulators}
A few other simulators were found in literature but considered less appropriate for the particular VR needs of this thesis. \textbf{CoppeliaSim (previously known as V-REP)} is widely applied in automation, yet it has less physics realism compared to the professional ones, and its scaling is limited \cite{Gonzalez2025}. \textbf{MuJoCo} does well with contact-rich tasks and Reinforcement Learning but works almost exclusively on the stability of the physics and not the visualization, so it lacks the inbuilt VR frameworks necessary for Mixed Reality interaction \cite{Gonzalez2025}.

\subsection{High-Visual-Fidelity and Game-Based Engines}
Game engines have been widely adopted within the robotics community to overcome the visualization limitations of traditional robotics simulators.

\textbf{NVIDIA Isaac Sim} is the state-of-the-art in industrial simulation \cite{dosSantos2025}. The platform is capable of photorealism through ray-tracing and has advanced GPU-accelerated physics through PhysX \cite{dosSantos2025}. It excels in generating synthetic data for AI perception \cite{Kargar2024}. But Isaac Sim has a very high hardware barrier where high-end NVIDIA RTX GPUs are specifically mandatory \cite{dosSantos2025}. Being proprietary, it more restricting for use in educational contexts compared to more open-source or widely accessible game engines \cite{dosSantos2025}.

\textbf{Unreal Engine} is renowned for photorealistic visuals is capable of handling big, open-world environments \cite{Gonzalez2025}. But it has a steep learning curve because of its dependency on C++ and therefore is not very well-suited for rapid prototyping compared to other engines like Unity \cite{Coronado2023}.

\textbf{Unity Engine} is establishing itself as the primary platform for Industry 5.0 applications, in particular those focusing on HRI and MR \cite{Gonzalez2025}. It boasts a balance of high-fidelity graphics with a robust physics engine, PhysX \cite{Singh2025}. Unity is also known for having one of the most mature MR ecosystems, along with a huge asset store, which lowers the barrier to entry for developers\cite{Singh2025, Coronado2023}. Literature points out that the user-friendly interface of Unity and supportive community help with a softer learning curve compared to Gazebo or Unreal, particularly for users without deep C++ expertise \cite{Singh2025, Coronado2023}.
ROS~2 is not natively supported by Unity. Support must be provided through third-party plugins. For the work described in this thesis, the \\textbf{ROS2 For Unity} plugin from Robotec.AI has been used ~\\cite{Rob24}. This asset offers high-performance communication by loading the ROS~2 middleware layer (RCL layer) directly inside the Unity engine \cite{Rob24}. Unlike bridged solutions, simulation entities now can act as native ROS~2 nodes in the context of Unity, thus respecting QoS and reaching lower latencies ~\cite{Rob24}.

\subsection{Comparison and Selection}

Based on this analysis, Unity is selected as the simulation engine. It has an integrated MR framework for Mixed Reality \cite{Coronado2023} and better visualization compared to Gazebo \cite{Gonzalez2025}. It maintains a balance between accessibility and performance, while C\# scripting, combined with large community support, makes learning it simpler than Unreal Engine \cite{Coronado2023}. In addition the \texttt{ros2-for-unity} plugin brings high-performance and low-latency communication \cite{Rob24}.

\begin{table}[h]
\centering
\caption{Comparison of Simulation Platforms for Mixed Reality Digital Twins \cite{Gonzalez2025, Kargar2024, Singh2025, Coronado2023}.}
\label{tab:sim_comparison}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Feature} & \textbf{Gazebo} & \textbf{Isaac Sim} & \textbf{Unreal Engine} & \textbf{Unity} \\ \hline
\textbf{Primary Use Case} & Control \& Navigation & AI \& Photorealism & Photorealism & HRI \& MR (VR/AR) \\ \hline
\textbf{Visual-Fidelity} & Moderate & Very High & Very High & High \\ \hline
\textbf{Physics Engine} & ODE/Bullet/DART & PhysX 5 (GPU) & Chaos/PhysX & PhysX \\ \hline
\textbf{Learning Curve} & Steep & Advanced & Steep (C++) & Moderate (C\#) \\ \hline
\textbf{Community Support} & High (ROS specific) & Moderate & High (Gaming) & Very High (MR/Gaming) \\ \hline
\textbf{ROS 2 Integration} & Native & Bridge & Bridge & Plugin (Native Stack) \\ \hline
\textbf{Hardware Req.} & Low & Very High (RTX) & High & Moderate \\ \hline
\end{tabular}%
}
\end{table}

\section{Synthesis? Discussion?}
\label{sec:synthesis_objectives}

The review of related work confirms that RitL testing \cite{Hu05} combined with Digital Twin technology \cite{AA23} sets up a paradigm for validating robotic systems. Current research highlights the need to move from basic kinematic simulations to high-visual-fidelity, physics-based environments that can more realistically simulate complex interactions and sensor data \cite{SKG+25, DSR+22}.

However, due to its reliance on custom software components, the original VERA framework has significant technical limitations \cite{Geh24}.  As explained in Section 2.5.3, the Pygame-based visualiser experiences performance bottlenecks in complex scenes and realistic object manipulation is impossible due to the lack of a dedicated physics engine \cite{Geh24}. Moreover, the potential for researching Human-Robot Collaboration is limited by the lack of Virtual Reality support \cite{Geh24}.

To overcome these limitations, this thesis proposes integrating a simulation engine into the architecture of VERA using the \textbf{Robot Operating System 2 (ROS~2)} \cite{MFG+22}. The goal is to establish a robust, physics-enabled Mixed Reality environment that bridges the gap between simulation and reality. This new development will be verified by creating demonstration scenarios where the EMAROS robot actively interacts with virtual elements and will serve as a proof of concept for improved system physical realism scalability and immersive interaction.