\chapter{Conclusion and Outlook}
\label{ch:conclusion_outlook}

The growing role of mobile robots in industry, agriculture, and research has led to a rising need for thorough development and testing methods~\cite{AMV23}. Developing these systems often requires a mixed approach, combining the safety of virtual simulation for quick prototyping with the accuracy of physical experiments for final validation~\cite{Brayanov2019}. However, virtual environments often fail to accurately represent complex dynamics like sensor noise and friction~\cite{BM18}. On the other hand, physical testing provides dependable data, but it can be costly and less adaptable, which hinders the investigation of edge cases and the rapid adjustment of experimental conditions~\cite{Hu05, Dos17}.

The Mixed Reality Environment created in this thesis tackles these challenges. It provides a shared workspace for simulation and physical testing. This setup supports a Robot-in-the-Loop platform, where a physical robot interacts with a synchronized virtual world. The system combines the Unity engine with the Robot Operating System 2. This setup allows the robot to see virtual objects as if they were real.

The system uses a Digital Twin to synchronize the physical robot with the simulation. An ArUco tracking system links the robot's position to the virtual environment for positional alignment. This setup enables the physical robot to interact with virtual obstacles. The simulation creates virtual sensors, such as LiDAR scans and camera feeds. This sensor data is sent back into the robot's perception system, enabling the navigation software to react to virtual objects as if they were real.

The system was evaluated using the Paint, Logistics, and Farm scenarios, which covered Line Following, Autonomous Logistics, and Smart Farming tasks. These cases demonstrated that the framework can manage various interactions, like changing surfaces or transporting objects. The successful execution of these diverse tasks validates the versatility of the architecture, demonstrating its capability to support everything from fundamental navigation to complex, multi-stage behaviors. This highlights its usefulness as a testbed for robotic applications.

The system's modular design allows for future improvements. Future efforts should aim to enhance physical realism by enabling the physical robot to react to virtual resistance. Right now, the digital twin acts as a kinematic body that applies infinite force to virtual objects. Implementing feedback would allow the simulation to limit the robot's motor torque or speed during collisions or resistance, mimicking the resistance of heavy obstacles, high-friction surfaces or inclines.

The digital twin could be expanded with virtual extension modules for more complex applications. Virtual tools like robotic arms or agricultural sprayers could connect to the digital twin and be controlled through ROS 2. This would allow researchers to test control software for hardware that isnâ€™t physically present and speed up the design of new tools.

The system could also expand to support multi-robot setups and better visualization capabilities. Adjusting the framework to accommodate multiple real and virtual robots would help validate collaborative algorithms. This change would enable the formation of hybrid swarms, allowing a small group of physical robots to work alongside a large fleet of virtual agents. Additionally, using Passthrough Mixed Reality headsets would extend the visualization beyond the floor plane. Unlike the current projection system, this would allow for the display of three-dimensional structures and vertical information, creating a more immersive and detailed feedback loop for the operator.

In conclusion, this thesis presents a Mixed Reality Environment that connects simulation and physical testing. By integrating game engine technology with ROS 2, the system allows for interaction between real robots and virtual elements through a synchronized Digital Twin. Furthermore, the system improves situational awareness by showing diagnostic data on status billboards and projecting live camera feeds. The evaluation confirms that this setup offers a stable and flexible platform for validating robotic behaviors. Its modular design provides a foundation for future improvements, supporting the development of more complex Robot-in-the-Loop applications.