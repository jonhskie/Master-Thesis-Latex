\chapter{Requirements}
\label{ch:requirements}
\cite{NOT FINAL}
This chapter defines the comprehensive requirements for the Mixed Reality Environment designed for the \textbf{EMAROs} (Educational Modular Autonomous Robot Osnabr√ºck) platform \cite{Geh24}. The primary objective is to create a versatile validation tool for RitL testing \cite{Hu05}. The requirements are categorized into \textbf{Platform Capabilities}, \textbf{System Architecture}, \textbf{Functional Capabilities}, \textbf{Scenario-Specific Logic}, and \textbf{Automated Scenario Execution}. This structure ensures the system supports both interactive user demonstration and fully autonomous robot operation.
\cite{NOT FINAL}

\section{Simulation Engine Capabilities}
\label{sec:platform_reqs}

These requirements define the technical capabilities of the simulation engine that will be used. The criteria provide a basis for the technology selection process in the implementation phase.

\begin{itemize}
    \item \textbf{FR-01: High-Visual-Fidelity Rendering:} The simulation engine must support rendering pipelines to generate image data representative of realworld lighting and material properties. This is important to feed the computer vision algorithms of the EMAROs \cite{Geh24} robot with realistic synthetic input.
    
    \item \textbf{FR-02: Advanced Physics Simulation:} In order to overcome the shortcomings found in the previous VERA framework \cite{Geh24}, the engine must offer a system that can resolve mass, friction, and drag. This should help the simulation create physical interactions, like the robot pushing obstacles.
    
    \item \textbf{FR-03: VR and AR Support:}  The platform needs to offer a mature framework for VR and AR. This makes it possible to integrate VR output and input devices and project 3D content onto physical surfaces without the need to create custom drivers.
    
    \item \textbf{FR-04: ROS Integration:} The engine should support the integration with ROS~2 \cite{MFG22}. Instead of depending on external bridge applications, the simulation should function as a first-party participant in the network to lower communication latency.
\end{itemize}

\section{System Architecture \& Middleware}
\label{sec:architecture_reqs}

The system architecture is defined by the constraints of the EMAROs \cite{Geh24} hardware and the need for standard robotic communication.

\begin{itemize}
    \item \textbf{FR-05: Exclusive ROS 2 Communication:}  The simulation will only use standardised ROS~2 communication to exchange any data, including sensor streams, telemetry, control commands, and system management. This guarantees that the simulation will continue to work with both current and upcoming EMAROs \cite{Geh24} software.
    
    \item \textbf{FR-06: Simulation Time:} The system must act as the simulation time source by publishing a clock signal. To avoid data drift in the robot's control algorithms, all simulated sensors and other publishers must obtain their timestamp from this source.
    
    \item \textbf{FR-07: Dynamic Scenario Management:} During runtime, the system must offer a way to dynamically unload the current environment and load a different scenario via a network command. This capability is required to execute automated test suites that go over multiple environments without restarting the application.
    
    \item \textbf{FR-08: Interface Standardization:} The system shall use the standard robotic message definitions for all outside communications. This guarantees that the real robot can interact with the virtual environment and process simulated sensor data just like it would in the real world.
\end{itemize}

\section{EMAROs Digital Twin Interfaces}
\label{sec:digital_twin_reqs}

The system must provide a digital representation of the EMAROs \cite{Geh24} platform.

\begin{itemize}
    \item \textbf{FR-09: Physical Twin Synchronization:} The system must accept real-time pose updates from an external tracking system in order to synchronize the physical robot with the Digital Twin. Alignment of the collision boundaries is required for interaction with virtual objects.
    
    \item \textbf{FR-10: Tracking Failsafe:} The system must implement a failsafe mechanism that stops the real robot's movement updates if the data stream of the tracking system is interrupted for a defined threshold. This prevents unexpected movements due to signal loss.

    \item \textbf{FR-11: Standalone Simulation Mode:} The system must mimic the kinematic behaviour of the EMAROs \cite{Geh24} robot in the absence of physical hardware. In order to validate navigation algorithms in a fully virtual environment, it must translate velocity commands into movement and publish the resulting odometry.
    
    \item \textbf{FR-12: Transform Publishing:} Dynamic transforms that depict the motion of the robot must be published by the system. It should also support the definition of static sensor offsets either internally within the Unity editor or via external configuration.
\end{itemize}

\section{Perception \& Sensor Simulation}
\label{sec:perception_reqs}

To support SitL testing \cite{VTS21}, the system must simulate a sensor suite similar to that equipped on the EMAROs \cite{Geh24} robot.

\begin{itemize}
    \item \textbf{FR-13: Synthetic Camera Feed:} The system must render the scene from the perspective of the robot's camera and publish it as a video stream. The resolution and update rate must be similar to the specific camera hardware used on the robot.
        
    \item \textbf{FR-14: Laser Scanner Simulation:} The system must simulate a 2D laser scanner by detecting geometry within a defined field of view. This sensor must detect static environment objects and dynamic obstacles to update the robot's navigation map.
    
    \item \textbf{FR-15: Sensor Verification Visualization:} In order to enable users to confirm sensor coverage and mounting orientation during setup, the system must offer a visual debugging mode that shows the active field of view and ray paths of sensors.
\end{itemize}

\section{General Interaction Mechanisms}
\label{sec:interaction_reqs}

These requirements define the general capabilities to interact with the simulation via the network. These mechanisms serve as the foundation for specific scenarios.

\begin{itemize}
    \item \textbf{FR-16: Universal Commands:} The system must provide a network interface to receive commands and return status feedback. This generic interface allows the robot to trigger simulation events.
    
    \item \textbf{FR-17: Object Attachment Capability:} The system must provide a mechanism for the robot to  attach and detach external objects via a command.
    
    \item \textbf{FR-18: Dynamic Obstacle Integration:} The system must provide functionality for users to add into or modify physical objects in the virtual environment in any active scenario.

    \item \textbf{FR-19: Real-Time Telemetry Visualization:} The system must support the visualization of real-time robot status data like battery level, system load, postion or speed directly within the 3D environment.
\end{itemize}

\section{Demonstrator \& Validation}
\label{sec:scenario_logic}

These requirements define the logic for specific environments used to validate versatility of the framework.

\subsection{Smart Farming Scenario}
\begin{itemize}
    \item \textbf{FR-20: Discrete Agricultural States:} The environment must simulate field areas with discrete states like Untouched, Cultivated and Seeded. These states must be visually distinct to be detectable by the robot's camera.
    
    \item \textbf{FR-21: Tool-Based Interaction:} The scenario must simulate specific virtual equipment that can be attached to the digital robot model. The change of the state of the field area must only occur when the robot interacts with it while the correct tool is used.
\end{itemize}

\subsection{Logistics Scenario}
\begin{itemize}
    \item \textbf{FR-22: Collectible Object:} The scenario must populate the environment with objects with specific colors that identified it as collectible. These objects must be physically interactable and capable of being attached to the digital robot model via the general attachment capability.
    
    \item \textbf{FR-23: Goal Validation:} The environment must contain zones capable of detecting the presence of specific target objects. Upon valid entry, the system must trigger a visual state change in the object to provide confirmation that a transport task is complete.
    
    \item \textbf{FR-24: Dynamic Obstacle Handling:} The scenario must allow a user to place obstacles in real-time.
\end{itemize}

\subsection{Surface Painting Scenario}
\begin{itemize}
    \item \textbf{FR-25: Surface Modification:} The scenario must allow the robot to dynamically modify the texture of the floor surface at its current location via a command. This is required to generate persistent visual trails for a line-following navigation tasks.
    
    \item \textbf{FR-26: Interactive Surface Painting:} The user interface must allow operators to draw lines directly onto the texture of the floor using a Mouse or VR Controller.
\end{itemize}

\subsection{Adaptive Navigation Scenario}
\begin{itemize}
    \item \textbf{FR-27: Dynamic Map Reconfiguration:} The system must automatically rearrange street segments in areas distant from the robot. This simulates a changing environment to test the robustness of the navigation logic.
    
    \item \textbf{FR-28: Visual Algorithm Compatibility:} The street segments must feature visual markings that match the style of the surface painting tool. This allows the same line-following algorithm to be used on both dynamic streets and user-painted paths.
\end{itemize}

\subsection{Competitive Game Scenario}
\begin{itemize}
    \item \textbf{FR-29: Game State Management:} The system must enforce game rules, tracking scores based on object positions and automatically resetting the environment state between rounds.

    \item \textbf{FR-30: Dynamic Physical Interaction:} The scenario must simulate an elastic moving object and provide a rigid attachment for the Digital Twin, allowing the robot to deflect the object via physical collision.

    \item \textbf{FR-31: Automated Adversary:} The system must provide an autonomous opponent that tracks and reacts to the object's movement.
\end{itemize}

\section{User Interface \& Mixed Reality}
\label{sec:ui_reqs}

These requirements define how human users view and interact with the system.

\begin{itemize}
    \item \textbf{FR-32: AR Projection:} The system must render a top-down, orthographic view of the scene. The projection parameters must be configurable to align the virtual view 1:1 with the physical dimensions of the testbed floor.
    
    \item \textbf{FR-33: VR Visualization:} The system must support a Virtual Reality mode allowing users to view the Digital Twin in 3D space, extending the Mixed Reality capabilities beyond the original AR projection \cite{Geh24, MV20}. A floating data interface attached to the robot must automatically orient itself to remain readable from the user's perspective.
        
    \item \textbf{FR-34: Interactive Goal Setting:} Users must be able to define navigation targets compatible with the \texttt{Nav2} stack \cite{macenski2020marathon2} by pointing at the virtual ground using the active input device.
    
    \item \textbf{FR-35: Visualization Control:} The user interface must allow runtime modification of the visual environment. This includes toggling the visibility of specific simulation components (e.g., robot models or lighting), robot models or lighting and selecting which sensor data streams are currently displayed on projection interface.
\end{itemize}

\section{Automated Scenario Execution}
\label{sec:automation_reqs}

To demonstrate the framework's utility as a validation platform, it is necessary to implement autonomous agents capable of executing complex, multi-step scenarios solely through the system's sensor and command interfaces.

\begin{itemize}
    \item \textbf{FR-36: Sensor-Based Autonomy:} The control agents must rely solely on simulated sensor data to make decisions, strictly avoiding access to the simulation's internal ground truth data.
    
    \item \textbf{FR-37: Source-Agnostic Visual Perception:} The automated agents must employ computer vision techniques to extract information from a video stream. The architecture must allow these algorithms to operate interchangeably on either the synthetic camera feed or the physical camera feed observing the AR projection.
          
    \item \textbf{FR-38: ROS 2 Automation:} The automation scripts must utilize standard ROS 2 interfaces to autonomously guide the robot through complex scenarios. This involves sending action commands and monitoring feedback topics to confirm that tasks were completed successfully.
\end{itemize}

\section{Non-Functional Requirements}
\label{sec:nfr}


\begin{itemize}
    \item \textbf{NFR-01: Processing Latency:} The system must minimize the internal processing delay between the reception of a ROS 2 state update and the rendering of the corresponding visual frame. This latency must be measurable via internal software logging and should consistently remain lower than the simulation frame interval (approx. 16.6ms at 60Hz) to ensure the Digital Twin reacts instantly to incoming data.
  
    \item \textbf{NFR-02: Frame Rate Stability:} The simulation must maintain a stable target frame rate of at least 60 frames per second to prevent simulator sickness in VR \cite{Chang20102020} and to ensure that AR projections do not exhibit visual artifacts that could confuse the robot's perception system.    
    
    \item \textbf{NFR-03: Standalone Script Execution:} The control logic must be designed to run without a dependency on the simulation software. The automation scripts must be capable of launching and executing in a pure ROS 2 environment, interacting with the system only through published topics and services, regardless of whether the backend is the simulation or another entity.
   
    \item \textbf{NFR-04: Scalability:} The system architecture must support scenes with a high density of dynamic objects without violating the frame rate stability requirement.
    
    \item \textbf{NFR-05: Configuration Flexibility:} Key system parameters, such as network topic names, robot dimensions, and projection resolution, must be accessible via a configuration interface to adapt to different laboratory setups without requiring source code modification.
\end{itemize}