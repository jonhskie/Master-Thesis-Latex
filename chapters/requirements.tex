\chapter{Requirements}
\label{ch:requirements}

This chapter establishes the functional and non-functional requirements for the Mixed Reality Environment. Intended as a validation tool for Robot-in-the-Loop (RitL) testing, the system has to bridge the gap between virtual simulation and physical reality. The requirements follow a hierarchical structure, covering the simulation engine capabilities, system architecture, and digital twin interfaces. In addition, specific requirements regarding the autonomous agents acting within this environment to validate the system are designed.

\section{Simulation Engine Capabilities}
\label{sec:platform_reqs}

These requirements define the technical capabilities of the underlying simulation engine. The criteria provide a basis for the technology selection process in the implementation phase.

\begin{itemize}
    \item \textbf{FR-01: Visual Fidelity:} The simulation engine shall be able to support rendering pipelines capable of producing image data representative of real-world lighting and material properties. This is required to provide realistic synthetic input for the computer vision algorithms of the robot.
    
    \item \textbf{FR-02: Physics Simulation:} Overcoming the limitations in the previous VERA framework, the engine shall provide a physics system that will be able to resolve mass, friction, collision, and drag to simulate physical interactions of such sort as a robot pushing obstacles.
    
    \item \textbf{FR-03: VR \& AR Support:} The platform shall provide a mature basis for VR and AR. It shall enable seamless integration of output and input devices without needing to write custom drivers.
\end{itemize}

\section{System Architecture \& Middleware}
\label{sec:architecture_reqs}

The system architecture is defined by limitations of the hardware and the need for standardized robotic communication.

\begin{itemize}
    \item \textbf{FR-04: ROS 2 Integration:} The system shall use ROS~2 as sole middleware for all communication. It shall act as first-party in the network to minimize latency and shall make use of standard message definitions for all sensor streams, telemetry, and control commands to ensure interoperability with the physical robot.
    
    \item \textbf{FR-05: Time Synchronization:} The system shall act as the simulation time source and publish the clock signal. To prevent data drift in control algorithms, all simulated sensors and publishers shall synchronize their timestamps to this source.
    
    \item \textbf{FR-06: Scenario Management:} The system shall provide a mechanism to dynamically load and unload simulation environments via a network command during runtime. This is to enable test runs across different environments without restarting the application.
\end{itemize}

\section{Digital Twin Interfaces}
\label{sec:digital_twin_reqs}

The system shall provide a digital representation of the robotic platform that mirrors its physical counterpart. This includes synchronization with real-world tracking, kinematic simulation, and the publishing of coordinate transforms.

\begin{itemize}
    \item \textbf{FR-07: Pose Synchronization:} The system shall accept real-time pose updates from an external tracking system to synchronize the position and orientation of the Digital Twin with the physical robot.
    
    \item \textbf{FR-08: Tracking Failsafe:} The system shall implement a failsafe that stops the robots movement if the tracking data stream is interrupted for a defined threshold, preventing unwanted movements of the robot due to signal loss.

    \item \textbf{FR-09: Standalone Mode:} The system shall be capable of mimicking the behavior of the robot in the absence of physical hardware. It shall accept velocity commands and publish resulting odometry to validate navigation algorithms in a fully virtual setting.
    
    \item \textbf{FR-10: Coordinate Transforms:} The system shall publish dynamic coordinate transforms representing the robot's motion. It shall also support the configuration of static sensor offsets to match the physical mounting positions.
\end{itemize}

\section{Environment Capabilities}
\label{sec:environment_reqs}

These requirements define the capabilities of the virtual environment itself. This includes the generation of sensor data and the mechanisms for dynamic interaction and visualization.

\begin{itemize}

    \item \textbf{FR-11: Sensor Simulation:} The system shall provide simulated sensors capable of generating synthetic data, including video feeds and ranging data. The sensors shall be capable of detecting both static environment structures and dynamic obstacles.
    
    \item \textbf{FR-12: Command Interface:} The system shall provide a generic network interface to receive commands and return status feedback, enabling the robot to trigger simulation events.
    
    \item \textbf{FR-13: Dynamic Environment Interaction:} The system shall enable dynamic manipulation of the environment. This includes the physical pushing and attaching of objects, the modification of object states, and the alteration of surface textures during runtime.
    
    \item \textbf{FR-14: Telemetry Display:} The system shall render real-time robot status data as floating UI elements attached to the Digital Twin within the 3D environment.
\end{itemize}

\section{User Interface \& Mixed Reality}
\label{sec:ui_reqs}

These requirements define how human operators visualize and interact with the system.

\begin{itemize}
    \item \textbf{FR-15: AR Projection:} The system shall render a top-down, orthographic view of the scene. The projection parameters shall be configurable to align the virtual view 1:1 with the physical dimensions of the testbed floor.
    
    \item \textbf{FR-16: VR Interface:} The system shall provide a Virtual Reality mode, allowing users to view the Digital Twin in 3D space. Information interfaces shall automatically orient themselves to remain readable from the user's perspective.
        
    \item \textbf{FR-17: Goal Setting:} The interface shall allow users to set navigation targets compatible with the Nav2 navigation stacks by pointing at the virtual ground using a mouse or VR controller.
    
    \item \textbf{FR-18: Visualization Control:} The user interface shall allow runtime modification of the visual environment to toggling the visibility or active state of various system components, such as the robot virtual body, lighting, or telemetry displays.
\end{itemize}

\section{Autonomous Agent Requirements}
\label{sec:agent_reqs}

These requirements apply to the autonomous agents developed to interact with the environment. These agents serve to show the system's capabilities and test the robot's responses to dynamic scenarios.

\begin{itemize}
    \item \textbf{FR-19: Sensor Dependence:} The autonomous agents shall make decisions based solely on environmental data provided through simulated sensors. They shall not access the internal state of the simulation engine to derive information.
              
    \item \textbf{FR-20: Standardized Communication:} The agents shall utilize standard robotic communication patterns to send action commands and monitor feedback topics to confirm task completion.
    
    \item \textbf{FR-21: Adversarial Behavior:} The system shall include an autonomous entity acting as an adversary, controlled by an algorithm that dynamically responds to the environment.
\end{itemize}

\section{Non-Functional Requirements}
\label{sec:nfr}

The following constraints define the standards necessary to ensure reliability and extensibility.

\begin{itemize}
    \item \textbf{NFR-01: Low Latency:} The system shall minimize the delay between the reception of a state update and the rendering of the corresponding visual frame. Latency shall remain lower than the simulation frame time to ensure immediate visual feedback.
  
    \item \textbf{NFR-02: Frame Rate:} The simulation shall maintain a stable target frame rate (minimum 60 FPS) to prevent simulator sickness in VR \cite{Chang20102020} and to ensure that AR projections remain visually stable and accurately aligned with the physical environment.    
       
    \item \textbf{NFR-03: Modularity:} The system architecture shall be modular, allowing individual components (e.g., sensor models, environment logic) to be added or modified without altering the core application.
    
    \item \textbf{NFR-04: Scalability:} The system shall support scenes with a high density of dynamic objects without violating frame rate stability requirements.
    
    \item \textbf{NFR-05: Configurability:} The system shall provide external interfaces to configure robot parameters, sensor properties, and environment settings, enabling adaptation to different physical setups without code recompilation.
\end{itemize}