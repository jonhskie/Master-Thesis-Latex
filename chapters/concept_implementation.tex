\chapter{Implementation}
\label{ch:concept_and_implementation}

This chapter describes the development of the Mixed Reality Environment, a platform designed to support RitL testing by combining simulation with augmented reality projection. The resulting system bridges the gap between purely digital simulations and real-world experiments by creating a common workspace where physical robots and virtual objects coexist and interact.

The central concept of this environment is the projection of a simulated world directly onto the laboratory floor. Rather than testing robots in completely physical setups or purely within software simulations, this system projects dynamic, physics-based elements into the real world. As can be seen from Figure \ref{fig:mr_environment_overview}, the physical robot moves across the real floor while sensing and acting upon virtual objects, such as a field, obstacles, or other entities. This projection has a dual purpose: it provides instant visual feedback to a human observer and enables the robot to receive synthetic sensor data that reflect the state of the virtual scene. This allows the robot to detect and respond to simulated entities as if they were physically present.

\begin{figure}[H]
\centering
%\includegraphics[width=0.9\textwidth]{figures/mr_environment_overview} % Placeholder for your figure
\caption{The Mixed Reality Environment in operation. The physical robot interacts with a projected Smart Farming scenario, where the robot and the digital twin are synchronized via ROS~2.}
\label{fig:mr_environment_overview}
\end{figure}

To enable these applications, the system integrates several key functions into an architecture. First, it maintains a dynamic twin of the robot. This digital twin is synchronized with the movement of the physical hardware but can interact with the simulation's physics engine. This allows for complex actions, such as pushing virtual boxes or colliding with moving objects. This ensures the simulation responds realistically to the robot's physical presence rather than serving as a static background.

In addition to maintaining a dynamic digital twin, the system also simulates sensors for the robot. It generates virtual LiDAR scans and camera images from the robot's perspective, allowing autonomous agents to receive virtual sensordata. This enables thorough testing of navigation and vision algorithms within the mixed reality environment.

The architecture uses ROS~2~\cite{MFG22a} as the primary communication backbone. By integrating ROS~2 nodes directly into the simulation engine, the virtual environment acts as a first-party participant in the robot's network. It manages time synchronization to prevent timing related errors, handles the loading of different scenarios and provides two-way exchange of status data and control commands.

Besides sensor simulation, the environment also supports real-time diagnostic feedback. The system functions as a monitoring tool by projecting the robot's internal operating data back onto the physical workspace and the VR interface. Performance metrics, such as temperature, RAM usage, and location coordinates, are shown on information panels attached to the digital twin in the 3D environment. Moreover, the system shows the robot's live camera feed within the virtual scene. This allows operators to observe the robot's physical behavior alongside its internal status and visual perception in real-time.

To demonstrate the flexibility and reliability of this architecture, three different autonomous agents were implemented:
\begin{itemize}
\item A Smart Farming Agent that autonomously explores a virtual field, identifies its boundaries, and operates simulated tools to perform fieldwork tasks.
\item A Logistics Agent that autonomously searches for, identifies and sorts colored objects within the environment and brings them to designated zones.
\item A Line Following Agent that uses visual tracking to follow paths drawn by a human user in real-time.
\end{itemize}

The platform provides an adaptable and scalable testing solution by integrating physics, sensor simulation, and autonomous control logic into a synchronised mixed reality environment. The only factor limiting the test environment's complexity is software, not its physical design. The technological decisions, system architecture, and particular implementation of these elements will be covered in detail in the sections that follow.
%%%%%%%%%%%%%%%%%%%%%FERTIG%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Comparative Analysis}
The simulation engines that were considered as candidates to implement the Mixed Reality Environment are: Gazebo, Isaac Sim, Unreal Engine, and Unity. All were evaluated based on five critical criteria: visual fidelity, physics capabilities, learning curve, community support, and native integration with VR, AR and ROS~2. Table \ref{tab:sim_comparison} summarizes the results of this evaluation.

\begin{table}[H]
    \centering
    \caption{Comparison of Simulation Engines for Mixed Reality Digital Twins~\cite{Gonzalez2025, Kargar2024, Singh2025, Coronado2023}.}
    \label{tab:sim_comparison}
    \begin{tabularx}{\textwidth}{@{}lXXXX@{}}
        \toprule
        \textbf{Feature} & \textbf{Gazebo} & \textbf{Isaac Sim} & \textbf{Unreal Engine} & \textbf{Unity} \\ 
        \midrule
        \textbf{Primary Use} & Control \& Navigation & AI \& Photorealism & Photorealism & HRI \& MR (VR/AR) \\ 
        \textbf{Visual Fidelity} & Moderate & Very High & Very High & High \\ 
        \textbf{Physics Engine} & ODE / Bullet & PhysX 5 (GPU) & Chaos / PhysX & PhysX \\ 
        \textbf{Learning Curve} & Steep & Advanced & Steep (C++) & Moderate (C\#) \\ 
        \textbf{Community} & High (ROS) & Moderate & High (Gaming) & Very High (MR) \\ 
        \textbf{ROS~2 Integ.} & Native & Bridge & Bridge & Plugin (Native) \\ 
        \textbf{Hardware} & Low & Very High (RTX) & High & Moderate \\ 
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Selection Rationale}
Based on the comparative analysis, the \textbf{Unity Engine} was selected as the implementation platform for this thesis. This decision is driven by four key factors that align with the system requirements:

\begin{itemize}
    \item \textbf{VR Framework} \hyperref[req:FR-03]{FR-03}: Unity has an integrated framework for VR applications~\cite{Coronado2023}. In contrast, simulators like Gazebo lack native VR support, which is critical for the proposed human-robot interaction interface.
    
    \item \textbf{Visual Fidelity \& Physics} (\hyperref[req:FR-01]{FR-01}, \hyperref[req:FR-02]{FR-02}): Unity provides high-fidelity visualization alongside PhysX integration. This offers an optimal balance between performance and visual quality, avoiding the restrictive hardware requirements associated with NVIDIA Isaac Sim~\cite{Gonzalez2025}.
    
    \item \textbf{Development Efficiency:} The use of C\# scripting, combined with documentation and community support, makes Unity more accessible for rapid prototyping than the C++ environment of Unreal Engine~\cite{Coronado2023}.

    \item \textbf{ROS~2 Integration} \hyperref[req:FR-04]{FR-04}: The availability of the \texttt{ros2-for-unity} ~\cite{Rob24} asset enables the simulation to function as a first-party participant in the ROS~2 network. This means it supports the low-latency communication requirement by avoiding external bridge applications.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%FERTIG%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{System Architecture}
\label{sec:system_architecture}

The realization of the Mixed Reality Environment requires a distributed software architecture that connects the advanced rendering capabilities of the Unity engine with real-time robotic control. The system is designed to satisfy the requirement for \textbf{Middleware Integration} \hyperref[req:FR-04]{FR-04} by establishing a one unified communication layer between the simulation host and the autonomous agents.

\subsection{Hardware and Network Topology}
\label{subsec:hardware_topology}

To ensure high performance and support for specific hardware peripherals, the system topology is consolidated into a single high-performance workstation and the mobile robot. As shown in Figure \ref{fig:hardware_topology},  the architecture is divided into three clear computational domains:

\begin{enumerate}
    \item \textbf{Simulation Host (Windows 11):} The Unity engine runs natively on Windows. This operating system was chosen due to currently missing support of VR interfaces in the Unity Editor under Linux. This layer manages the physics engine (PhysX), renders the digital twin, and communicates with the VR headset. Additionally, a projector is connected to this host to visualize the environment on the physical floor \cite{PLACEHOLDER_HARDWARE_SETUP}.
    
    \item \textbf{Control Environment (WSL/Ubuntu):} ROS~2~Jazzy runs within the Windows Subsystem for Linux within Ubuntu 24.04. This environment hosts the computationally intensive software, such as the autonomous navigation stack (\textit{Nav2})~\cite{NAV2Ref} and the mission logic agents. Running these components directly on the robot causes performance issues or system crashes due to limited onboard resources, so they are offloaded to the workstation to ensure stable and responsive operation.
    
    \item \textbf{Physical Robot (Hardware Layer):} The EMAROs~\cite{EMAROs_Ref} robot operates as an independent node in the network. It uses Ubuntu 24.04 and runs ROS~2~Jazzy on an onboard Raspberry Pi to handle hardware drivers, such as motor controllers and IMU sensors. It connects to the workstation via Wi-Fi.
\end{enumerate}

\begin{figure}[H]
    \centering
    % TODO: Create a diagram showing:
    % 1. PC Box (split into Windows and WSL sections).
    % 2. Windows connected to Projector (HDMI) and VR Headset (Wi-Fi).
    % 3. WSL connected to the internal network (localhost).
    % 4. Robot connected via Wi-Fi to the Router.
    %\includegraphics[width=1.0\textwidth]{figures/hardware_topology} 
    \caption{Hardware and Network Topology. The workstation handles both the Unity simulation (Windows) and the heavy control logic (Ubuntu/WSL) to preserve the robot's onboard resources. The robot, VR headset, and projector connect as peripherals to this central core.}
    \label{fig:hardware_topology}
\end{figure}

For the VR interface (\hyperref[req:FR-16]{FR-16}), the visual output from Unity is streamed via Wi-Fi to the Meta Quest Pro headset using \textit{ALVR}~\cite{ALVR_Github}, an open-source streaming solution. This setup allows the user to move freely without a cable tether.

\subsection{Software Component Architecture}
\label{subsec:software_components}

The software architecture follows a modular publisher-subscriber pattern. To integrate the Unity game engine with the robotic network, the system utilizes the \textit{ROS2ForUnity} library \cite{ROS2ForUnityDocs}.

The core of this integration is the \texttt{ROS2UnityComponent}~\cite{ROS2ForUnityDocs} class. This component acts as a wrapper around the standard ROS~2 middleware. Inheriting from Unity's \texttt{MonoBehaviour} class, it must be attached to a \texttt{GameObject} in the scene to function. Upon startup, this component initializes the ROS~2 context, makes sure that a valid connection is established, and handles the lifecycle of the ROS~2 nodes running within the simulation. It allows other scripts to access the ROS~2 network by referencing this central component to create publishers, subscribers, and service clients.

The functionality of the digital twin is driven by C\# scripts that interact with the engine's internal state machine. To ensure that physical simulation remains deterministic while visualization remains smooth, the architecture uses specific phases of the Unity execution loop~\cite{UnityDocumentation}:

\begin{itemize}
    \item \textbf{Initialization (\texttt{Start}):} This method is called once when the script is first enabled, before any frames are updated. In this architecture, it is used to retrieve the reference to the \texttt{ROS2UnityComponent}~\cite{ROS2ForUnityDocs}, initialize the specific ROS nodes, and set up the necessary publishers and subscribers.
    
    \item \textbf{Physics Loop (\texttt{FixedUpdate}):} This method executes at a constant, user-defined time step, independent of the visual frame rate. All physics-based calculations, such as updating the robot's position based on received velocity commands or handling collision detection, occur in this loop. This ensures that the robot's physical behavior is consistent regardless of graphical performance.
    
    \item \textbf{Rendering Loop (\texttt{Update}):} This method runs once every frame. It is utilized for logic that is not physics-related, such as updating UI elements, rendering diagnostic lines, or capturing camera frames for visualization. This separation ensures that high-frequency visual updates do not interfere with the stability of the physics engine.
\end{itemize}

Figure \ref{fig:software_architecture} provides an overview of how these Unity scripts interact with the external ROS~2 nodes via topics. Detailed descriptions of these components follow in Sections 4.2 and 4.3.

\begin{figure}[H]
    \centering
    % TODO: Create a diagram showing:
    % Left side: Unity Components (LidarSensor, CameraSensor, VirtualRobotController).
    % Middle: Arrows representing ROS Topics (/scan, /camera/image, /cmd_vel).
    % Right side: ROS Nodes (Nav2 Stack, Perception Node, Mission State Machine).
    %\includegraphics[width=1.0\textwidth]{figures/software_architecture} 
    \caption{Software Component Architecture. Unity scripts act as ROS nodes, publishing sensor data and subscribing to control commands using ROS~2 topics. The external control stack, running in WSL, processes this data to make autonomous decisions.}
    \label{fig:software_architecture}
\end{figure} 

\subsection{Time Synchronization}
\label{subsec:time_sync}

A critical challenge in Robot-in-the-Loop testing is how to synchronize time between the simulation and the robot's software. If the simulation runs slower or faster than real-time, the navigation algorithms of the robot may calculate velocities that are incorrect. To fulfill the requirement for Time Synchronization \hyperref[req:FR-05]{FR-05}, the system decouples the ROS network from the computer's system time.

The \texttt{ClockPublisher.cs} script in Unity acts as the master clock. It publishes the current simulation time to the \texttt{/clock} topic. The ROS~2 nodes running on the workstation such as the navigation stack are configured with \texttt{use\_sim\_time = true}, forcing them to synchronize their logic with the Unity engine.

To ensure consistency, a \texttt{RosTimeHelper.cs} utility is used by all simulated sensors. This script ensures that sensor data leaving Unity is stamped with the exact simulation time of the frame it was rendered in to prevent data drift.

%%%%%%%%%%%%%%%%%%%%%FERTIG%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Robot Representation and Control}
\label{sec:robot_representation_and_control}
The robot in the Mixed Reality Environment runs in one of two modes: either as a completely simulated virtual robot or as a synchronized digital twin of the physical EMAROs~\cite{EMAROs_Ref} platform. To ensure consistency, both of these modes use the same visual and geometric model. This model is a simple cylinder that approximates the physical size of the robot for collision detection while keeping computational costs low. The behavior of this cylinder depends on the active control script, allowing the system to switch between physics-based simulation and position-based synchronization based on the requirements of the test. To select the desired mode, the user can enable in the Unity Editor either the model of the digital twin or the purely virtual robot.

\subsection{The Purely Virtual Robot}
In the standalone simulation mode, the robot is controlled by the \texttt{VirtualRobotController} (\hyperref[req:FR-09]{FR-09}). This component allows for the validation of high-level control logic in a safe environment before it is deployed on real hardware. Unlike simulations that teleport a robot to a target point, this implementation uses the Unity physics engine PhysX to simulate realistic movement.

The controller acts as a ROS~2 node that accepts velocity commands (\texttt{cmd\_vel}) and publishes odometry data. To handle data exchange between the asynchronous ROS~2 thread and the Unity main thread, incoming commands are buffered in a thread-safe \texttt{ConcurrentQueue}. During the physics step of Unity (\texttt{FixedUpdate}), the controller retrieves the latest command and applies it to the \texttt{Rigidbody} of the robot.

The script does not directly change the position of the robot. Instead, it modifies the velocity of the physics body. By applying limits to acceleration, the system simulates mass and inertia for the robot. The robot can push lightweight virtual objects, but heavy obstacles or walls will stop its movement. This provides realistic feedback to the navigation system.

Simultaneously, the controller provides ground truth data. It tracks the movement of the robot in Unity, converts the coordinates to the ROS standard, and publishes a timestamped odometry and tf messages (\hyperref[req:FR-10]{FR-10}). This closes the control loop for the external navigation logic~\cite{MFG+22}.

\subsection{The Digital Twin Implementation}
When operating in RitL mode, the \texttt{PoseRobotController} takes control. In this configuration, the virtual robot stops acting as a dynamic entity and instead becomes a digital twin of the real EMAROs~\cite{EMAROs_Ref} robot \hyperref[req:FR-07]{FR-07}.

The controller listens to a pose topic given by the external ArUco tracking to move the digital twin. The incoming pose is provided in pixel coordinates from the tracking camera. It must be mapped onto the defined virtual area. On every new incoming pose, the script converts the pixel coordinates into Unity world space. To ensure alignment, an offset parameter enables the operator to perform a calibration of the digital twin so that its center is aligned with the physical mounting position of the tracking marker. The script republishes the synchronized pose as odometry and tf messages (\hyperref[req:FR-10]{FR-10}).

Unlike the purely virtual robot, the digital twin overrides the physics engine. It is set to a kinematic state, meaning it is immune to virtual forces. If the physical robot moves, the digital twin moves with it, ignoring virtual barriers. This allows the physical robot to push virtual objects with absolute force. This way, the state of the virtual world always reflects physical reality.
\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/control_architecture.png}
    % use here the real virtaul image of the robots, maybe make them in diffrent color. add pfeile to show data flow
    \caption{Data flow comparison between the Purely Virtual Robot and the digital twin. The Purely Virtual Robot (left) is driven by internal physics and ROS commands, while the digital twin (right) is driven directly by external tracking data.}
    \label{fig:control_architecture}
\end{figure}

\subsection{Safety and Failsafe Mechanisms}
A major challenge in mixed reality testing is signal latency or loss. If the physical robot leaves the tracking area or if the ArUco marker is blocked, position updates will stop. Without a failsafe, the digital twin would freeze while the physical robot continues to move blindly, causing a mismatch between reality and simulation.

To prevent this, the \texttt{PoseRobotController} implements a safety watchdog  (\hyperref[req:FR-08]{FR-08}). The system continuously checks how long it has been since receiving the last pose update. If this delay exceeds a given safety limit, the system assumes tracking has been lost.

When this occurs, the controller marks the tracking state as invalid. Although the digital twin stops moving in the simulation, the system is designed to send a stop command to the motors of the physical robot. This ensures the robot does not perform actions without simulation guidance. Once the tracking system re-acquires the robot, the watchdog resets and the digital twin resumes synchronization immediately.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FERTIG%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Environmental Simulation Capabilities}
\label{sec:environmental_simulation_capabilities}

To serve as an effective testbed for autonomous agents, the virtual environment must offer more than just static scenery. It requires the capability to manipulate objects physically, generate realistic synthetic sensor data, and modify the state of the environment persistently. This section describes the implementation of these features, fulfilling the requirements for Sensor Simulation (\hyperref[req:FR-11]{FR-11}), Dynamic Environment Interaction (\hyperref[req:FR-13]{FR-13}), and the Command Interface (\hyperref[req:FR-12]{FR-12}).

\subsection{Physics-Based Interaction}
A key feature of the system is the robot's ability to physically connect with environmental objects. To achieve a rigid and predictable connection between the robot and manipulated items, the architecture uses a kinematic attachment method managed by the \texttt{AttachmentCommandManager} and the \texttt{RobotAttachmentController}.


ROS~2 command and control (\hyperref[req:FR-12]{FR-12}) is provided via the \texttt{/robot\_command} topic. Commands are sent as simple string messages (e.g., \texttt{"attach,plow"} or \texttt{"detach"}), offering a single, consistent interface for the high-level agents to request simulation state changes. The Command Manager runs on the Unity main thread and processes incoming entries from a thread-safe queue. On receiving an attach request, it searches for valid targets within a configurable radius using \texttt{Physics.OverlapSphere}.

When a valid target is identified, the system attaches it to the robot. In Unity, the object is attached as a child of the robot's transform hierarchy at a specific mount point. Crucially, the system modifies the physics state of the attached object (\hyperref[req:FR-13]{FR-13}). Upon attachment, the script sets the object's \texttt{Rigidbody.isKinematic} property to true. This removes the object from the physics engine's dynamic calculations. This state change grants the robot's transform absolute authority over the object's position, eliminating any relative motion, jitter, or drift between the robot and the load during transport.

At the same time, the controller disables collisions between the robot and the tool using \texttt{Physics.IgnoreCollision}. Without this step, snapping the tool to the robot would cause their geometries to overlap. The physics engine would attempt to separate them forcibly, potentially causing the equipment to be ejected erratically.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/attachment.png}
    \caption{The robot model with an attached object. The kinematic coupling ensures the object moves rigidly with the robot chassis during transport. }
    \label{fig:attachment_logic}
\end{figure}

\subsection{ Surface Modification}
To fulfill the requirement for dynamic environment interaction regarding surface textures (\hyperref[req:FR-13]{FR-13}), the system implements a continuous modification mechanism. 

The modification logic relies on physics raycasting to identify the coordinates of the surface at the point of contact. When a valid hit is detected on the target surface, the \texttt{TrackPainter} which is attached to the robot model calculates the pixel coordinates on the texture map. It then modifies the pixel buffer directly using \texttt{SetPixels32}, changing the color to a configured color and thickness. Crucially, these changes are committed to the GPU using the \texttt{Apply} method. In complex scenarios, multiple agents (e.g., the robot, a VR controller, or a mouse interface) may attempt to modify the terrain simultaneously. To manage this, the \texttt{SharedPaintTextureRegistry} functions as a singleton manager. It maintains a dictionary of active textures and ensures that all painters operate on a single, shared instance of the \texttt{Texture2D}. This prevents race conditions where one agent overwrites the work of another and ensures data consistency across the simulation.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/texture_painting.png}
    \caption{The dynamic terrain modification system. A raycast determines the UV coordinate of the tool's contact point (left), which is used to update the shared texture registry (right), permanently altering the ground appearance.}
    \label{fig:texture_painting}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%fertig aber AI%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sensor Simulation}
To validate the perception algorithms of the autonomous agents, the system provides simulated sensors capable of generating synthetic data, including video feeds and ranging data (\hyperref[req:FR-11]{FR-11}).

The \texttt{LidarSensor} component simulates a 2D planar laser scanner which can be attached to the virtual model of the robot. Unlike GPU-based depth buffers often used in visual rendering, this implementation utilizes the Unity physics engine's raycasting API to query the scene geometry directly. In every simulation step, the component executes a loop corresponding to the configured number of beams (e.g., 360). For each beam, the system calculates a direction vector based on the scan angle. Since Unity utilizes a Left-Handed coordinate system (Y-up) and ROS uses a Right-Handed system (Z-up), the script employs a custom extension method to transform the desired scan angle into the correct Unity world-space vector.

For each beam a \texttt{Physics.Raycast} is cast along the computed direction. If the ray intersects a collider on the configured collision layer, its hit distance is recorded; otherwise the beam is reported as an infinite range. The collected distances are packaged into a standard \texttt{sensor\_msgs/LaserScan} message. Immediately after the raycast loop finishes, the message header is stamped with the current simulation time from the \texttt{ClockPublisher}. This exact timestamping preserves temporal alignment with the navigation stack and prevents the ROS~2 transform (TF) system from rejecting the scan due to extrapolation errors caused by unsynchronized timestamps.
\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/lidar_raycasting.png}
    \caption{Visualization of the LiDAR simulation in the editor. Red lines indicate raycasts that did not hit an obstacle within range, while yellow lines indicate valid hits registered by the physics engine.}
    \label{fig:lidar_raycasting}
\end{figure}

Visual perception is handled by the \texttt{CameraSensor} component. This script attaches a dedicated Unity Camera to the robot model, which is configured to render the scene into a \texttt{RenderTexture} rather than the user's screen. This allows the simulation to generate visual data at a resolution independent of the application window.

The data extraction pipeline transfers the raw pixel data from the Graphics Processing Unit (GPU) memory to the Central Processing Unit (CPU) memory. Once on the CPU, the data is encoded into the appropriate format (e.g., raw RGB8 or JPEG) and published as a \texttt{sensor\_msgs/Image} or \texttt{CompressedImage}. To initialize the sensor, the \texttt{RobotCameraManager} instantiates the camera prefab at runtime and attaches it to the defined mount point in the robot hierarchy.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FERTIG ABER AI%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scenario and Object Management}
To support the requirement for \textbf{Scenario Management}\hyperref[req:FR-06]{FR-06}, the architecture allows the system to switch between different test environments without restarting the entire application.

This is handled by the \texttt{SceneManager.cs} script. When it receives a command via the \texttt{/vera/load\_scenario} topic, it performs a clean reset sequence:
\begin{enumerate}
    \item \textbf{Shutdown:} The active ROS~2 connections are terminated. This ensures that all Unity nodes are properly removed from the network, preventing any lingering nodes from publishing outdated or invalid data.
    \item \textbf{Load:} Unity loads the new environment scene asynchronously.
    \item \textbf{Restart:} A fresh \texttt{ROS2UnityComponent}~\cite{ROS2ForUnityDocs} initializes, creating a new connection for the loaded scenario.
\end{enumerate}
This fresh start approach ensures that Unity components specific to one scenario do not interfere with others.


Beyond static geometry, the environment must support the lifecycle management of transient entities, such as the delivery boxes in the Logistics scenario. This is handled by the \texttt{DynamicObjectManager}, which acts as a bridge between the ROS~2 data stream and the Unity instantiation engine (\hyperref[req:FR-13]{FR-13}).

The manager subscribes to the \texttt{/vera/virtual\_objects} topic, processing a custom message structure that defines an object's ID, type, and pose. Internally, the system maintains a dictionary mapping string identifiers (e.g., "box\_red", "obstacle\_A") to Unity Prefabs.
When an "ADD" or "MODIFY" command is received, the system checks if the object already exists. If it is new, the corresponding prefab is instantiated into the scene. If it exists, its transform is updated. Crucially, this component handles the coordinate conversion between the two systems, mapping the right-handed ROS position ($x, y, z$) and orientation ($x, y, z, w$) to the left-handed Unity world space. This allows external scripts or test runners to populate the scene with obstacles dynamically, validating the robot's ability to navigate changing environments.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FETIG ABER AI%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mixed Reality and User Interaction}
\label{sec:mixed_reality_and_user_interaction}

A primary objective of the framework is to enhance the transparency of the robotic system by providing intuitive, real-time feedback to human operators. This is achieved through a Mixed Reality (MR) interface that visualizes internal robot states and allows for direct environmental manipulation. This section details the implementation of augmented telemetry, sensor projection, and the interactive control systems designed for both desktop and Virtual Reality (VR) contexts.
\subsection{Environment Projection}
\label{subsec:environment_projection}

To fulfill the requirement for a 1:1 Augmented Reality projection (\hyperref[req:FR-15]{FR-15}), the system utilizes a dedicated Unity camera component configured in orthographic projection mode \cite{UnityDocumentation}. Unlike perspective cameras, the orthographic view preserves parallel lines and consistent object scales regardless of their distance from the camera\cite{UnityDocumentation}. This property is essential for projecting a map that aligns physically with the flat laboratory floor.

The camera is positioned at a fixed height looking downward. The alignment with the physical floor is achieved by calculating the exact \textit{Orthographic Size} parameter, which determines the vertical half-extent of the camera's viewing volume in world units \cite{UnityDocumentation}. By mapping the pixel resolution of the projector to the metric dimensions of the projection area, the system ensures that one unit in the simulation corresponds exactly to one meter on the physical floor.

To display this view on the physical floor, the ceiling-mounted projector acts as a secondary monitor for the workstation. During operation, the Game View window within the Unity Editor is undocked, moved to the projector's screen space, and maximized to fill the projection area.

\subsection{Augmented Telemetry and Diagnosis}

To fulfill the Telemetry Display requirement (\hyperref[req:FR-14]{FR-14}), the system implements the \texttt{RobotInfoBillboard} component. This script serves as a central data aggregator, establishing ROS~2 subscriptions based on a configurable list of topic definitions. It processes diverse message types and utilizes regular expressions to extract specific metrics, such as CPU and RAM usage, from text payloads.

For visualization in standard desktop views and AR projections, the system employs Unity's immediate mode GUI (\texttt{OnGUI}). The script calculates the screen-space coordinates of the robot's anchor using \texttt{Camera.WorldToScreenPoint} and applies rotational matrices to align the text. This ensures that the interface remains legible and strictly aligned with the screen plane, creating a floating overlay locked to the robot's position.

In contrast, screen-space rendering is unsuitable for Virtual Reality due to depth perception issues. To address this, the architecture supports a World Space Canvas for VR users. In this configuration, textual information is rendered onto a 3D plane floating physically above the digital twin. To ensure readability from any perspective, the canvas implements a constraint that continuously orients the UI to face the VR headset, allowing operators to inspect the robot's status naturally within the virtual environment.

\subsection{Sensor Data Projection}
While the digital twin visualizes the robot's kinematics, it does not inherently convey what the robot sees. To bridge this gap, the system utilizes the \texttt{RosImageToMaterial} component to project the robot's internal perception directly into the environment.

This component subscribes to ROS~2 camera topics. Upon receiving a frame—either raw or compressed—it decodes the data into a Unity \texttt{Texture2D} and applies it to a material on a planar mesh attached to the robot's chassis.

In the AR setup, this plane is positioned horizontally above the robot, projecting the camera stream onto the floor moving along with the robot. Similarly, in VR, this projection functions as a virtual dashboard, allowing the user to see the camera stream from a top-down perspective.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/augmented_telemetry.png}
    \caption{The Augmented Telemetry interface. A billboard displays system stats (top), while the \texttt{RosImageToMaterial} component projects the live OpenCV debug feed onto a plane attached to the robot (bottom), visualizing the internal state of the perception stack.}
    \label{fig:augmented_telemetry}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FERTIG ABER AI%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interactive Control Interfaces}
The system enables users to actively modify the simulation state (\hyperref[req:FR-13]{FR-13}) and direct the robot's actions (\hyperref[req:FR-17]{FR-17}). The implementation follows an input-agnostic architecture, where the core logic relies on 3D raycasting rather than specific hardware events. This design allows the system to support both standard mouse inputs and 3D VR controllers interchangeably.

The \texttt{NavigationGoalController} facilitates high-level control of the robot. In the desktop configuration, it casts a ray from the camera through the mouse cursor coordinates. When the ray intersects with the ground layer, the script calculates the target point and orientation based on the user's drag gesture. This data is serialized into a \texttt{geometry\_msgs/PoseStamped} message and published to the \texttt{/goal\_pose} topic, triggering the ROS~2 navigation stack. For Virtual Reality, this logic utilizes the \texttt{XR Ray Interactor}, enabling the operator to point at the virtual floor and dispatch the robot using the controller's trigger.

To support dynamic scenarios like Line Following, the user must be able to alter the environment at runtime. The \texttt{MouseSurfacePainter} component enables users to draw directly onto the terrain texture using a continuous raycast. In VR, this allows the operator to naturally draw paths or obstacles in 3D space that the physical robot can immediately perceive and follow.

Beyond painting, the VR interface leverages the physics engine to allow direct object manipulation. Users can physically grab and move interactive objects, such as the logistics boxes, using the VR controller's grip function. This allows for the manual reset of test scenarios or the introduction of dynamic obstacles (e.g., dropping a box in the robot's path) to validate the system's reactive planning capabilities.

In complex mixed reality scenarios, the density of visual information can become overwhelming. To manage this, the \texttt{VisibilityToggleManager} (\hyperref[req:FR-18]{FR-18}) allows users to selectively hide or reveal specific visualization aids attached to the robot. This component maps input actions to the rendering state of the \texttt{RobotInfoBillboard}, the projected camera plane, and even the robot's visual cylinder body. This feature allows operators to declutter the view when necessary—for example, hiding the large floating billboard to inspect the interaction between the robot's chassis and a tool—without disabling the underlying functionality.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%fertig aber ai%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementation of Autonomous Agents}
\label{sec:autonomous_agents}

To validate the capabilities of the Mixed Reality Environment, the system hosts three distinct autonomous agents. These agents are robotic applications that utilize the synthetic sensor data provided by the digital twin to make decisions (\hyperref[req:FR-19]{FR-19}). They serve to demonstrate the system's ability to support various domains of robotics, from agricultural field coverage to logic-based logistics.

The high-level decision-making for all agents is orchestrated using \textit{YASMIN} (Yet Another State MachINe)\cite{YASMIN}, a ROS~2-native library that allows for the creation of hierarchical, interruptible mission behaviors. This architecture ensures that the robot can switch between reactive behaviors (e.g., obstacle avoidance) and deliberative tasks (e.g., path planning) seamlessly.

\subsection{Navigation and Control Stack}
\label{subsec:nav_stack}

The fundamental capability of movement is provided by the \textit{Navigation2 (Nav2)} stack~\cite{NAV2Ref} (\hyperref[req:FR-17]{FR-17}). To address the varying physical characteristics of the simulated versus the physical robot, the architecture employs a dual-configuration strategy. Two distinct parameter sets—\texttt{nav2\_params\_virtual.yaml} and \texttt{nav2\_params\_robot.yaml}—are maintained. While they share the same behavioral tree structure, they utilize different tuning for inflation radii and cost scaling to account for the physical robot's safety margins and sensor noise. The appropriate parameter set is selected automatically at launch time based on the active system mode (Simulation vs. Robot-in-the-Loop), ensuring the navigation stack is always tuned to the current constraints.

A critical design choice in the controller configuration is the use of the \textbf{Rotation Shim Controller} wrapping the \textbf{DWB Local Planner}. Standard differential drive controllers often attempt to execute arc turns. In the confined space of the projection area, these arcs would cause the robot's footprint to collide with the costmap. The Rotation Shim intercepts navigation commands and forces the robot to rotate in place to align with the path heading before attempting to move forward. This ensures that the robot can maneuver in confined spaces where the available turning radius is smaller than the robot's kinematic limits.

To ensure that the robot does not collide with virtual object during autonomous operation, a \textbf{Collision Monitor} node runs in parallel with the navigation stack. Configured with a defined polygon representing the robot's footprint plus a safety margin, this node monitors the virtual LiDAR stream of the robot directly. If an obstacle breaches the safety polygon, the monitor overrides the navigation stack and publishes a zero-velocity command to the motor controller, acting as a software-level emergency stop.

\subsection{Modular Perception Stack}
\label{subsec:perception_architecture}

To interpret the visual data generated by the \texttt{CameraSensor} (\hyperref[req:FR-19]{FR-19}), the agents employ specialized computer vision pipelines tailored to their operational requirements.

\textbf{Modular Perception Node (Farm \& Logistics)}
For the Smart Farming and Logistics agents, a modular \texttt{perception\_node.py} was developed. Rather than running all detection algorithms simultaneously, which would consume excessive computational resources, this node acts as a central dispatcher. It switches between specialized processors based on the current mission state defined by the state machine (e.g., switching from \textit{Field Detection} to \textit{Edge Following}).
All processors inherit from a base class (\texttt{base\_processor.py}) that standardizes the ingestion of \texttt{sensor\_msgs/CompressedImage} messages and their conversion to OpenCV format (BGR8).

\textbf{Dedicated Line Perception Node}
The Line Following agent utilizes a standalone \texttt{line\_perception\_node.py}. This separation allows for a streamlined, high-performance pipeline optimized specifically for low-latency visual servoing, unburdened by the overhead of modular task switching.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Agent 1: Smart Farming}
\label{subsec:agent_farming}

\textbf{Scenario Context}
This agent operates in a simulated farm with an agricultural field. The goal is to identify the field boundaries and position and sequentially apply three tools (plow, seeder, harvester) to the field.

\textbf{Perception Modes}
The agent utilizes four distinct perception processors depending on the active state:
\begin{enumerate}
    \item \textbf{Field Detection (\texttt{FieldDetectionProcessor}):} Determines if the robot is currently on arable land by calculating the ratio of field-colored pixels in the image using HSV thresholding.
    \item \textbf{Edge Detection (\texttt{EdgeDetectionProcessor}):} Used during the approach phase. It creates binary masks for both field and its surrounding outer textures. By applying morphological dilation to the field mask and computing the bitwise AND with the outer mask, it identifies the boundary line where the two textures meet.
    \item \textbf{Edge Adjustment \& Following:} Implemented via two processors (\texttt{EdgeAdjustmentProcessor} and \texttt{EdgeFollowingProcessor}), these modules are used for precise alignment and tracking. They utilize the Probabilistic Hough Transform (\texttt{cv2.HoughLinesP}) to find line segments along the detected boundary, calculating the robot's lateral error (distance from the line centroid to image center) and angular error (deviation from vertical).
    \item \textbf{Equipment Detection (\texttt{EquipmentDetectionProcessor}):} Active during tool search. It isolates specific tool colors (e.g., yellow for harvester) using HSV masking and calculates the object's centroid to guide the docking approach.
\end{enumerate}

The real-time debug stream (Figure \ref{fig:farm_debug}) visualizes these detections, overlaying the regression line (green) or the equipment bounding box (blue) onto the camera feed.

\begin{figure}[H]
    \centering
    %\includegraphics[width=0.8\textwidth]{figures/farm_perception_debug} 
    \caption{Debug view of the Smart Farming agent. Left: Edge detection visualizing the crop/soil boundary intersection. Right: Equipment detection highlighting the target tool for attachment.}
    \label{fig:farm_debug}
\end{figure}

\textbf{State Machine Logic}
The mission is governed by a hierarchical YASMIN state machine \cite{YASMIN} (Figure \ref{fig:farm_state_machine}) which executes two distinct phases:

\textbf{Phase 1: Field Mapping (Initialization)}
\begin{enumerate}
    \item \textbf{CheckOnField:} The robot searches the field using a spiral search pattern until it verifies it is inside the field boundaries.
    \item \textbf{DriveToEdge:} The robot drives forward until the \texttt{EdgeDetectionProcessor} identifies a field boundary. It then executes a forward drive for a calibrated distance (e.g., 20cm) to account for the camera's blind spot, ensuring the robot's center of rotation is aligned with the edge.
    \item \textbf{AdjustToEdge:} The robot rotates in place until the \texttt{EdgeAdjustmentProcessor} confirms that the edge line is vertical (angular error near zero) and centered.
    \item \textbf{FollowEdge (Mapping):} The robot circumnavigates the field by tracing its boundary. A PID controller using the lateral error computes the required velocity commands and steers the robot. Crucially, this action detects field corners by monitoring the robot's odometry. A sharp change in yaw (approx. 90 degrees) triggers the recording of the robot's current pose. Once four corners are recorded, the field boundary is defined.
\end{enumerate}

\textbf{Phase 2: Equipment Cycle}
\begin{enumerate}
    \item \textbf{FindEquipment:} The robot executes a spiral search pattern using the Nav2 stack~\cite{NAV2Ref} to locate a specific tool.
    \item \textbf{EquipTool:} Upon detection, the robot approaches the tool. The action triggers the Unity \texttt{AttachmentCommandManager} via the \texttt{/robot\_command} topic to kinematically lock the tool to the robot.
    \item \textbf{GenerateCoveragePath:} Using the four corners identified in Phase 1, this action calculates a boustrophedon (lawnmower) path that covers the entire field area.
    \item \textbf{ExecuteCoverage:} The robot drives the generated path. During this phase, the Unity \texttt{TrackPainter} modifies the ground texture to visually represent tilled soil.
    \item \textbf{Return \& Unequip:} The robot returns the tool to its origin and detaches it using the \texttt{/robot\_command}. The state machine then loops back to search for the next tool in the sequence (Seeder, then Harvester).
\end{enumerate}

\begin{figure}[H]
    \centering
    %\includegraphics[width=0.8\textwidth]{figures/farm_agent_fsm} 
    \caption{The Smart Farming State Machine. The robot first maps the field boundaries (FollowEdge) to identify corners, then uses these corners to generate coverage paths for various tools.}
    \label{fig:farm_state_machine}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%fertig aber ai%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Agent 2: Logistics}
\label{subsec:agent_logistics}

\textbf{Scenario Context}
The Logistics agent operates in a warehouse environment populated with colored transport boxes and corresponding projected delivery zones. The robot's objective is to autonomously search for items, physically attach them, identify the correct sorting destination, and transport the payload.

\textbf{Perception Modes}
The agent employs the modular \texttt{perception\_node.py}, switching between two specialized processors based on the mission phase:
\begin{enumerate}
    \item \textbf{Box Detection (\texttt{BoxDetectionProcessor}):} This processor isolates manipulatable objects from the background. It applies an HSV color mask to filter target colors and utilizes \texttt{cv2.findContours} to identify object blobs, filtering them by a minimum area threshold to reject sensor noise. The centroid of the largest valid contour is calculated using image moments (\texttt{cv2.moments}) to provide a steering target.
    \item \textbf{Zone Detection (\texttt{ZoneDetectionProcessor}):} This processor is tuned to detect the flat, projected delivery zones on the floor. While similar to box detection, it uses distinct HSV ranges and calculates the zone's center.
\end{enumerate}

To assist in debugging, the perception node publishes an annotated video stream (Figure \ref{fig:logistics_debug}). The system draws bounding contours around valid targets and overlays classification labels (e.g., "RED BOX", "Distance: 1.2m") and the active search status directly onto the feed.

\begin{figure}[H]
    \centering
    %\includegraphics[width=0.8\textwidth]{figures/logistics_perception_debug} 
    \caption{Debug view of the Logistics agent. The processor identifies a red transport box, drawing a contour around it and calculating the centroid (red dot) for the approach vector.}
    \label{fig:logistics_debug}
\end{figure}

\textbf{State Machine Logic}
The mission logic is defined in \texttt{mission\_state\_machine.py} (Logistics variant) and implements a "Search-Retrieve-Deliver" loop. A key feature of this agent is the utilization of the YASMIN \textbf{Blackboard} \cite{YASMIN} to implement spatial memory, allowing the robot to learn the environment structure over time.

\begin{enumerate}
    \item \textbf{FindBox:} The robot executes the \texttt{find\_box\_action\_server}. This generates a spiral pattern of waypoints using the Nav2 stack~\cite{NAV2Ref} to efficiently cover the floor plan. The robot navigates these waypoints until the \texttt{BoxDetectionProcessor} identifies a valid target in the camera's Region of Interest.
    \item \textbf{AttachBox:} Upon reaching the target, the robot performs a predefined approach. The \texttt{AttachBox} action sends a string command via the \texttt{/robot\_command} topic to the Unity \texttt{AttachmentCommandManager}, which kinematically locks the virtual box to the robot's chassis for transport.
    \item \textbf{Spatial Memory Query:} Before searching for the destination, the agent queries the Blackboard. If the location of the matching delivery zone (e.g., "Red Zone") was discovered and cached during a previous traversal or was given by the user, the search phase is skipped.
    \item \textbf{FindDeliveryZone / NavigateToZone:} If the location is unknown, the robot triggers \texttt{FindDeliveryZone} to execute a new spiral search. If the location is known, it uses \texttt{NavigateToZone} to drive directly to the stored coordinates.
    \item \textbf{VerifyZoneColor (Recovery):} Upon arriving at the target, the robot executes the \texttt{VerifyZoneColor} action. This serves as a critical reliability check. If the camera view is obstructed (e.g., by the carried box) or odometry drift has occurred, the robot performs a "Spin Recovery" maneuver. It rotates 90 degrees left and right to re-acquire the visual target before confirming the drop.
    \item \textbf{DropBox:} Finally, the robot drives forward into the zone and triggers the detach command via the \texttt{/robot\_command}, releasing the object into the sorting area.
\end{enumerate}

\begin{figure}[H]
    \centering
    %\includegraphics[width=0.8\textwidth]{figures/logistics_agent_fsm} 
    \caption{The Logistics Agent State Machine. The logic flows from searching for boxes to delivering them, utilizing the Blackboard to store and recall zone locations to optimize efficiency over time.}
    \label{fig:logistics_state_machine}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%fertig aber ai%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Agent 3: Line Follower}
\label{subsec:agent_line}

\textbf{Scenario Context}
The Line Follower agent operates in an interactive playground where a user using the robot, a mouse or in VR can draw paths on the virtual ground in real-time. The robot must identify and track this line immediately.

\textbf{Perception Modes}
Unlike the other agents, this implementation utilizes the dedicated node \texttt{line\_perception\_node.py}:
\begin{itemize}
    \item \textbf{ROI Cropping:} The image is cropped to the \textbf{bottom 50\% (configurable via parameters)}, focusing solely on the immediate path in front of the robot. This removes background noise and simplifies the processing required.
     \item \textbf{Line Fitting:} The processor applies a color threshold to isolate the high-contrast line drawn by the user. It then utilizes \texttt{cv2.fitLine} to fit a vector through the detected white pixels.
    \item \textbf{Error Calculation:} From this vector, the system calculates two control metrics: the lateral error (horizontal distance of the line from the image center) and the \textit{Angular Error} (deviation of the line's heading from the robot's forward vector).
\end{itemize}
The debug view (Figure \ref{fig:line_debug}) visualizes the binary mask and overlays the calculated heading vector, allowing the operator to see the error terms driving the control loop.

\begin{figure}[H]
    \centering
    %\includegraphics[width=0.8\textwidth]{figures/line_perception_debug} 
    \caption{Debug view of the Line Follower. The system isolates the user-drawn path (binary mask) and fits a vector (blue line) to calculate lateral and angular errors for the PID controller.}
    \label{fig:line_debug}
\end{figure}

\textbf{State Machine Logic}
While the previous agents relied on the Nav2 stack~\cite{NAV2Ref} for motion planning, this agent implements a custom \textbf{Visual Servoing} control loop. The \texttt{mission\_state\_machine.py} orchestrates a three-stage sequence:

\begin{enumerate}
    \item \textbf{FindLine:} The robot executes a spiral search pattern using the \texttt{find\_line\_action\_server} and Nav2 stack to locate the start of the user-drawn path.
    \item \textbf{AlignToLine:} Once the line is detected, the robot transitions to the \texttt{AlignToLine} action. The robot rotates in place (zero linear velocity) until the angular error returned by the perception node is minimized, ensuring the robot is parallel to the path before moving.
    \item \textbf{FollowLine:} The robot engages a PID controller implemented in \texttt{follow\_line\_action\_server.py}. This controller calculates velocity commands (\texttt{cmd\_vel}) directly from the perception error values. By bypassing the global path planning and costmap layers of ~\cite{NAV2Ref}, this direct control loop achieves the low latency required to react to sudden changes in the user-drawn line. If the line is erased or ends, the state machine transitions back to \texttt{FindLine} to search for a new path.
\end{enumerate}

\begin{figure}[H]
    \centering
    %\includegraphics[width=0.6\textwidth]{figures/line_follower_fsm} 
    \caption{The Line Follower State Machine. The logic ensures the robot first locates and aligns with the path before entering the high-speed PID following loop.}
    \label{fig:line_fsm}
\end{figure}