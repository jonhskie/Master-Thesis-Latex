\chapter{Concept and Implementation}
\label{ch:implementation}

\cite{NOT FINAL}This chapter details the realization of the Mixed Reality Environment based on the requirements defined in Chapter 3. It begins with the selection of the simulation engine, followed by the system architecture, and concludes with the implementation of specific interaction logic and the autonomous ROS 2 agents.\cite{NOT FINAL}

\section{Technology Selection}
\label{sec:tech_selection}

To fulfill the platform capabilities defined in Section \ref{sec:platform_reqs} (Requirements FR-01 to FR-04), it is essential to select a suitable simulation engine. This section evaluates four industry-standard platforms: Gazebo, NVIDIA Isaac Sim, Unreal Engine, and Unity.

\subsection{Comparative Analysis}
The candidates were evaluated based on five key criteria: visual fidelity, physics capabilities, learning curve, community support, and native integration with XR and ROS 2. The results of this comparison are summarized in Table \ref{tab:sim_comparison}.

\begin{table}[H]
    \centering
    \caption{Comparison of Simulation Platforms for Mixed Reality Digital Twins~\cite{Gonzalez2025, Kargar2024, Singh2025, Coronado2023}.}
    \label{tab:sim_comparison}
    \begin{tabularx}{\textwidth}{@{}lXXXX@{}}
        \toprule
        \textbf{Feature} & \textbf{Gazebo} & \textbf{Isaac Sim} & \textbf{Unreal Engine} & \textbf{Unity} \\ 
        \midrule
        \textbf{Primary Use} & Control \& Navigation & AI \& Photorealism & Photorealism & HRI \& MR (VR/AR) \\ 
        \textbf{Visual Fidelity} & Moderate & Very High & Very High & High \\ 
        \textbf{Physics Engine} & ODE / Bullet & PhysX 5 (GPU) & Chaos / PhysX & PhysX \\ 
        \textbf{Learning Curve} & Steep & Advanced & Steep (C++) & Moderate (C\#) \\ 
        \textbf{Community} & High (ROS) & Moderate & High (Gaming) & Very High (MR) \\ 
        \textbf{ROS 2 Integ.} & Native & Bridge & Bridge & Plugin (Native) \\ 
        \textbf{Hardware} & Low & Very High (RTX) & High & Moderate \\ 
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Selection Rationale}
Based on the comparative analysis, the \textbf{Unity Engine} was selected as the implementation platform for this thesis. This decision is driven by four key factors that align with the system requirements:

\begin{itemize}
    \item \textbf{XR Framework (FR-03):} Unity offers a mature and integrated framework for Mixed Reality (AR/VR) applications~\cite{Coronado2023}. In contrast, traditional simulators like Gazebo lack native VR support, which is critical for the proposed human-robot interaction interface.
    
    \item \textbf{Visual \& Physics Balance (FR-01, FR-02):} Unity provides high-fidelity visualization alongside robust PhysX integration. This offers an optimal balance between performance and visual quality, avoiding the restrictive hardware requirements (e.g., RTX GPUs) associated with NVIDIA Isaac Sim~\cite{Gonzalez2025}.
    
    \item \textbf{Development Efficiency:} The use of C\# scripting, combined with extensive documentation and community support, makes Unity significantly more accessible for rapid prototyping than the C++ environment of Unreal Engine~\cite{Coronado2023}.

    \item \textbf{ROS 2 Integration (FR-04):} The availability of the \texttt{ros2-for-unity} asset enables the simulation to function as a first-party participant in the ROS 2 network. This satisfies the low-latency communication requirement by eliminating the need for external bridge applications~\cite{Rob24}.
\end{itemize}

% ==============================================================================
% 4.2 SYSTEM ARCHITECTURE
% ==============================================================================
\section{System Architecture}
\label{sec:system_architecture}


\subsection{High-Level Overview}
The system is divided into three logical domains: the Physical Space containing the robot and tracking hardware, the Virtual Space hosting the Digital Twin and scenarios, and the ROS 2 middleware acting as the central data backbone (FR-05).

\subsection{Network Topology}
To ensure modularity (NFR-03), the system utilizes a strict namespace separation between simulation nodes and autonomous agent nodes. This topology allows the agents to operate agnostic of whether they are controlling the real or virtual robot.

% ==============================================================================
% 4.3 UNITY AND ROS 2 INTEGRATION
% ==============================================================================
\section{Integration of Unity and ROS 2 (The Environment)}
\label{sec:unity_integration}

This section details the technical implementation of the simulation environment and the Digital Twin within the Unity Engine.

\subsection{The ROS 2 for Unity Plugin}
% Introduced FIRST, so we can reference it in the next subsection.
The \texttt{ros2-for-unity} asset is configured to instantiate the ROS 2 Client Library (rcl) directly within the Unity process memory. This enables high-throughput communication without external bridge applications (FR-04).

\subsection{Simulation Time Management}
% Now we describe the implementation using the plugin introduced above.
To prevent data drift between the simulation physics and the robot's control algorithms, a custom C\# script was developed. This script utilizes the plugin interface to publish the \texttt{/clock} topic, establishing Unity as the authoritative time source for the entire ROS 2 network (FR-06).

\subsection{Physical Twin Implementation}
The Digital Twin of the EMAROS robot incorporates the physical properties of the real hardware, including mass and inertia (FR-02). Its pose is synchronized in real-time using data from the external ArUco tracking system (FR-09).

\subsection{Sensor Simulation}
\begin{itemize}
    \item \textbf{Virtual Camera:} A Unity rendering camera captures the scene and converts the frame buffer into standard ROS \texttt{sensor\_msgs/Image} messages (FR-13).
    \item \textbf{LiDAR Raycasting:} The 2D laser scanner is simulated using Unity's physics raycasting API to detect obstacles and publish \texttt{sensor\_msgs/LaserScan} data (FR-14).
\end{itemize}

% ==============================================================================
% 4.4 MIXED REALITY INTERFACE
% ==============================================================================
\section{Mixed Reality Interface (The User View)}
\label{sec:mr_interface}

To support Human-Robot Interaction, the system implements both Augmented Reality projections and Virtual Reality interfaces.

\subsection{AR Projection}
An orthographic camera is configured to render a top-down view of the scene, which is projected 1:1 onto the physical testbed floor to create the Augmented Reality environment (FR-31).

\subsection{VR Interaction}
The VR interface allows a user to view the Digital Twin in 3D space. It includes "God-Mode" interaction capabilities, allowing users to manipulate virtual objects or set navigation goals directly using VR controllers (FR-33).

% ==============================================================================
% 4.5 SCENARIO LOGIC (UNITY-SIDE)
% ==============================================================================
\section{Implementation of Scenario Logic (Unity-Side Behaviors)}
\label{sec:unity_scenarios}

While the robot is controlled by external agents, the rules of the environment and the behaviors of virtual objects are implemented via C\# scripts within Unity.

\subsection{Smart Farming Environment}
The environment manages the state of field segments, visually toggling them between "Untouched" (green) and "Cultivated" (brown) when the robot's tool interacts with them (FR-20).

\subsection{Logistics and Navigation Environment}
This scenario implements "Collectible Objects" that attach to the robot upon contact (FR-22) and dynamic street segments that automatically rearrange themselves to test adaptive navigation (FR-26).

\subsection{Surface Painting Mechanics}
To enable the Line Following scenario, a texture modification script allows users to draw paths directly onto the virtual floor mesh in real-time (FR-24).

\subsection{Competitive Game and Unity Opponent}
The competitive scenario includes a physics-based scoring system. Unlike the external ROS agents, the \textbf{Automated Adversary} is implemented entirely within Unity to track the game object and react to the player's movements with zero network latency (FR-30).

% ==============================================================================
% 4.6 AUTONOMOUS AGENTS (ROS-SIDE)
% ==============================================================================
\section{Implementation of Autonomous Agents (ROS-Side)}
\label{sec:autonomous_agents}

To validate the framework, specific autonomous agents were developed for each scenario. These agents share a common architecture based on the ROS 2 Navigation Stack (Nav2) and Finite State Machines (FSM), but utilize distinct perception logic.

\subsection{General Agent Architecture}
Each agent follows a \textbf{Two-Node Pattern} to decouple perception from decision-making:
\begin{enumerate}
    \item \textbf{Perception Node:} Processes raw sensor data (Camera/LiDAR) and publishes high-level features.
    \item \textbf{Control Node:} Executes a Finite State Machine (FSM) that sends goals to the Nav2 Action Server (FR-37).
\end{enumerate}

\subsection{Smart Farming Agent}
\textbf{Perception:} The perception node utilizes OpenCV to perform color-based segmentation, distinguishing between green (unharvested) and brown (harvested) zones.\\
\textbf{Control:} The FSM directs the robot to systematically cover unharvested areas while managing the tool state.

\subsection{Logistics Agent}
\textbf{Perception:} This node implements blob detection to identify colored "Collectible Objects" and recognizes the geometry of "Goal Zones."\\
\textbf{Control:} The logistics FSM executes a four-stage sequence: \textit{Search} $\rightarrow$ \textit{Approach} $\rightarrow$ \textit{Attach} $\rightarrow$ \textit{Deliver}.

\subsection{Line Following Agent}
\textbf{Perception:} Using the synthetic camera feed, this node applies contour detection algorithms to identify user-drawn lines or street markings.\\
\textbf{Control:} Instead of global path planning, this agent utilizes a \textbf{Visual Servoing} state. It bypasses the Nav2 planner to send direct velocity commands (\texttt{cmd\_vel}) to align the robot with the detected line trajectory (FR-27).

\subsection{Abstraction of Input Sources}
A key feature of these agents is their source agnosticism (FR-36). By remapping the input topics, the exact same Perception Node can process either the synthetic video stream from Unity or the physical camera feed from the real world without code modification.

% ==============================================================================
% 4.7 SUMMARY
% ==============================================================================
\section{Summary of Implementation}
This chapter described the technical realization of the Mixed Reality Environment. By integrating the Unity Engine with ROS 2, a robust Digital Twin was created. The validity of this system is demonstrated by the implementation of complex scenarios and autonomous agents that can operate interchangeably in both virtual and physical contexts.