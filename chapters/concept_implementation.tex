\chapter{Concept and Implementation}
\label{ch:concept_and_implementation}

This chapter describes the development of the Mixed Reality Environment, a platform designed to support RitL testing by combining simulation with augmented reality projection. The resulting system bridges the gap between purely digital simulations and real-world experiments by creating a common workspace where physical robots and virtual objects coexist and interact.

The central concept of this environment is the projection of a simulated world directly onto the laboratory floor. Rather than testing robots in completely physical setups or purely within software simulations, this system projects dynamic, physics-based elements into the real world. As can be seen from Figure \ref{fig:mr_environment_overview}, the physical robot moves across the real floor while sensing and acting upon virtual objects, such as a field, obstacles, or other entities. This projection has a dual purpose: it provides instant visual feedback to a human observer and enables the robot to receive synthetic sensor data that reflect the state of the virtual scene. This allows the robot to detect and respond to simulated entities as if they were physically present.

\begin{figure}[ht]
\centering
%\includegraphics[width=0.9\textwidth]{figures/mr_environment_overview} % Placeholder for your figure
\caption{The Mixed Reality Environment in operation. The physical robot interacts with a projected Smart Farming scenario, where the robot and the digital twin are synchronized via ROS~2.}
\label{fig:mr_environment_overview}
\end{figure}

To enable these applications, the system integrates several key functions into an architecture. First, it maintains a dynamic twin of the robot. This digital twin is synchronized with the movement of the physical hardware but can interact with the simulation's physics engine. This allows for complex actions, such as pushing virtual boxes or colliding with moving objects. This ensures the simulation responds realistically to the robot's physical presence rather than serving as a static background.

In addition to maintaining a dynamic digital twin, the system also simulates sensor for the robot. It generates virtual LiDAR scans and camera images from the robot's perspective, allowing autonomous agents to receive virtual sensordata. This enables thorough testing of navigation and vision algorithms within the mixed reality environment.

The architecture uses ROS~2~\cite{MFG22a} as the primary communication backbone. By integrating a ROS~2 nodes directly into the simulation engine, the virtual environment acts as a first-party participant in the robot's network. It manages time synchronization to prevent timing related errors, handles the loading of different scenarios and provides two-way exchange of status data and control commands.

Besides sensor simulation, the environment also supports real-time diagnostic feedback. The system functions as a monitoring tool by projecting the robot's internal operating data back onto the physical workspace and the VR interface. Performance metrics, such as temperature, RAM usage, and location coordinates, are shown on information panels attached to the digital twin in the 3D environment. Moreover, the system shows the robot's live camera feed within the virtual scene. This allows operators to observe the robot's physical behavior alongside its internal status and visual perception in real-time.

To demonstrate the flexibility and reliability of this architecture, three different Autonomous Agents were implemented:
\begin{itemize}
\item A Smart Farming Agent that autonomously explores a virtual field, identifies its boundaries, and operates simulated tools to perform fieldwork tasks.
\item A Logistics Agent that autonomously searches for, identifies and sorts colored objects within the environment and brings them to designated zones.
\item A Line Following Agent that uses visual tracking to follow paths drawn by a human user in real-time.
\end{itemize}

The platform provides an adaptable and scalable testing solution by integrating physics, sensor simulation, and autonomous control logic into a synchronised mixed reality environment. The only factor limiting the test environment's complexity is software, not its physical design. The technological decisions, system architecture, and particular implementation of these elements will be covered in detail in the sections that follow.
%%%%%%%%%%%%%%%%%%%%%FERTIG%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Comparative Analysis}
The simulation engines that were considered as candidates to implement the Mixed Reality Environment are: Gazebo, Isaac Sim, Unreal Engine, and Unity. All were evaluated based on five critical criteria: visual fidelity, physics capabilities, learning curve, community support, and native integration with VR, AR and ROS~2. Table \ref{tab:sim_comparison} summarizes the results of this evaluation.

\begin{table}[H]
    \centering
    \caption{Comparison of Simulation engines for Mixed Reality Digital Twins~\cite{Gonzalez2025, Kargar2024, Singh2025, Coronado2023}.}
    \label{tab:sim_comparison}
    \begin{tabularx}{\textwidth}{@{}lXXXX@{}}
        \toprule
        \textbf{Feature} & \textbf{Gazebo} & \textbf{Isaac Sim} & \textbf{Unreal Engine} & \textbf{Unity} \\ 
        \midrule
        \textbf{Primary Use} & Control \& Navigation & AI \& Photorealism & Photorealism & HRI \& MR (VR/AR) \\ 
        \textbf{Visual Fidelity} & Moderate & Very High & Very High & High \\ 
        \textbf{Physics Engine} & ODE / Bullet & PhysX 5 (GPU) & Chaos / PhysX & PhysX \\ 
        \textbf{Learning Curve} & Steep & Advanced & Steep (C++) & Moderate (C\#) \\ 
        \textbf{Community} & High (ROS) & Moderate & High (Gaming) & Very High (MR) \\ 
        \textbf{ROS~2 Integ.} & Native & Bridge & Bridge & Plugin (Native) \\ 
        \textbf{Hardware} & Low & Very High (RTX) & High & Moderate \\ 
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Selection Rationale}
Based on the comparative analysis, the \textbf{Unity Engine} was selected as the implementation platform for this thesis. This decision is driven by four key factors that align with the system requirements:

\begin{itemize}
    \item \textbf{XR Framework (FR-09):} Unity has an integrated framework for VR applications~\cite{Coronado2023}. In contrast, simulators like Gazebo lack native VR support, which is critical for the proposed human-robot interaction interface.
    
    \item \textbf{Visual \& Physics Balance (FR-04, FR-05):} Unity provides high-fidelity visualization alongside PhysX integration. This offers an optimal balance between performance and visual quality, avoiding the restrictive hardware requirements associated with NVIDIA Isaac Sim~\cite{Gonzalez2025}.
    
    \item \textbf{Development Efficiency:} The use of C\# scripting, combined with documentation and community support, makes Unity more accessible for rapid prototyping than the C++ environment of Unreal Engine~\cite{Coronado2023}.

    \item \textbf{ROS~2 Integration (FR-01):} The availability of the \texttt{ros2-for-unity} asset enables the simulation to function as a first-party participant in the ROS~2 network. This means it supports the low-latency communication requirement by avoiding external bridge applications~\cite{Rob24}.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%FERTIG%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{System Architecture}
\label{sec:system_architecture}

The realization of the Mixed Reality Environment requires a distributed software architecture that connects the advanced rendering capabilities of the Unity engine with real-time robotic control. The system is designed to satisfy the requirement for \textbf{Middleware Integration (FR-04)} by establishing a one unified communication layer between the simulation host and the autonomous agents.

\subsection{Hardware and Network Topology}
\label{subsec:hardware_topology}

To ensure high performance and support for specific hardware peripherals, the system topology is consolidated into a single high-performance workstation and the mobile robot. As shown in Figure \ref{fig:hardware_topology},  the architecture is divided into three clear computational domains:

\begin{enumerate}
    \item \textbf{Simulation Host (Windows 11):} The Unity engine runs natively on Windows. This operating system was chosen due to currently missing support of VR interfaces in the Unity Editor under Linux. This layer manages the physics engine (PhysX), renders the digital twin, and communicates with the VR headset. Additionally, a projector is connected to this host to visualize the environment on the physical floor \cite{PLACEHOLDER_HARDWARE_SETUP}.
    
    \item \textbf{Control Environment (WSL/Ubuntu):} ROS~2~Jazzy runs within the Windows Subsystem for Linux within Ubuntu 24.04. This environment hosts the computationally intensive software, such as the autonomous navigation stack (\textit{Nav2})~\cite{NAV2Ref} and the mission logic agents. Running these components directly on the robot causes performance issues or system crashes due to limited onboard resources, so they are offloaded to the workstation to ensure stable and responsive operation.
    
    \item \textbf{Physical Robot (Hardware Layer):} The EMAROs~\cite{EMAROs_Ref} robot operates as an independent node in the network. It uses Ubuntu 24.04 and runs ROS~2~Jazzy on an onboard Raspberry Pi to handle hardware drivers, such as motor controllers and IMU sensors. It connects to the workstation via Wi-Fi.
\end{enumerate}

\begin{figure}[ht]
    \centering
    % TODO: Create a diagram showing:
    % 1. PC Box (split into Windows and WSL sections).
    % 2. Windows connected to Projector (HDMI) and VR Headset (Wi-Fi).
    % 3. WSL connected to the internal network (localhost).
    % 4. Robot connected via Wi-Fi to the Router.
    %\includegraphics[width=1.0\textwidth]{figures/hardware_topology} 
    \caption{Hardware and Network Topology. The workstation handles both the Unity simulation (Windows) and the heavy control logic (Ubuntu/WSL) to preserve the robot's onboard resources. The robot, VR headset, and projector connect as peripherals to this central core.}
    \label{fig:hardware_topology}
\end{figure}

For the Mixed Reality interface \textbf{(FR-16)}, the visual output from Unity is streamed via Wi-Fi to the Meta Quest Pro headset using \textit{ALVR}~\cite{ALVR_Github}, an open-source streaming solution. This setup allows the user to move freely without a cable tether.

\subsection{Software Component Architecture}
\label{subsec:software_components}

The software architecture follows a modular publisher-subscriber pattern. To integrate the Unity game engine with the robotic network, the system utilizes the \textit{ROS2ForUnity} library \cite{ROS2ForUnityDocs}.

\textbf{ROS~2 Integration via MonoBehavior}

The core of this integration is the \texttt{ROS2UnityComponent}~\cite{ROS2ForUnityDocs} class. This component acts as a wrapper around the standard ROS~2 middleware. Inheriting from Unity's \texttt{MonoBehaviour} class, it must be attached to a \texttt{GameObject} in the scene to function. Upon startup, this component initializes the ROS~2 context, makes sure that a valid connection is established, and handles the lifecycle of the ROS~2 nodes running within the simulation. It allows other scripts to access the ROS~2 network by referencing this central component to create publishers, subscribers, and service clients.

\textbf{Unity Scripting and Execution Loops}

The functionality of the digital twin is driven by C\# scripts that interact with the engine's internal state machine. To ensure that physical simulation remains deterministic while visualization remains smooth, the architecture uses specific phases of the Unity execution loop~\cite{UnityDocumentation}:

\begin{itemize}
    \item \textbf{Initialization (\texttt{Start}):} This method is called once when the script is first enabled, before any frames are updated. In this architecture, it is used to retrieve the reference to the \texttt{ROS2UnityComponent}~\cite{ROS2ForUnityDocs}, initialize the specific ROS nodes, and set up the necessary publishers and subscribers.
    
    \item \textbf{Physics Loop (\texttt{FixedUpdate}):} This method executes at a constant, user-defined time step, independent of the visual frame rate. All physics-based calculations, such as updating the robot's position based on received velocity commands or handling collision detection, occur in this loop. This ensures that the robot's physical behavior is consistent regardless of graphical performance.
    
    \item \textbf{Rendering Loop (\texttt{Update}):} This method runs once every frame. It is utilized for logic that is not physics-related, such as updating UI elements, rendering diagnostic lines, or capturing camera frames for visualization. This separation ensures that high-frequency visual updates do not interfere with the stability of the physics engine.
\end{itemize}

Figure \ref{fig:software_architecture} provides an overview of how these Unity scripts interact with the external ROS~2 nodes via topics. Detailed descriptions of these components follow in Sections 4.2 and 4.3.

\begin{figure}[ht]
    \centering
    % TODO: Create a diagram showing:
    % Left side: Unity Components (LidarSensor, CameraSensor, VirtualRobotController).
    % Middle: Arrows representing ROS Topics (/scan, /camera/image, /cmd_vel).
    % Right side: ROS Nodes (Nav2 Stack, Perception Node, Mission State Machine).
    %\includegraphics[width=1.0\textwidth]{figures/software_architecture} 
    \caption{Software Component Architecture. Unity scripts act as ROS nodes, publishing sensor data and subscribing to control commands using ROS~2 topics. The external control stack, running in WSL, processes this data to make autonomous decisions.}
    \label{fig:software_architecture}
\end{figure}

\subsection{Time Synchronization}
\label{subsec:time_sync}

A critical challenge in Robot-in-the-Loop testing is how to synchronize time between the simulation and the robot's software. If the simulation runs slower or faster than real-time, the navigation algorithms of the robot may calculate velocities that are incorrect. To fulfill the requirement for \textbf{Time Synchronization (FR-05)}, the system decouples the ROS network from the computer's system time.

The \texttt{ClockPublisher.cs} script in Unity acts as the master clock. It publishes the current simulation time to the \texttt{/clock} topic. The ROS~2 nodes running on the workstation such as the navigation stack are configured with \texttt{use\_sim\_time = true}, forcing them to synchronize their logic with the Unity engine.

To ensure consistency, a \texttt{RosTimeHelper.cs} utility is used by all simulated sensors. This script ensures that sensor data leaving Unity is stamped with the exact simulation time of the frame it was rendered in to prevent data drift.

\subsection{Scenario Management}
\label{subsec:scenario_management}

To support the requirement for \textbf{Scenario Management (FR-06)}, the architecture allows the system to switch between different test environments without restarting the entire application.

This is handled by the \texttt{SceneManager.cs} script. When it receives a command via the \texttt{/vera/load\_scenario} topic, it performs a clean reset sequence:
\begin{enumerate}
    \item \textbf{Shutdown:} The active ROS~2 connections are terminated. This ensures that all Unity nodes are properly removed from the network, preventing any lingering nodes from publishing outdated or invalid data.
    \item \textbf{Load:} Unity loads the new environment scene asynchronously.
    \item \textbf{Restart:} A fresh \texttt{ROS2UnityComponent}~\cite{ROS2ForUnityDocs} initializes, creating a new connection for the loaded scenario.
\end{enumerate}
This fresh start approach ensures that Unity components specific to one scenario do not interfere with others.

%%%%%%%%%%%%%%%%%%%%%FERTIG%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Robot Representation and Control}
\label{sec:robot_representation_and_control}
The robot in the Mixed Reality Environment runs in one of two modes: either as a completely simulated Virtual Robot or as a synchronized Digital Twin of the physical EMAROs~\\cite{EMAROs_Ref} platform. To ensure consistency, both of these modes use the same visual and geometric model. This model is a simple cylinder that approximates the physical size of the robot for collision detection while keeping computational costs low. The behavior of this cylinder depends on the active control script, allowing the system to switch between physics-based simulation and position-based synchronization based on the requirements of the test.

\subsection{The Virtual Robot Implementation}
In the standalone simulation mode, the robot is controlled by the \texttt{VirtualRobotController}. This component allows for the validation of high-level control logic in a safe environment before it is deployed on real hardware. Unlike simulations that teleport a robot to a target point, this implementation uses the Unity physics engine PhysX to simulate realistic movement (FR-09).

The controller acts as a ROS~2 node that accepts velocity commands (\texttt{cmd\_vel}) and publishes odometry data. To handle data exchange between the asynchronous ROS~2 thread and the Unity main thread, incoming commands are buffered in a thread-safe \texttt{ConcurrentQueue}. During the physics step of Unity (\texttt{FixedUpdate}), the controller retrieves the latest command and applies it to the \texttt{Rigidbody} of the robot.

The script does not directly change the position of the robot. Instead, it modifies the velocity of the physics body. By applying limits to acceleration, the system simulates mass and inertia for the robot. The robot can push lightweight virtual objects, but heavy obstacles or walls will stop its movement. This provides realistic feedback to the navigation system.

Simultaneously, the controller provides ground truth data. It tracks the movement of the robot in Unity, converts the coordinates to the ROS standard, and publishes a timestamped odometry message. This closes the control loop for the external navigation logic~\cite{MFG+22}.

\subsection{The Digital Twin Implementation}
When operating in RitL mode, the \texttt{PoseRobotController} takes control. In this configuration, the virtual robot stops acting as a dynamic entity and instead becomes a digital twin of the real EMAROss~\cite{EMAROs_Ref} robot (FR-07).

The controller listens to a pose topic given by the external ArUco tracking. On every new incoming pose, the script converts the coordinates into Unity world space. To ensure alignment, an offset parameter enables the operator to perform a calibration of the Digital Twin so that its center is aligned with the physical mounting position of the tracking marker.

Unlike the Virtual Robot, the Digital Twin overrides the physics engine. It is set to a kinematic state, meaning it is immune to virtual forces. If the physical robot moves, the Digital Twin moves with it, ignoring virtual barriers. This allows the physical robot to push virtual objects with absolute force. This way, the state of the virtual world always reflects physical reality.
\begin{figure}[h]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/control_architecture.png}
    % use here the real virtaul image of the robots, maybe make them in diffrent color. add pfeile to show data flow
    \caption{Data flow comparison between the Virtual Robot and the Digital Twin. The Virtual Robot (left) is driven by internal physics and ROS commands, while the Digital Twin (right) is driven directly by external tracking data.}
    \label{fig:control_architecture}
\end{figure}

\subsection{Safety and Failsafe Mechanisms}
A major challenge in mixed reality testing is signal latency or loss. If the physical robot leaves the tracking area or if the ArUco marker is blocked, position updates will stop. Without a failsafe, the Digital Twin would freeze while the physical robot continues to move blindly, causing a mismatch between reality and simulation (FR-08).

To prevent this, the \texttt{PoseRobotController} implements a safety watchdog. The system continuously checks how long it has been since receiving the last pose update. If this delay exceeds a given safety limit, the system assumes tracking has been lost.

When this occurs, the controller marks the tracking state as invalid. Although the Digital Twin stops moving in the simulation, the system is designed to send a stop command to the motors of the physical robot. This ensures the robot does not perform actions without simulation guidance. Once the tracking system re-acquires the robot, the watchdog resets and the Digital Twin resumes synchronization immediately.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FERTIG%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Environmental Simulation Capabilities}
\label{sec:environmental_simulation_capabilities}

To serve as a valid testbed for autonomous agents, the virtual environment must provide more than static geometry. It must facilitate mechanical manipulation of objects, generate synthetic perception data that mirrors real hardware, and support the persistent modification of environmental states. This section details the technical implementation of these capabilities, addressing the requirements for Sensor Simulation (FR-11), Dynamic Environment Interaction (FR-13), and the Command Interface (FR-12).

\subsection{Physics-Based Interaction}
A critical feature of the system is the ability for the robot to mechanically couple with environmental objects, such as agricultural tools or logistics containers. While standard game engines often use physics joints to connect rigid bodies, these can introduce instability and jitter when subjected to the high-frequency velocity changes typical of robotic controllers. To address this, the architecture implements a robust kinematic attachment system managed by the \texttt{AttachmentCommandManager} and the \texttt{RobotAttachmentController}.

\textbf{Command Interface and Logic}

To enable interoperability with the ROS~2 ecosystem (FR-12), the system exposes a high-level command interface via the \texttt{/robot\_command} topic. This interface utilizes standard string-based messages (e.g., "attach,plow" or "detach") rather than complex service calls. This design choice simplifies the debugging process, allowing developers to trigger complex physical sequences manually via the ROS~2 command line interface (CLI) without requiring custom service clients. The command manager operates on the Unity main thread, processing a thread-safe queue of incoming messages. When an attachment command is received, it performs a spatial query using \texttt{Physics.OverlapSphere} to identify valid target objects within a defined search radius.

\textbf{Kinematic Coupling Strategy}

Once a valid target is identified, the controller executes a kinematic handover. The target object is re-parented to the robot's transform hierarchy at a specific mount point defined by local offsets. Crucially, the system modifies the physics state of the attached object (FR-13). Upon attachment, the script sets the object's \texttt{Rigidbody.isKinematic} property to true. In the Nvidia PhysX engine, this removes the object from the dynamic simulation loop, effectively rendering it massless relative to the robot. This ensures that the robot's navigation controller does not need to compensate for the sudden addition of mass or the shifting center of gravity, resulting in stable and predictable movement during transport.

Simultaneously, the controller modifies the collision detection matrix using \texttt{Physics.IgnoreCollision}. This explicitly suppresses collision checks between the robot's chassis colliders and the attached tool's colliders. Without this suppression, the instantaneous snapping of the tool to the robot's mount point would result in immediate interpenetration of geometry. The physics engine would resolve this by applying immense depenetration forces, causing the equipment to be violently ejected from the robot.

\begin{figure}[h]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/attachment_logic.png}
    \caption{State diagram of the attachment logic. When the state transitions from Detached to Attached, the object is parented, its physics are disabled (Kinematic), and collisions with the robot are ignored.}
    \label{fig:attachment_logic}
\end{figure}

\subsection{Sensor Simulation}
To validate the perception algorithms of the autonomous agents, the environment must generate synthetic data streams that are structurally identical to those produced by the physical EMAROs~\cite{EMAROs_Ref} robot (FR-11).

\textbf{LiDAR Simulation}

The \texttt{LidarSensor} component simulates a 2D planar laser scanner. Unlike GPU-based depth buffers often used in visual rendering, this implementation utilizes the Unity physics engine's raycasting API to query the scene geometry directly. In every simulation step, the component executes a loop corresponding to the configured number of beams (e.g., 360). For each beam, the system calculates a direction vector based on the scan angle. Since Unity utilizes a Left-Handed coordinate system (Y-up) and ROS uses a Right-Handed system (Z-up), the script employs a custom extension method to transform the desired scan angle into the correct Unity world-space vector.

A \texttt{Physics.Raycast} is projected along this vector. If the ray hits a collider on the collision layer, the distance is recorded; otherwise, the sensor returns infinity. The resulting range data is serialized into a standard \texttt{sensor\_msgs/LaserScan} message. To ensure temporal consistency with the navigation stack, the message header is stamped using the \texttt{ClockPublisher} immediately after the raycast loop completes. This precise timestamping prevents the ROS~2 transform tree (TF) from rejecting the scan data due to extrapolation errors, which can occur if the timestamp lags behind the robot's rapid movement.

\begin{figure}[h]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/lidar_raycasting.png}
    \caption{Visualization of the LiDAR simulation in the editor. Red lines indicate raycasts that did not hit an obstacle within range, while yellow lines indicate valid hits registered by the physics engine.}
    \label{fig:lidar_raycasting}
\end{figure}

\textbf{Camera Simulation}

Visual perception is handled by the \texttt{CameraSensor} component. This script attaches a dedicated Unity Camera to the robot model, which is configured to render the scene into a \texttt{RenderTexture} rather than the user's screen. This allows the simulation to generate visual data at a resolution independent of the application window.

The data extraction pipeline transfers the raw pixel data from the Graphics Processing Unit (GPU) memory to the Central Processing Unit (CPU) memory. Once on the CPU, the data is encoded into the appropriate format—either raw RGB8 for local processing or JPEG for bandwidth-efficient network transmission. The resulting byte array is published as a \texttt{sensor\_msgs/Image} or \texttt{CompressedImage}. To support various testing setups, the camera configuration is dynamic. The \texttt{RobotCameraManager} instantiates sensor prefabs at runtime based on the loaded scenario, attaching them to defined "bones" in the robot hierarchy. This allows the system to switch between a downward-facing camera for line following and a forward-facing camera for object detection without modifying the core robot definition.

\subsection{Dynamic Visuals and Terrain Modification}
A specific requirement for the system was the ability to support Reactive Behavior (FR-22), where the environment changes in response to the robot's actions. This is realized through a dynamic texture painting architecture implemented in the \texttt{TrackPainter} and \texttt{SharedPaintTextureRegistry} scripts. This feature allows specific interactions—such as a plow tool touching the ground—to persistently alter the visual appearance of the terrain. Unlike temporary decals or particle effects, this system modifies the underlying texture data of the ground material.

\textbf{Shared Texture Registry}

In complex scenarios, multiple agents (e.g., the robot, a VR controller, or a mouse interface) may attempt to modify the terrain simultaneously. To manage this, the \texttt{SharedPaintTextureRegistry} functions as a singleton manager. It maintains a dictionary of active textures and ensures that all painters operate on a single, shared instance of the \texttt{Texture2D}. This prevents race conditions where one agent overwrites the work of another and ensures data consistency across the simulation.

\textbf{Persistent State Modification}

The modification logic relies on physics raycasting to identify the precise UV coordinates of the surface at the point of contact. When a valid hit is detected on the target layer, the \texttt{TrackPainter} calculates the pixel coordinates on the texture map. It then modifies the pixel buffer directly using \texttt{SetPixels32}, changing the color of the "soil" to represent a new state (e.g., tilled earth or harvested crops).

Crucially, these changes are committed to the GPU using the \texttt{Apply} method. This ensures that the modification is not just visual but persistent. The changed pixels remain in the texture data, meaning that if the robot leaves the area and returns, the camera sensor will perceive the altered terrain. This closes the feedback loop between action and perception: the robot acts on the environment, the environment updates its state, and the robot's sensors perceive this new state to inform future decisions.

\begin{figure}[h]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/texture_painting.png}
    \caption{The dynamic terrain modification system. A raycast determines the UV coordinate of the tool's contact point (left), which is used to update the shared texture registry (right), permanently altering the ground appearance.}
    \label{fig:texture_painting}
\end{figure}

\subsection{Scenario and Object Management}
To support the requirement for comprehensive Scenario Management (FR-06), the system must be capable of reconfiguring the simulation context at runtime without requiring a restart of the entire application stack. This capability is essential for automated testing, allowing a test runner to cycle through various environments (e.g., a warehouse, a crop field, or a game arena) sequentially.

\textbf{Dynamic Scenario Loading}
\cite{ACHTUNG DOPPENLT DRIN }
The transition logic is encapsulated in the \texttt{SceneManager} component. This script maintains a subscription to the \texttt{/vera/load\_scenario} topic, listening for string-based triggers identifying the target environment. Upon receiving a valid command, the manager executes a rigid shutdown sequence to ensure system stability.
First, it explicitly triggers a shutdown of the ROS~2 communication layer via \texttt{Ros2cs.Shutdown()}. This step is critical; it forces the destruction of all active nodes, publishers, and subscribers, preventing "zombie" processes from lingering and polluting the network with invalid data during the transition. Once the network is severed, the Unity Scene Management API loads the new environment asynchronously. Upon completion, the new scene initializes its own \texttt{ROS2UnityComponent}~\cite{ROS2ForUnityDocs}, establishing a clean connection state for the next test iteration.

To ensure continuity of critical services—such as the simulation settings or the network configuration itself—specific GameObjects are governed by the \texttt{PersistAcrossScenes} utility. This implements a Singleton pattern using \texttt{DontDestroyOnLoad}, ensuring that essential infrastructure survives the scene transition while the environmental geometry is swapped out.

\textbf{Dynamic Object Spawning}

Beyond static geometry, the environment must support the lifecycle management of transient entities, such as the delivery boxes in the Logistics scenario. This is handled by the \texttt{DynamicObjectManager}, which acts as a bridge between the ROS~2 data stream and the Unity instantiation engine (FR-13).

The manager subscribes to the \texttt{/vera/virtual\_objects} topic, processing a custom message structure that defines an object's ID, type, and pose. Internally, the system maintains a dictionary mapping string identifiers (e.g., "box\_red", "obstacle\_A") to Unity Prefabs.
When an "ADD" or "MODIFY" command is received, the system checks if the object already exists. If it is new, the corresponding prefab is instantiated into the scene. If it exists, its transform is updated. Crucially, this component handles the coordinate conversion between the two systems, mapping the right-handed ROS position ($x, y, z$) and orientation ($x, y, z, w$) to the left-handed Unity world space. This allows external scripts or test runners to populate the scene with obstacles dynamically, validating the robot's ability to navigate changing environments.

\section{Mixed Reality and User Interaction}
\label{sec:mixed_reality_and_user_interaction}

A primary objective of the framework is to enhance the transparency of the robotic system by providing intuitive, real-time feedback to human operators. This is achieved through a Mixed Reality (MR) interface that visualizes internal robot states and allows for direct environmental manipulation. This section details the implementation of augmented telemetry, sensor projection, and the interactive control systems designed for both desktop and Virtual Reality (VR) contexts.

\subsection{Augmented Telemetry and Diagnosis}
To fulfill the requirement for Telemetry Display (FR-14), the system implements the \texttt{RobotInfoBillboard} component. This script aggregates critical health and status data—such as CPU usage, battery voltage, and the current mission task—and renders it as a floating interface anchored to the robot's coordinate frame.

\textbf{Hybrid Visualization Strategy}

For the AR floor projection and standard desktop views, the system utilizes Unity's immediate mode GUI (\texttt{OnGUI}) to render text overlays. This ensures that the text remains legible and strictly aligned with the screen plane, regardless of the projector's angle.

However, in a Virtual Reality environment, screen-space rendering breaks immersion and depth perception. To address this, the architecture supports a World Space Canvas solution for VR users. In this configuration, the textual information is rendered onto a 3D plane floating physically above the Digital Twin. To ensure readability from any perspective, the canvas implements a \texttt{LookAt} constraint in its update loop, continuously orienting the UI to face the VR headset's main camera. This allows an operator walking around the virtual field to inspect the robot's status by simply looking at it, mimicking a physical diagnostic display.

\subsection{Sensor Data Projection}
While the Digital Twin visualizes the robot's kinematics, it does not inherently convey what the robot ``sees.'' To bridge this gap, the system utilizes the \texttt{RosImageToMaterial} component to project the robot's internal perception directly into the environment (FR-15).

This component subscribes to the camera topics published by the robot. Upon receiving a frame—either raw or compressed—it decodes the data into a Unity \texttt{Texture2D} and applies it to a material on a planar mesh attached to the robot's chassis.

In the AR setup, this plane is positioned horizontally above the robot, projecting the computer vision debug output (such as bounding boxes or detected lines) onto the floor moving along with the robot. This provides observers with immediate visual confirmation of the perception stack's performance without requiring a separate monitor. Similarly, in VR, this projection functions as a virtual dashboard, allowing the user to verify alignment between the physical world and the robot's internal model from a top-down perspective.

\begin{figure}[h]
    \centering
    % \includegraphics[width=0.8\textwidth]{figures/augmented_telemetry.png}
    \caption{The Augmented Telemetry interface. A billboard displays system stats (top), while the \texttt{RosImageToMaterial} component projects the live OpenCV debug feed onto a plane attached to the robot (bottom), visualizing the internal state of the perception stack.}
    \label{fig:augmented_telemetry}
\end{figure}

\subsection{Interactive Control Interfaces}
The system enables users to actively modify the simulation state and direct the robot's actions (FR-17, FR-18). The implementation follows an input-agnostic architecture, where the core logic relies on 3D raycasting rather than specific hardware events. This design allows the system to support both standard mouse inputs and 3D VR controllers interchangeably.

\textbf{Goal Setting and Navigation}

The \texttt{NavigationGoalController} facilitates high-level control of the robot. In the desktop configuration, it casts a ray from the camera through the mouse cursor coordinates. When the ray intersects with the ground layer, the script calculates the target point and orientation based on the user's drag gesture. This data is serialized into a \texttt{geometry\_msgs/PoseStamped} message and published to the \texttt{/goal\_pose} topic, triggering the ROS~2 navigation stack. For Virtual Reality, this logic utilizes the \texttt{XR Ray Interactor}, enabling the operator to point at the virtual floor and dispatch the robot using the controller's trigger.

\textbf{Environmental Modification and Physics Interaction}

To support dynamic scenarios like Line Following, the user must be able to alter the environment at runtime. The \texttt{MouseSurfacePainter} component enables users to draw directly onto the terrain texture using a continuous raycast. In VR, this allows the operator to naturally draw paths or obstacles in 3D space that the physical robot can immediately perceive and follow.

Beyond painting, the VR interface leverages the physics engine to allow direct object manipulation. Users can physically grab and move interactive objects, such as the logistics boxes, using the VR controller's grip function. This allows for the manual reset of test scenarios or the introduction of dynamic obstacles (e.g., dropping a box in the robot's path) to validate the system's reactive planning capabilities (FR-22).

\textbf{Visualization Management}

In complex mixed reality scenarios, the density of visual information can become overwhelming. To manage this, the \texttt{VisibilityToggleManager} allows users to selectively hide or reveal specific visualization aids attached to the robot. This component maps input actions to the rendering state of the \texttt{RobotInfoBillboard}, the projected camera plane, and even the robot's visual cylinder body. This feature allows operators to declutter the view when necessary—for example, hiding the large floating billboard to inspect the interaction between the robot's chassis and a tool—without disabling the underlying functionality.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Implementation of Autonomous Agents}
\label{sec:autonomous_agents}

To validate the capabilities of the Mixed Reality Environment, the system hosts three distinct autonomous agents. These agents are not simple hard-coded scripts but fully featured robotic applications that utilize the synthetic sensor data provided by the Digital Twin to make decisions (FR-19). They serve to demonstrate the system's ability to support various domains of robotics, from agricultural field coverage to logic-based logistics.

The high-level decision-making for all agents is orchestrated using \textit{YASMIN} (Yet Another State MachINe), a ROS~2-native library that allows for the creation of hierarchical, interruptible mission behaviors. This architecture ensures that the robot can switch between reactive behaviors (e.g., obstacle avoidance) and deliberative tasks (e.g., path planning) seamlessly.

\subsection{Navigation and Control Stack}
\label{subsec:nav_stack}

The fundamental capability of movement is provided by the \textit{Navigation2 (Nav2)} stack~\cite{NAV2Ref}. To address the varying physical characteristics of the simulated versus the physical robot, the architecture employs a dual-configuration strategy. Two distinct parameter sets—\texttt{nav2\_params\_virtual.yaml} and \texttt{nav2\_params\_robot.yaml}—are maintained. While they share the same behavioral tree structure, they utilize different tuning for inflation radii and cost scaling to account for the physical robot's safety margins and sensor noise.

A critical design choice in the controller configuration is the use of the \textbf{Rotation Shim Controller} wrapping the \textbf{DWB Local Planner}. In tight operating environments, such as the crop rows of the Smart Farming scenario or the dense storage areas of the Logistics scenario, standard differential drive controllers often fail to navigate sharp turns without collision. The Rotation Shim intercepts navigation commands and forces the robot to rotate in place to align with the path heading before attempting to move forward. This ensures that the robot can maneuver in confined spaces where the available turning radius is smaller than the robot's kinematic limits.

To ensure safety during autonomous operation, a \textbf{Collision Monitor} node runs in parallel with the navigation stack. Configured with a defined polygon representing the robot's footprint plus a safety margin, this node monitors the LiDAR stream directly. If an obstacle breaches the safety polygon, the monitor overrides the navigation stack and publishes a zero-velocity command to the motor controller, acting as a software-level emergency stop.

\subsection{Modular Perception Stack}
\label{subsec:perception_architecture}

To interpret the visual data generated by the \texttt{CameraSensor}, the agents employ specialized computer vision pipelines tailored to their operational requirements. This satisfies the requirement for Perception Compatibility (FR-20) by ensuring algorithms function identically on virtual and real camera feeds.

\textbf{Modular Perception Node (Farm \& Logistics)}
For the Smart Farming and Logistics agents, a modular \texttt{perception\_node.py} was developed. Rather than running all detection algorithms simultaneously, which would consume excessive computational resources, this node acts as a central dispatcher. It switches between specialized \textbf{Processors} based on the current mission state defined by the state machine (e.g., switching from \textit{Field Detection} to \textit{Edge Following}).
All processors inherit from a base class (\texttt{base\_processor.py}) that standardizes the ingestion of \texttt{sensor\_msgs/CompressedImage} messages and their conversion to OpenCV format (BGR8).

\textbf{Dedicated Line Perception Node}
The Line Following agent utilizes a standalone \texttt{line\_perception\_node.py}. This separation allows for a streamlined, high-performance pipeline optimized specifically for low-latency visual servoing, unburdened by the overhead of modular task switching.

\subsection{Agent 1: Smart Farming}
\label{subsec:agent_farming}

\textbf{Scenario Context}
This agent operates in a simulated agricultural field consisting of crop rows and uncultivated soil. The goal is to identify the field boundaries, navigate along the crop edges without damaging them, and sequentially apply three tools (plow, seeder, harvester) to the field.

\textbf{Perception Modes}
The agent utilizes four distinct perception processors depending on the active state:
\begin{enumerate}
    \item \textbf{Field Detection (\texttt{FieldDetectionProcessor}):} Determines if the robot is currently on arable land by calculating the ratio of "field-colored" pixels in the image using HSV thresholding.
    \item \textbf{Edge Detection (\texttt{EdgeDetectionProcessor}):} Used during the approach phase. It creates binary masks for both "field" and "outer" textures. By applying morphological dilation to the field mask and computing the bitwise AND with the outer mask, it identifies the boundary line where the two textures meet.
    \item \textbf{Edge Adjustment \& Following:} Implemented via two specialized processors (\texttt{EdgeAdjustmentProcessor} and \texttt{EdgeFollowingProcessor}), these modules are used for precise alignment and tracking. They utilize the Probabilistic Hough Transform (\texttt{cv2.HoughLinesP}) to find line segments along the detected boundary, calculating the robot's lateral error (distance from the line centroid to image center) and angular error (deviation from vertical).
    \item \textbf{Equipment Detection (\texttt{EquipmentDetectionProcessor}):} Active during tool search. It isolates specific tool colors (e.g., yellow for harvester) using HSV masking and calculates the object's centroid to guide the docking approach.
\end{enumerate}

The real-time debug stream (Figure \ref{fig:farm_debug}) visualizes these detections, overlaying the regression line (green) or the equipment bounding box (blue) onto the camera feed.

\begin{figure}[ht]
    \centering
    %\includegraphics[width=0.8\textwidth]{figures/farm_perception_debug} 
    \caption{Debug view of the Smart Farming agent. Left: Edge detection visualizing the crop/soil boundary intersection. Right: Equipment detection highlighting the target tool for attachment.}
    \label{fig:farm_debug}
\end{figure}

\textbf{State Machine Logic}
The mission is governed by a hierarchical YASMIN state machine (Figure \ref{fig:farm_state_machine}) which executes two distinct phases:

\textbf{Phase 1: Field Mapping (Initialization)}
\begin{enumerate}
    \item \textbf{CheckOnField:} The robot verifies it is inside the field boundaries.
    \item \textbf{DriveToEdge:} The robot drives forward until the \texttt{EdgeDetectionProcessor} identifies a boundary. It then executes an open-loop forward drive for a calibrated distance (e.g., 20cm) to account for the camera's blind spot, ensuring the robot's center of rotation is aligned with the edge.
    \item \textbf{AdjustToEdge:} The robot rotates in place until the \texttt{EdgeAdjustmentProcessor} confirms that the edge line is vertical (angular error near zero) and centered.
    \item \textbf{FollowEdge (Mapping):} The robot circumnavigates the field using a PID controller on the lateral error. Crucially, this action detects \textbf{Field Corners} by monitoring the robot's odometry. A sharp change in yaw (approx. 90 degrees) triggers the recording of the robot's current pose. Once four corners are recorded, the field boundary is defined.
\end{enumerate}

\textbf{Phase 2: Equipment Cycle}
\begin{enumerate}
    \item \textbf{FindEquipment:} The robot executes a \textbf{Spiral Search} pattern using the Nav2 stack~\cite{NAV2Ref} to locate a specific tool.
    \item \textbf{EquipTool:} Upon detection, the robot approaches the tool. The action triggers the Unity \texttt{AttachmentCommandManager} to kinematically lock the tool to the robot.
    \item \textbf{GenerateCoveragePath:} Using the four corners identified in Phase 1, this action calculates a boustrophedon (lawnmower) path that covers the entire field area.
    \item \textbf{ExecuteCoverage:} The robot drives the generated path. During this phase, the Unity \texttt{TrackPainter} modifies the ground texture to visually represent tilled soil.
    \item \textbf{Return \& Unequip:} The robot returns the tool to its origin and detaches it. The state machine then loops back to search for the next tool in the sequence (Seeder, then Harvester).
\end{enumerate}

\begin{figure}[ht]
    \centering
    %\includegraphics[width=0.8\textwidth]{figures/farm_agent_fsm} 
    \caption{The Smart Farming State Machine. The robot first maps the field boundaries (FollowEdge) to identify corners, then uses these corners to generate coverage paths for various tools.}
    \label{fig:farm_state_machine}
\end{figure}

\subsection{Agent 2: Logistics}
\label{subsec:agent_logistics}

\textbf{Scenario Context}
The Logistics agent operates in a dynamic warehouse environment populated with colored transport boxes (Red, Blue, Green, Yellow) and corresponding projected delivery zones. The robot's objective is to autonomously search for items, physically attach them, identify the correct sorting destination, and transport the payload. This scenario validates the system's capabilities regarding object manipulation (FR-13), spatial memory, and error recovery.

\textbf{Perception Modes}
The agent employs the modular \texttt{perception\_node.py}, switching between two specialized processors based on the mission phase:
\begin{enumerate}
    \item \textbf{Box Detection (\texttt{BoxDetectionProcessor}):} This processor isolates manipulatable objects from the background. It applies an HSV color mask to filter specific target colors (or iterates through all known colors for an "Any" search). It utilizes \texttt{cv2.findContours} to identify object blobs, filtering them by a minimum area threshold to reject sensor noise. The centroid of the largest valid contour is calculated using image moments (\texttt{cv2.moments}) to provide a steering target.
    \item \textbf{Zone Detection (\texttt{ZoneDetectionProcessor}):} This processor is tuned to detect the flat, projected delivery zones on the floor. While similar to box detection, it uses distinct HSV ranges to account for the additive lighting nature of the projector and calculates the zone's center to ensure the robot drops the box inside the boundary.
\end{enumerate}

To assist in debugging, the perception node publishes an annotated video stream (Figure \ref{fig:logistics_debug}). The system draws bounding contours around valid targets and overlays classification labels (e.g., "RED BOX", "Distance: 1.2m") and the active search status directly onto the feed.

\begin{figure}[ht]
    \centering
    %\includegraphics[width=0.8\textwidth]{figures/logistics_perception_debug} 
    \caption{Debug view of the Logistics agent. The processor identifies a red transport box, drawing a contour around it and calculating the centroid (red dot) for the approach vector.}
    \label{fig:logistics_debug}
\end{figure}

\textbf{State Machine Logic}
The mission logic is defined in \texttt{mission\_state\_machine.py} (Logistics variant) and implements a "Search-Retrieve-Deliver" loop. A key feature of this agent is the utilization of the YASMIN \textbf{Blackboard} to implement spatial memory, allowing the robot to learn the environment structure over time.

\begin{enumerate}
    \item \textbf{FindBox (Spiral Search):} The robot executes the \texttt{find\_box\_action\_server}. This generates a spiral pattern of waypoints using the Nav2 stack~\cite{NAV2Ref} to efficiently cover the floor plan. The robot navigates these waypoints until the \texttt{BoxDetectionProcessor} identifies a valid target in the camera's Region of Interest.
    \item \textbf{AttachBox:} Upon reaching the target, the robot performs a precision approach. The \texttt{AttachBox} action sends a formatted string command (e.g., \texttt{"attach,box\_red"}) via the \texttt{/robot\_command} topic to the Unity \texttt{AttachmentCommandManager}, which kinematically locks the virtual box to the robot's chassis for stable transport.
    \item \textbf{Spatial Memory Query:} Before searching for the destination, the agent queries the Blackboard. If the location of the matching delivery zone (e.g., "Red Zone") was discovered and cached during a previous traversal, the search phase is skipped.
    \item \textbf{FindDeliveryZone / NavigateToZone:} If the location is unknown, the robot triggers \texttt{FindDeliveryZone} to execute a new spiral search. If the location is known, it uses \texttt{NavigateToZone} to drive directly to the stored coordinates.
    \item \textbf{VerifyZoneColor (Recovery):} Upon arriving at the target, the robot executes the \texttt{VerifyZoneColor} action. This serves as a critical reliability check. If the camera view is obstructed (e.g., by the carried box) or odometry drift has occurred, the robot performs a "Spin Recovery" maneuver. It rotates 90 degrees left and right to re-acquire the visual target before confirming the drop.
    \item \textbf{DropBox:} Finally, the robot drives forward into the zone and triggers the detach command, releasing the object into the sorting area.
\end{enumerate}

\begin{figure}[ht]
    \centering
    %\includegraphics[width=0.8\textwidth]{figures/logistics_agent_fsm} 
    \caption{The Logistics Agent State Machine. The logic flows from searching for boxes to delivering them, utilizing the Blackboard to store and recall zone locations to optimize efficiency over time.}
    \label{fig:logistics_state_machine}
\end{figure}

\subsection{Agent 3: Line Follower}
\label{subsec:agent_line}

\textbf{Scenario Context}
The Line Follower agent operates in an interactive playground where a human user can draw paths on the virtual ground in real-time. The robot must identify and track this line immediately. This scenario serves as a stress test for the system's end-to-end latency and validates the \textbf{Reactive Behavior (FR-22)} capability, as the path is not pre-calculated but defined dynamically by the user.

\textbf{Perception Modes}
Unlike the other agents, this implementation utilizes a dedicated, high-performance node: \texttt{line\_perception\_node.py}. This node is optimized for low-latency visual servoing:
\begin{itemize}
    \item \textbf{ROI Cropping:} The image is cropped to the \textbf{bottom 50\% (configurable via parameters)}, focusing solely on the immediate path in front of the robot. This removes background noise and simplifies the processing required.
     \item \textbf{Line Fitting:} The processor applies a color threshold to isolate the high-contrast line drawn by the user. It then utilizes \texttt{cv2.fitLine} to fit a vector through the detected white pixels.
    \item \textbf{Error Calculation:} From this vector, the system calculates two control metrics: the \textit{Lateral Error} (horizontal distance of the line from the image center) and the \textit{Angular Error} (deviation of the line's heading from the robot's forward vector).
\end{itemize}
The debug view (Figure \ref{fig:line_debug}) visualizes the binary mask and overlays the calculated heading vector, allowing the operator to see the error terms driving the control loop.

\begin{figure}[ht]
    \centering
    %\includegraphics[width=0.8\textwidth]{figures/line_perception_debug} 
    \caption{Debug view of the Line Follower. The system isolates the user-drawn path (binary mask) and fits a vector (blue line) to calculate lateral and angular errors for the PID controller.}
    \label{fig:line_debug}
\end{figure}

\textbf{State Machine Logic}
While the previous agents relied on the Nav2 stack~\cite{NAV2Ref} for motion planning, this agent implements a custom \textbf{Visual Servoing} control loop. The \texttt{mission\_state\_machine.py} orchestrates a three-stage sequence:

\begin{enumerate}
    \item \textbf{FindLine:} The robot executes a spiral search pattern using the \texttt{find\_line\_action\_server} to locate the start of the user-drawn path.
    \item \textbf{AlignToLine:} Once the line is detected, the robot transitions to the \texttt{AlignToLine} action. The robot rotates in place (zero linear velocity) until the angular error returned by the perception node is minimized, ensuring the robot is parallel to the path before moving.
    \item \textbf{FollowLine:} The robot engages a custom Proportional-Integral-Derivative (PID) controller implemented in \texttt{follow\_line\_action\_server.py}. This controller calculates velocity commands (\texttt{cmd\_vel}) directly from the perception error values. By bypassing the global path planning and costmap layers of ~\cite{NAV2Ref}, this direct control loop achieves the low latency required to react to sudden changes in the user-drawn line. If the line is erased or ends, the state machine transitions back to \texttt{FindLine} to search for a new path.
\end{enumerate}

\begin{figure}[ht]
    \centering
    %\includegraphics[width=0.6\textwidth]{figures/line_follower_fsm} 
    \caption{The Line Follower State Machine. The logic ensures the robot first locates and aligns with the path before entering the high-speed PID following loop.}
    \label{fig:line_fsm}
\end{figure}