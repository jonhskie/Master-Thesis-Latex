\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces An example of a VitL setup. A real car on a test track connected to a high-fidelity simulator like CARLA that generates virtual traffic and sensor data.~\blx@tocontentsinit {0}\cite {DSR22}}}{7}{figure.2.1}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces The foundational concept of a Digital Twin, illustrating the three core components: the physical entity, the virtual model and the bi-directional data connection that links them~\blx@tocontentsinit {0}\cite {Grieves2017, Leng2021}.~\blx@tocontentsinit {0}\cite {Grieves15}}}{9}{figure.2.2}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces The Reality-Virtuality Continuum, illustrating the spectrum from a real environment to a completely virtual one.~\blx@tocontentsinit {0}\cite {Walker2023}}}{10}{figure.2.3}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces The ROS~2 publish-subscribe model. Node A publishes messages onto a central topic and any number of subscriber nodes (B and C) may receive that data without direct knowledge of the publisher.}}{12}{figure.2.4}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces A simplified example of a ROS~2 transform tree (tf tree). The library maintains the hierarchical relationships so that a program can easily determine the transform from the \texttt {camera\_link} to the \texttt {world} frame, for instance.}}{12}{figure.2.5}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces The physical setup of the VERA platform: A projector and tracking system are mounted on a frame above the test area.~\blx@tocontentsinit {0}\cite {Geh24}}}{14}{figure.2.6}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces The EMAROs robot is the robotic platform used in the testbed, equipped with a modular sensor suite and running ROS~2.~\blx@tocontentsinit {0}\cite {Geh24}}}{15}{figure.2.7}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces The original VERA used point clouds~\blx@tocontentsinit {0}\cite {Geh24}, while the current iteration uses ArUco markers for pose estimation~\blx@tocontentsinit {0}\cite {JuliaBA}.}}{16}{figure.2.8}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Original VERA visualization via Pygame, showing a projection of the robot's path and virtual obstacles onto the floor.~\blx@tocontentsinit {0}\cite {Geh24}}}{17}{figure.2.9}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces The Mixed Reality Environment in operation. The physical robot interacts with a projected Smart Farming scenario, where the robot and the digital twin are synchronized via ROS~2.}}{27}{figure.4.1}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces The Hardware and Software Topology of the Mixed Reality Environment.}}{31}{figure.4.2}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Software Component Architecture. Unity scripts act as ROS~2 nodes, publishing sensor data and subscribing to control commands using ROS~2 topics.}}{33}{figure.4.3}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces The robot attachment logic. (a) The robot approaches the target object.\\ (b) The object is kinematically coupled to the robot chassis.}}{37}{figure.4.4}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Demonstration of dynamic surface modification. The robot uses the \texttt {TrackPainter.cs} component to modify the floor texture in real time based on its trajectory, creating a persistent black trail.}}{38}{figure.4.5}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Visualization of the LiDAR simulation in the editor. Red debug lines indicate raycasts that did not hit an obstacle within range, while yellow debug lines indicate valid hits registered by the physics engine.}}{39}{figure.4.6}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Schematic of the Environment Projection logic. The Unity Orthographic Camera (left) is calibrated such that its \texttt {orthographicSize} corresponds exactly to half the physical height of the projection area on the laboratory floor (right), ensuring a 1:1 metric scale.}}{41}{figure.4.7}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces The Augmented Telemetry interface. A billboard displays system stats, while the \texttt {RosImageToMaterial.cs} component projects the live OpenCV debug feed onto a plane attached to the robot, visualizing the internal state of the perception stack.}}{43}{figure.4.8}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces Debug view of the Line Follower. The system isolates the user-drawn path and fits a vector (blue line) to calculate lateral and angular errors for the PID controller.}}{47}{figure.4.9}%
\contentsline {figure}{\numberline {4.10}{\ignorespaces Debug view of the Logistics application. The processor identifies a red transport box in its ROI (green) and calculates the centroid (red dot).\blx@tocontentsinit {0}\cite {FALSCHEBOXBILD}}}{49}{figure.4.10}%
\contentsline {figure}{\numberline {4.11}{\ignorespaces Debug view of the Smart Farming application during edge following. The processor identifies the boundary between the field (left) and the path (right). The green line represents the detected edge used for navigation, while the red dot indicates the centroid used to compute the lateral and angular errors displayed in the overlay.}}{51}{figure.4.11}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.3}{\ignorespaces The Augmented Reality projection in the physical laboratory. The system projects the virtual map, the robot's status billboard, and the live camera feed onto the floor, following the movement of the physical EMAROs robot.}}{57}{figure.5.3}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Augmented Reality projection of the Paint scenario. The physical EMAROs robot (center) follows a red path drawn by the user. The projected line serves as the visual stimulus for the robot's internal perception stack, closing the loop between user input and physical action.}}{59}{figure.5.4}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Top-down view of the Pong scenario. The user utilizes the Nav2 stack to position the physical robot (left) to intercept the virtual ball. The ball interacts with the digital twin's geometry, bouncing off the attached paddle based on the robot's real-world position.}}{60}{figure.5.5}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Augmented Reality projection of the Logistics scenario. The physical robot (center) navigates through the projected warehouse to transport a virtual box to the designated colored zone. The environment allows the VR user to dynamically place obstacles, forcing the robot to replan its path.}}{62}{figure.5.6}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces AR projection of the Smart Farming scenario. The physical EMAROs robot, equipped with the Seeder tool, navigates the virtual field, modifying the ground texture from "Cultivated" to "Seeded" as it progresses.}}{63}{figure.5.7}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces Performance breakdown illustrating the CPU execution time for Physics, Rendering, and Scripts across the four operational scenarios. The total active frame time remains well below the 16.67 ms threshold required for 60 FPS.}}{65}{figure.5.8}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Scalability stress test illustrating CPU frame time vs. object count. The 60 FPS target (16.67 ms) serves as the threshold for visual stability in Mixed Reality.}}{68}{figure.5.9}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces Visualization latency measured across four application scenarios. The chart compares the delay between pose reception and frame completion in AR Projection mode versus Virtual Reality mode. Error bars indicate the jitter (min/max range).}}{70}{figure.5.10}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
